{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train.drop(['SalePrice','Id'],axis=1)\n",
    "test = test.drop(['Id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 79)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 79)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DropMissingValue(data:pd,threshold = 0.3) -> pd: # drop missing value that exceed threshold of number of observations\n",
    "    df_null = data.isnull().sum()\n",
    "    null_index = df_null[df_null >= threshold*data.shape[0]].index\n",
    "    data = data.drop(null_index,axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = DropMissingValue(train_features)\n",
    "test_new = DropMissingValue(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new.columns == test_new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill nan with mean for float numbers except for year,fill nan with mode for categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_null = train_new.isnull().sum().sort_values(ascending=False) != 0\n",
    "train_null_idx = train_null[train_null].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_null = test_new.isnull().sum().sort_values(ascending=False) != 0\n",
    "test_null_idx = test_null[test_null].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LotFrontage     float64\n",
       "GarageFinish     object\n",
       "GarageType       object\n",
       "GarageCond       object\n",
       "GarageQual       object\n",
       "GarageYrBlt     float64\n",
       "BsmtExposure     object\n",
       "BsmtFinType2     object\n",
       "BsmtFinType1     object\n",
       "BsmtCond         object\n",
       "BsmtQual         object\n",
       "MasVnrArea      float64\n",
       "MasVnrType       object\n",
       "Electrical       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new[train_null_idx].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LotFrontage', 'GarageFinish', 'GarageType', 'GarageCond', 'GarageQual',\n",
       "       'GarageYrBlt', 'BsmtExposure', 'BsmtFinType2', 'BsmtFinType1',\n",
       "       'BsmtCond', 'BsmtQual', 'MasVnrArea', 'MasVnrType', 'Electrical'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_null_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new[['LotFrontage','MasVnrArea']] = train_new[['LotFrontage','MasVnrArea']].fillna(train_new.mean().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new[[ 'GarageCond', 'GarageType', 'GarageYrBlt','GarageFinish', 'GarageQual', 'BsmtFinType2', 'BsmtExposure',\n",
    "           'BsmtFinType1', 'BsmtCond', 'BsmtQual', 'MasVnrType','Electrical']] = train_new[[ 'GarageCond', 'GarageType', 'GarageYrBlt','GarageFinish', 'GarageQual', 'BsmtFinType2', 'BsmtExposure',\n",
    "           'BsmtFinType1', 'BsmtCond', 'BsmtQual', 'MasVnrType','Electrical']].fillna(train_new.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LotFrontage', 'GarageFinish', 'GarageCond', 'GarageQual',\n",
       "       'GarageYrBlt', 'GarageType', 'BsmtCond', 'BsmtQual', 'BsmtExposure',\n",
       "       'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MasVnrArea', 'MSZoning',\n",
       "       'Functional', 'BsmtHalfBath', 'BsmtFullBath', 'Utilities',\n",
       "       'Exterior2nd', 'Exterior1st', 'KitchenQual', 'TotalBsmtSF',\n",
       "       'GarageCars', 'SaleType', 'BsmtUnfSF', 'GarageArea', 'BsmtFinSF2',\n",
       "       'BsmtFinSF1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_null_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new[['LotFrontage','MasVnrArea','BsmtHalfBath','BsmtFullBath','BsmtUnfSF','BsmtFinSF2','BsmtFinSF1','TotalBsmtSF',\n",
    "          'GarageArea']] = test_new[['LotFrontage','MasVnrArea','BsmtHalfBath','BsmtFullBath','BsmtUnfSF','BsmtFinSF2','BsmtFinSF1','TotalBsmtSF',\n",
    "          'GarageArea']].fillna(train_new.mean().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new[[ 'GarageCond', 'GarageQual', 'GarageYrBlt',\n",
    "       'GarageFinish', 'GarageType', 'BsmtCond', 'BsmtQual', 'BsmtExposure',\n",
    "       'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSZoning', 'Utilities', 'Functional', 'BsmtUnfSF',\n",
    "       'SaleType', 'Exterior2nd', 'Exterior1st', 'GarageCars', 'KitchenQual']] = test_new[[ 'GarageCond', 'GarageQual', 'GarageYrBlt',\n",
    "       'GarageFinish', 'GarageType', 'BsmtCond', 'BsmtQual', 'BsmtExposure',\n",
    "       'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSZoning', 'Utilities', 'Functional', 'BsmtUnfSF',\n",
    "       'SaleType', 'Exterior2nd', 'Exterior1st', 'GarageCars', 'KitchenQual']].fillna(train_new.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_new,test_new])  #Combine train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dummies(data:pd) -> pd:  # dummy all categorical features\n",
    "    object_features = data.dtypes[df.dtypes == object].index\n",
    "    dummies = pd.get_dummies(data[object_features])\n",
    "    df_new = pd.concat([data,dummies],axis=1)\n",
    "    df_new.drop(columns=object_features,inplace=True)\n",
    "    df_new = df_new.loc[:,~df_new.columns.duplicated()]\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = Dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 270)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the train and test back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = df_new.iloc[:1460,:]\n",
    "test_new = df_new.iloc[1460:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 270 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_ConLw  \\\n",
       "0          2003       196.0       706.0         0.0  ...               0   \n",
       "1          1976         0.0       978.0         0.0  ...               0   \n",
       "2          2002       162.0       486.0         0.0  ...               0   \n",
       "3          1970         0.0       216.0         0.0  ...               0   \n",
       "4          2000       350.0       655.0         0.0  ...               0   \n",
       "\n",
       "   SaleType_New  SaleType_Oth  SaleType_WD  SaleCondition_Abnorml  \\\n",
       "0             0             0            1                      0   \n",
       "1             0             0            1                      0   \n",
       "2             0             0            1                      0   \n",
       "3             0             0            1                      1   \n",
       "4             0             0            1                      0   \n",
       "\n",
       "   SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n",
       "0                      0                     0                     0   \n",
       "1                      0                     0                     0   \n",
       "2                      0                     0                     0   \n",
       "3                      0                     0                     0   \n",
       "4                      0                     0                     0   \n",
       "\n",
       "   SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                     1                      0  \n",
       "1                     1                      0  \n",
       "2                     1                      0  \n",
       "3                     0                      0  \n",
       "4                     1                      0  \n",
       "\n",
       "[5 rows x 270 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.09193234],\n",
       "       [-0.09193234,  1.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(train_new['OverallQual'],train_new['OverallCond'])  \n",
    "# these two features looks correlated as names implies, but they are not correlated much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YearBuilt: Original construction date<br>\n",
    "YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)<br>\n",
    "MoSold: Month Sold (MM)<br>\n",
    "YrSold: Year Sold (YYYY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "House's life and remodel  may be a critical feature to price the house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do something with YearBuilt and YearRemodAdd and MoSold: Month Sold (MM) and YrSold: Year Sold (YYYY)\n",
    "train_new['LifeBuilt'] = train_new['MoSold']/12 + train_new['YrSold'] - train_new['YearBuilt']\n",
    "train_new['LifeRemod'] = train_new['MoSold']/12 + train_new['YrSold'] - train_new['YearRemodAdd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new['LifeBuilt'] = test_new['MoSold']/12 + test_new['YrSold'] - test_new['YearBuilt']\n",
    "test_new['LifeRemod'] = test_new['MoSold']/12 + test_new['YrSold'] - test_new['YearRemodAdd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = train_new.drop(['MoSold','YrSold','YearBuilt','YearRemodAdd'],axis=1)\n",
    "test_new = test_new.drop(['MoSold','YrSold','YearBuilt','YearRemodAdd'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YearBuilt and YearRemodAdd : Result does not improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: <br>\n",
    "Try to do something with square feet : <br>\n",
    "\n",
    "BsmtFinSF2: Type 2 finished square feet<br>\n",
    "BsmtUnfSF: Unfinished square feet of basement area<br>\n",
    "TotalBsmtSF: Total square feet of basement area<br>\n",
    "\n",
    "1stFlrSF: First Floor square feet<br>\n",
    "2ndFlrSF: Second floor square feet<br>\n",
    "LowQualFinSF: Low quality finished square feet (all floors)<br>\n",
    "GrLivArea: Above grade (ground) living area square feet<br>\n",
    "\n",
    "GarageArea: Size of garage in square feet<br>\n",
    "\n",
    "WoodDeckSF: Wood deck area in square feet<br>\n",
    "\n",
    "OpenPorchSF: Open porch area in square feet<br>\n",
    "\n",
    "EnclosedPorch: Enclosed porch area in square feet<br>\n",
    "3SsnPorch: Three season porch area in square feet<br>\n",
    "ScreenPorch: Screen porch area in square feet<br>\n",
    "PoolArea: Pool area in square feet<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_new\n",
    "y = train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:23:12] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimator = [20,30,50,100,120,200,300,500,900,1000]\n",
    "booster = ['gbtree','gblinear']\n",
    "base_score = [0.25,0.5,0.75,1]\n",
    "max_depth = [2,3,5,10,15]\n",
    "learning_rate = [0.05,0.1,0.15,0.2,0.25,0.3]\n",
    "min_childweight = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparametergrid = {'n_estimator':n_estimator,\n",
    "                      'max_depth':max_depth,\n",
    "                     'learning_rate':learning_rate,\n",
    "                     'min_child_weight':min_childweight,\n",
    "                     'booster':booster,\n",
    "                     'base_score':base_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cv = RandomizedSearchCV(estimator=xgb_model,\n",
    "                              param_distributions=hyperparametergrid,\n",
    "                              cv=5,scoring='neg_mean_squared_error',return_train_score=True,verbose=5,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[09:23:21] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-133948725.635, test=-595281283.199), total=   0.7s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[09:23:22] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-153272548.448, test=-1485458666.538), total=   0.7s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[09:23:23] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-130417701.470, test=-1182855501.734), total=   0.7s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[09:23:24] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-138693577.789, test=-709100125.951), total=   0.7s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[09:23:24] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    2.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-137702883.456, test=-740258929.714), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1 \n",
      "[09:23:25] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1, score=(train=-1527042827.471, test=-828498479.683), total=   0.5s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1 \n",
      "[09:23:25] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1, score=(train=-1276830854.822, test=-1853397852.588), total=   0.5s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1 \n",
      "[09:23:26] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1, score=(train=-1073803553.336, test=-3813948415.383), total=   0.5s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1 \n",
      "[09:23:26] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1, score=(train=-1382483363.976, test=-1699636463.907), total=   0.5s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1 \n",
      "[09:23:27] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1, score=(train=-1462119585.834, test=-1143591113.575), total=   0.5s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25 \n",
      "[09:23:27] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25, score=(train=-70865609.410, test=-675279498.131), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25 \n",
      "[09:23:28] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25, score=(train=-76659465.824, test=-1570644293.540), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25 \n",
      "[09:23:29] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25, score=(train=-61898606.506, test=-1119458134.737), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25 \n",
      "[09:23:29] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25, score=(train=-65866227.085, test=-837050911.412), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25 \n",
      "[09:23:30] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25, score=(train=-78229133.326, test=-685455495.989), total=   0.7s\n",
      "[CV] n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[09:23:31] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-1561667392.851, test=-854511372.084), total=   0.5s\n",
      "[CV] n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[09:23:31] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-1317000201.175, test=-1874359320.461), total=   0.5s\n",
      "[CV] n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[09:23:32] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-1092886431.488, test=-3838296556.105), total=   0.5s\n",
      "[CV] n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[09:23:32] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-1419673009.267, test=-1745522561.437), total=   0.5s\n",
      "[CV] n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[09:23:33] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-1501617105.953, test=-1165374511.116), total=   0.5s\n",
      "[CV] n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[09:23:33] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-1589893184.494, test=-872574575.206), total=   0.5s\n",
      "[CV] n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[09:23:34] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-1350087685.855, test=-1905297527.843), total=   0.5s\n",
      "[CV] n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[09:23:34] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-1110664407.594, test=-3789907109.257), total=   0.5s\n",
      "[CV] n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[09:23:35] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-1449658064.572, test=-1792108290.401), total=   0.5s\n",
      "[CV] n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[09:23:35] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-1531949522.574, test=-1182086253.067), total=   0.5s\n",
      "[CV] n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[09:23:35] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-242004140.692, test=-678741115.730), total=   0.5s\n",
      "[CV] n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[09:23:36] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-241444374.100, test=-1523612117.133), total=   0.5s\n",
      "[CV] n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[09:23:37] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-234182223.266, test=-1275752085.720), total=   0.5s\n",
      "[CV] n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[09:23:37] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-269355037.498, test=-817175306.019), total=   0.5s\n",
      "[CV] n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[09:23:38] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-235552658.654, test=-751113673.583), total=   0.5s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75 \n",
      "[09:23:38] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75, score=(train=-5850496.312, test=-603414696.090), total=   1.0s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75 \n",
      "[09:23:39] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75, score=(train=-5494440.612, test=-1450425748.275), total=   1.0s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75 \n",
      "[09:23:40] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75, score=(train=-4764753.495, test=-1293996717.302), total=   1.0s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75 \n",
      "[09:23:41] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75, score=(train=-5276435.793, test=-910272844.891), total=   1.0s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75 \n",
      "[09:23:42] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75, score=(train=-5278310.707, test=-668727630.070), total=   1.0s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5 \n",
      "[09:23:43] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5, score=(train=-1516929537.373, test=-820129669.939), total=   0.5s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5 \n",
      "[09:23:44] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5, score=(train=-1265560186.044, test=-1850779461.861), total=   0.5s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5 \n",
      "[09:23:44] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5, score=(train=-1067938854.260, test=-3792462150.328), total=   0.5s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5 \n",
      "[09:23:45] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5, score=(train=-1371756054.978, test=-1686984589.692), total=   0.5s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5 \n",
      "[09:23:45] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5, score=(train=-1449639775.184, test=-1138082677.444), total=   0.5s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[09:23:46] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-350171178.849, test=-541843920.878), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[09:23:46] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-338153262.777, test=-1504816472.502), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[09:23:47] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-334654866.208, test=-1315581889.262), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[09:23:48] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-334814950.875, test=-820376746.439), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[09:23:48] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-340664625.617, test=-852575478.195), total=   0.7s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[09:23:49] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-371289639.047, test=-606988211.464), total=   0.5s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[09:23:50] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-346941171.844, test=-1661704203.448), total=   0.5s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[09:23:50] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-323067753.011, test=-1054933549.152), total=   0.6s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[09:23:51] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-387312278.861, test=-1243536086.505), total=   0.5s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[09:23:51] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-362544175.499, test=-1064453914.364), total=   0.5s\n",
      "[09:23:52] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   30.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                          colsample_bylevel=1,\n",
       "                                          colsample_bynode=1,\n",
       "                                          colsample_bytree=1, gamma=0,\n",
       "                                          importance_type='gain',\n",
       "                                          learning_rate=0.1, max_delta_step=0,\n",
       "                                          max_depth=3, min_child_weight=1,\n",
       "                                          missing=None, n_estimators=100,\n",
       "                                          n_jobs=1, nthread=None,\n",
       "                                          objective='reg:linear',\n",
       "                                          random_st...\n",
       "                   param_distributions={'base_score': [0.25, 0.5, 0.75, 1],\n",
       "                                        'booster': ['gbtree', 'gblinear'],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2,\n",
       "                                                          0.25, 0.3],\n",
       "                                        'max_depth': [2, 3, 5, 10, 15],\n",
       "                                        'min_child_weight': [1, 2, 3, 4, 5],\n",
       "                                        'n_estimator': [20, 30, 50, 100, 120,\n",
       "                                                        200, 300, 500, 900,\n",
       "                                                        1000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=123, refit=True,\n",
       "                   return_train_score=True, scoring='neg_mean_squared_error',\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.15, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=4, missing=None, n_estimator=50,\n",
       "             n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize model using hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_opt = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.15, max_delta_step=0,\n",
    "             max_depth=3, min_child_weight=4, missing=None, n_estimator=50,\n",
    "             n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
    "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "             seed=None, silent=None, subsample=1, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:24:08] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.15, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=4, missing=None, n_estimator=50,\n",
       "             n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_opt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using Optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xgb_model_opt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = sqrt(mean_squared_error(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22524.1644873968\n"
     ]
    }
   ],
   "source": [
    "print(RMSE)  # error is higher than yesterday after doing the features of YearBuilt,YearRemodAdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1461</td>\n",
       "      <td>169277.052498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1462</td>\n",
       "      <td>187758.393989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1463</td>\n",
       "      <td>183583.683570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1464</td>\n",
       "      <td>179317.477511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1465</td>\n",
       "      <td>150730.079977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id      SalePrice\n",
       "0  1461  169277.052498\n",
       "1  1462  187758.393989\n",
       "2  1463  183583.683570\n",
       "3  1464  179317.477511\n",
       "4  1465  150730.079977"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "saleprice = xgb_model_opt.predict(test_new )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['SalePrice'] = saleprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1461</td>\n",
       "      <td>122592.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1462</td>\n",
       "      <td>154926.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1463</td>\n",
       "      <td>174332.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1464</td>\n",
       "      <td>193412.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1465</td>\n",
       "      <td>204540.671875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id      SalePrice\n",
       "0  1461  122592.929688\n",
       "1  1462  154926.968750\n",
       "2  1463  174332.906250\n",
       "3  1464  193412.609375\n",
       "4  1465  204540.671875"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1168, 268)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(60,input_shape=(input_shape,),activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(25,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(5,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = SGD(lr=0.2, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='adamax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/800\n",
      "1168/1168 [==============================] - 0s 150us/sample - loss: 37791022879.5616 - val_loss: 41064831228.4931\n",
      "Epoch 2/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 36965124264.3288 - val_loss: 40070011833.8630\n",
      "Epoch 3/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 35671017163.3973 - val_loss: 38186210121.6438\n",
      "Epoch 4/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 33269887929.8630 - val_loss: 34892631979.8356\n",
      "Epoch 5/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 29392817053.8082 - val_loss: 29641587164.9315\n",
      "Epoch 6/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 23835010763.3973 - val_loss: 22404860829.8082\n",
      "Epoch 7/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 17566523700.6027 - val_loss: 14907713683.2877\n",
      "Epoch 8/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 12890372369.5342 - val_loss: 9825068663.2329\n",
      "Epoch 9/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 10656819915.3973 - val_loss: 8059018331.1781\n",
      "Epoch 10/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 9635397141.0411 - val_loss: 7011752777.6438\n",
      "Epoch 11/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 8939645162.9589 - val_loss: 6176795963.6164\n",
      "Epoch 12/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 8474777158.1370 - val_loss: 5609818232.9863\n",
      "Epoch 13/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 8081263728.2192 - val_loss: 5139819516.4932\n",
      "Epoch 14/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 7756066097.0959 - val_loss: 4864575249.5342\n",
      "Epoch 15/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 7471751245.1507 - val_loss: 4647086641.0959\n",
      "Epoch 16/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 7212888320.0000 - val_loss: 4389520433.0959\n",
      "Epoch 17/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 6990488972.2740 - val_loss: 4353213950.2466\n",
      "Epoch 18/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 6775789922.1918 - val_loss: 4180055944.7671\n",
      "Epoch 19/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 6641429498.7397 - val_loss: 4129952831.1233\n",
      "Epoch 20/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 6375771069.3699 - val_loss: 4003711819.3973\n",
      "Epoch 21/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 6214746450.4110 - val_loss: 3861079150.4658\n",
      "Epoch 22/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 6011294895.3425 - val_loss: 3825040222.6849\n",
      "Epoch 23/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 5834264393.6438 - val_loss: 3846907497.2055\n",
      "Epoch 24/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 5654991051.3973 - val_loss: 3695813276.2740\n",
      "Epoch 25/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 5479663684.3836 - val_loss: 3703996099.0685\n",
      "Epoch 26/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 5317243086.9041 - val_loss: 3596125617.0959\n",
      "Epoch 27/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 5184249259.8356 - val_loss: 3556971207.4521\n",
      "Epoch 28/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 5015799541.4795 - val_loss: 3472284026.7397\n",
      "Epoch 29/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 4886052541.3699 - val_loss: 3454500836.8219\n",
      "Epoch 30/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 4778708529.0959 - val_loss: 3416076266.9589\n",
      "Epoch 31/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 4661931723.3973 - val_loss: 3350550087.8904\n",
      "Epoch 32/800\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 4556833532.4932 - val_loss: 3303881021.3699\n",
      "Epoch 33/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 4453837136.6575 - val_loss: 3293279351.2329\n",
      "Epoch 34/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 4346176171.8356 - val_loss: 3243787840.8767\n",
      "Epoch 35/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 4244933723.1781 - val_loss: 3192954539.8356\n",
      "Epoch 36/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 4149328469.9178 - val_loss: 3149746407.4521\n",
      "Epoch 37/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 4064265563.1781 - val_loss: 3142110425.4247\n",
      "Epoch 38/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 3974636673.7534 - val_loss: 3098465296.6575\n",
      "Epoch 39/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 3887622035.2877 - val_loss: 3084798062.4658\n",
      "Epoch 40/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 3798319507.2877 - val_loss: 3046654071.2329\n",
      "Epoch 41/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 3722826613.4795 - val_loss: 3047608076.2740\n",
      "Epoch 42/800\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 3638964879.7808 - val_loss: 3004001704.3288\n",
      "Epoch 43/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 3565612638.6849 - val_loss: 2987284367.7808\n",
      "Epoch 44/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 3493595670.7945 - val_loss: 2986064965.9178\n",
      "Epoch 45/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 3420174702.4658 - val_loss: 2927985549.1507\n",
      "Epoch 46/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 3355897470.2466 - val_loss: 2890372390.5753\n",
      "Epoch 47/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 3283485282.1918 - val_loss: 2911780864.0000\n",
      "Epoch 48/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 3241614253.5890 - val_loss: 2931109886.2466\n",
      "Epoch 49/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 3177285572.3836 - val_loss: 2869738743.6712\n",
      "Epoch 50/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 3132385972.6027 - val_loss: 2877213809.9726\n",
      "Epoch 51/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 3076494448.2192 - val_loss: 2837876378.3014\n",
      "Epoch 52/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 3028039765.9178 - val_loss: 2826233493.0411\n",
      "Epoch 53/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 2975244338.8493 - val_loss: 2807717011.2877\n",
      "Epoch 54/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 2932084897.3151 - val_loss: 2784106971.1781\n",
      "Epoch 55/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 2878050537.2055 - val_loss: 2780410539.8356\n",
      "Epoch 56/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 2830906334.6849 - val_loss: 2763080103.4521\n",
      "Epoch 57/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 2787186055.0137 - val_loss: 2745652982.3562\n",
      "Epoch 58/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 2741656835.5068 - val_loss: 2728996508.9315\n",
      "Epoch 59/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 2702658419.7260 - val_loss: 2710286847.1233\n",
      "Epoch 60/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 2668101063.8904 - val_loss: 2695939017.6438\n",
      "Epoch 61/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 2626189622.3562 - val_loss: 2713998009.8630\n",
      "Epoch 62/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 2592085646.0274 - val_loss: 2689183445.0411\n",
      "Epoch 63/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 2557126508.7123 - val_loss: 2642182684.9315\n",
      "Epoch 64/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 2524719530.0822 - val_loss: 2637475412.1644\n",
      "Epoch 65/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 2494104186.7397 - val_loss: 2638273601.7534\n",
      "Epoch 66/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 39us/sample - loss: 2464415305.6438 - val_loss: 2614319202.1918\n",
      "Epoch 67/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 2441450543.3425 - val_loss: 2605628037.2603\n",
      "Epoch 68/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 2420289241.4247 - val_loss: 2587014758.5753\n",
      "Epoch 69/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 2395266433.7534 - val_loss: 2562925778.4110\n",
      "Epoch 70/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 2373937155.5068 - val_loss: 2567334263.2329\n",
      "Epoch 71/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 2350934573.5890 - val_loss: 2537586354.8493\n",
      "Epoch 72/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 2334295667.7260 - val_loss: 2535830243.9452\n",
      "Epoch 73/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 2318622011.6164 - val_loss: 2513921795.5068\n",
      "Epoch 74/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 2299934453.4795 - val_loss: 2517695886.9041\n",
      "Epoch 75/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 2281179362.1918 - val_loss: 2508669948.4932\n",
      "Epoch 76/800\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 2268315216.6575 - val_loss: 2476312770.6301\n",
      "Epoch 77/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 2265270689.3151 - val_loss: 2463270883.9452\n",
      "Epoch 78/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 2243036522.9589 - val_loss: 2460615357.5890\n",
      "Epoch 79/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 2226064224.4384 - val_loss: 2448865290.5205\n",
      "Epoch 80/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 2212741093.6986 - val_loss: 2453000404.1644\n",
      "Epoch 81/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 2205700750.0274 - val_loss: 2451230828.7123\n",
      "Epoch 82/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 2189780513.3151 - val_loss: 2417019511.2329\n",
      "Epoch 83/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 2180035722.5205 - val_loss: 2409762875.6164\n",
      "Epoch 84/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 2166531252.6027 - val_loss: 2400810264.5479\n",
      "Epoch 85/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 2157763306.0822 - val_loss: 2376062604.2740\n",
      "Epoch 86/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 2145566998.7945 - val_loss: 2376261354.9589\n",
      "Epoch 87/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 2133337634.6301 - val_loss: 2359837677.7534\n",
      "Epoch 88/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 2125873925.2603 - val_loss: 2360701718.7945\n",
      "Epoch 89/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 2111541525.0411 - val_loss: 2335962154.0822\n",
      "Epoch 90/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 2102063936.8767 - val_loss: 2318184994.1918\n",
      "Epoch 91/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 2098288808.3288 - val_loss: 2311817105.5342\n",
      "Epoch 92/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 2082893220.8219 - val_loss: 2291291479.6712\n",
      "Epoch 93/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 2073675827.7260 - val_loss: 2304709411.0685\n",
      "Epoch 94/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 2066345388.7123 - val_loss: 2266803964.4932\n",
      "Epoch 95/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 2064693567.1233 - val_loss: 2252700051.2877\n",
      "Epoch 96/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 2052053866.9589 - val_loss: 2260182906.7397\n",
      "Epoch 97/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 2049597904.6575 - val_loss: 2281124759.6712\n",
      "Epoch 98/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 2039195511.2329 - val_loss: 2221912291.9452\n",
      "Epoch 99/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 2028930374.1370 - val_loss: 2236243079.4521\n",
      "Epoch 100/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 2019433085.3699 - val_loss: 2259238005.4795\n",
      "Epoch 101/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 2015711603.7260 - val_loss: 2240325923.9452\n",
      "Epoch 102/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 2009451527.0137 - val_loss: 2204688397.5890\n",
      "Epoch 103/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1997772785.0959 - val_loss: 2204743659.8356\n",
      "Epoch 104/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1994406276.3836 - val_loss: 2206997789.8082\n",
      "Epoch 105/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1981162015.5616 - val_loss: 2185164459.8356\n",
      "Epoch 106/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1979213238.3562 - val_loss: 2190307012.3836\n",
      "Epoch 107/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1966470380.7123 - val_loss: 2147173758.2466\n",
      "Epoch 108/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1963130375.0137 - val_loss: 2169047122.4110\n",
      "Epoch 109/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1956722522.3014 - val_loss: 2122154243.5068\n",
      "Epoch 110/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1951408613.6986 - val_loss: 2147016774.1370\n",
      "Epoch 111/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1941187861.0411 - val_loss: 2140148595.7260\n",
      "Epoch 112/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1938120786.4110 - val_loss: 2132998947.0685\n",
      "Epoch 113/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1933334601.6438 - val_loss: 2127221581.1507\n",
      "Epoch 114/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1925101746.8493 - val_loss: 2100595764.6027\n",
      "Epoch 115/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1922321737.6438 - val_loss: 2090893150.6849\n",
      "Epoch 116/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1914284679.8904 - val_loss: 2080477921.3151\n",
      "Epoch 117/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1908112405.0411 - val_loss: 2069566506.0822\n",
      "Epoch 118/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1906850682.7397 - val_loss: 2103960685.1507\n",
      "Epoch 119/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1897536668.0548 - val_loss: 2083059342.9041\n",
      "Epoch 120/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1893755773.3699 - val_loss: 2059158817.3151\n",
      "Epoch 121/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1890474234.7397 - val_loss: 2075839978.9589\n",
      "Epoch 122/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1886079433.6438 - val_loss: 2054050495.5616\n",
      "Epoch 123/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1877689377.3151 - val_loss: 2053158571.8356\n",
      "Epoch 124/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1876910397.3699 - val_loss: 2040801741.1507\n",
      "Epoch 125/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1868143292.4932 - val_loss: 2045471269.6986\n",
      "Epoch 126/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1866100903.4521 - val_loss: 2036658389.9178\n",
      "Epoch 127/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1860357833.6438 - val_loss: 2035326913.7534\n",
      "Epoch 128/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1862183791.3425 - val_loss: 2043891721.6438\n",
      "Epoch 129/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1860594024.3288 - val_loss: 2028354568.7671\n",
      "Epoch 130/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1854375907.9452 - val_loss: 2010340646.5753\n",
      "Epoch 131/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1847345962.0822 - val_loss: 2013685124.3836\n",
      "Epoch 132/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1844059372.7123 - val_loss: 2011061177.8630\n",
      "Epoch 133/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1838574087.0137 - val_loss: 1985676232.0000\n",
      "Epoch 134/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1840889258.9589 - val_loss: 2011463062.7945\n",
      "Epoch 135/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1832587686.5753 - val_loss: 1997119724.0548\n",
      "Epoch 136/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1827654703.3425 - val_loss: 2000427038.6849\n",
      "Epoch 137/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1830058126.0274 - val_loss: 1977098040.9863\n",
      "Epoch 138/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1819271274.0822 - val_loss: 1988306525.8082\n",
      "Epoch 139/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1820623617.7534 - val_loss: 1952858287.3425\n",
      "Epoch 140/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1811301414.5753 - val_loss: 2003523620.0548\n",
      "Epoch 141/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1813988215.2329 - val_loss: 1973242669.5890\n",
      "Epoch 142/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1806330133.9178 - val_loss: 1958162452.1644\n",
      "Epoch 143/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1808567631.7808 - val_loss: 1968710084.3836\n",
      "Epoch 144/800\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 1796869980.9315 - val_loss: 1937478904.9863\n",
      "Epoch 145/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1803166124.7123 - val_loss: 1944660835.9452\n",
      "Epoch 146/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1795603946.9589 - val_loss: 1934504266.9589\n",
      "Epoch 147/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1789357248.8767 - val_loss: 1936144531.2877\n",
      "Epoch 148/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1789146351.3425 - val_loss: 1922648490.0822\n",
      "Epoch 149/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1785981185.7534 - val_loss: 1933576097.3151\n",
      "Epoch 150/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1787669109.4795 - val_loss: 1928055252.1644\n",
      "Epoch 151/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1780576519.8904 - val_loss: 1906703493.2603\n",
      "Epoch 152/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1776354179.5068 - val_loss: 1950409025.7534\n",
      "Epoch 153/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1774245952.8767 - val_loss: 1909713590.3562\n",
      "Epoch 154/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1774547910.1370 - val_loss: 1929383791.7808\n",
      "Epoch 155/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1769765048.1096 - val_loss: 1895495971.0685\n",
      "Epoch 156/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1774397289.2055 - val_loss: 1917727899.1781\n",
      "Epoch 157/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1767796767.5616 - val_loss: 1875245319.0137\n",
      "Epoch 158/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1755477790.6849 - val_loss: 1934719314.4110\n",
      "Epoch 159/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1758181103.3425 - val_loss: 1916970685.3699\n",
      "Epoch 160/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1756196316.9315 - val_loss: 1893219801.4247\n",
      "Epoch 161/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1753904085.9178 - val_loss: 1906230334.2466\n",
      "Epoch 162/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1759256159.5616 - val_loss: 1886204002.1918\n",
      "Epoch 163/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1755821400.5479 - val_loss: 1906119105.4247\n",
      "Epoch 164/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1749712569.8630 - val_loss: 1900236565.7534\n",
      "Epoch 165/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1747956340.3836 - val_loss: 1893161149.3699\n",
      "Epoch 166/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1744274245.2603 - val_loss: 1887078900.6027\n",
      "Epoch 167/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1746377534.2466 - val_loss: 1873639613.3699\n",
      "Epoch 168/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1742869110.3562 - val_loss: 1873416637.3699\n",
      "Epoch 169/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1741684987.6164 - val_loss: 1877944327.8904\n",
      "Epoch 170/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1742575937.7534 - val_loss: 1869768774.1370\n",
      "Epoch 171/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1737904448.0000 - val_loss: 1864571767.2329\n",
      "Epoch 172/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1735539669.9178 - val_loss: 1881141577.6438\n",
      "Epoch 173/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1735228361.6438 - val_loss: 1856991330.1918\n",
      "Epoch 174/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1730426349.5890 - val_loss: 1881005404.9315\n",
      "Epoch 175/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1731901813.4795 - val_loss: 1870053539.0685\n",
      "Epoch 176/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1727355541.9178 - val_loss: 1867297013.4795\n",
      "Epoch 177/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1727990565.6986 - val_loss: 1873428693.0411\n",
      "Epoch 178/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1725422247.4521 - val_loss: 1864540317.8082\n",
      "Epoch 179/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1725370224.2192 - val_loss: 1864881448.3288\n",
      "Epoch 180/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1724434140.4932 - val_loss: 1863492042.0822\n",
      "Epoch 181/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1719957619.7260 - val_loss: 1862028565.4795\n",
      "Epoch 182/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1718497257.2055 - val_loss: 1850761103.7808\n",
      "Epoch 183/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1718114705.5342 - val_loss: 1860899787.3973\n",
      "Epoch 184/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1718726217.6438 - val_loss: 1849120907.8356\n",
      "Epoch 185/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1716768303.3425 - val_loss: 1818034393.4247\n",
      "Epoch 186/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1713134571.8356 - val_loss: 1872405287.0137\n",
      "Epoch 187/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1710457825.3151 - val_loss: 1859924693.4795\n",
      "Epoch 188/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1709420784.2192 - val_loss: 1856122585.6164\n",
      "Epoch 189/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1707413546.0822 - val_loss: 1838784936.3288\n",
      "Epoch 190/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1706105414.1370 - val_loss: 1845887189.1507\n",
      "Epoch 191/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1705556305.5342 - val_loss: 1853911974.5753\n",
      "Epoch 192/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1705616990.6849 - val_loss: 1847642975.5616\n",
      "Epoch 193/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1703663369.6438 - val_loss: 1829845609.2055\n",
      "Epoch 194/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1703228965.6986 - val_loss: 1833372137.2055\n",
      "Epoch 195/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1701468075.8356 - val_loss: 1841996561.5342\n",
      "Epoch 196/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1698487796.6027 - val_loss: 1851649386.9589\n",
      "Epoch 197/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1699292502.7945 - val_loss: 1850076484.3836\n",
      "Epoch 198/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1697112611.9452 - val_loss: 1833290976.4384\n",
      "Epoch 199/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1699763933.8082 - val_loss: 1826720582.1370\n",
      "Epoch 200/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1700742179.0685 - val_loss: 1807744115.7260\n",
      "Epoch 201/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1691442005.0411 - val_loss: 1857214485.0411\n",
      "Epoch 202/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1694606477.1507 - val_loss: 1847679863.2329\n",
      "Epoch 203/800\n",
      "1168/1168 [==============================] - 0s 43us/sample - loss: 1690553480.7671 - val_loss: 1837338595.9452\n",
      "Epoch 204/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1689785053.8082 - val_loss: 1813282814.2466\n",
      "Epoch 205/800\n",
      "1168/1168 [==============================] - 0s 42us/sample - loss: 1695384944.2192 - val_loss: 1834605661.1507\n",
      "Epoch 206/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1689498960.2192 - val_loss: 1820557862.5753\n",
      "Epoch 207/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1687183126.7945 - val_loss: 1831353120.0000\n",
      "Epoch 208/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1687126049.3151 - val_loss: 1814517367.2329\n",
      "Epoch 209/800\n",
      "1168/1168 [==============================] - 0s 46us/sample - loss: 1685481976.1096 - val_loss: 1836911143.4521\n",
      "Epoch 210/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1682972533.4795 - val_loss: 1806347626.9589\n",
      "Epoch 211/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1685891390.2466 - val_loss: 1828851194.7397\n",
      "Epoch 212/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1699734748.9315 - val_loss: 1792616842.5205\n",
      "Epoch 213/800\n",
      "1168/1168 [==============================] - 0s 44us/sample - loss: 1680195131.6164 - val_loss: 1837918193.9726\n",
      "Epoch 214/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1686711731.7260 - val_loss: 1824285268.1644\n",
      "Epoch 215/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1680251583.1233 - val_loss: 1789405279.1233\n",
      "Epoch 216/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1671305381.6986 - val_loss: 1836830649.8630\n",
      "Epoch 217/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1675782275.5068 - val_loss: 1837794651.1781\n",
      "Epoch 218/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1675437259.3973 - val_loss: 1815010234.9589\n",
      "Epoch 219/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1673259581.3699 - val_loss: 1806378073.4247\n",
      "Epoch 220/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1681404958.6849 - val_loss: 1819307662.0274\n",
      "Epoch 221/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1671911018.0822 - val_loss: 1811522086.5753\n",
      "Epoch 222/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1668781741.5890 - val_loss: 1826691040.4384\n",
      "Epoch 223/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1667133425.0959 - val_loss: 1813111589.6986\n",
      "Epoch 224/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1665549629.3699 - val_loss: 1799672930.1918\n",
      "Epoch 225/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1667955072.0000 - val_loss: 1814467527.8904\n",
      "Epoch 226/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1667637567.1233 - val_loss: 1808879573.9178\n",
      "Epoch 227/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1662419703.2329 - val_loss: 1797146095.7808\n",
      "Epoch 228/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1664215532.7123 - val_loss: 1813317008.6575\n",
      "Epoch 229/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1660649390.9041 - val_loss: 1784238912.0000\n",
      "Epoch 230/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1662623369.6438 - val_loss: 1804966000.2192\n",
      "Epoch 231/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1659597799.0137 - val_loss: 1810642040.9863\n",
      "Epoch 232/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1656190726.1370 - val_loss: 1808589019.1781\n",
      "Epoch 233/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1659669521.5342 - val_loss: 1794802239.1233\n",
      "Epoch 234/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1659207163.6164 - val_loss: 1804884848.2192\n",
      "Epoch 235/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1665421906.4110 - val_loss: 1811287296.4384\n",
      "Epoch 236/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1658549076.1644 - val_loss: 1799343617.7534\n",
      "Epoch 237/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1652639513.4247 - val_loss: 1785063405.5890\n",
      "Epoch 238/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1655207402.9589 - val_loss: 1815302594.6301\n",
      "Epoch 239/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1655751255.6712 - val_loss: 1815068959.5616\n",
      "Epoch 240/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1657299529.6438 - val_loss: 1820968692.9315\n",
      "Epoch 241/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1648065409.7534 - val_loss: 1790607917.5890\n",
      "Epoch 242/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1648544216.5479 - val_loss: 1792803720.7671\n",
      "Epoch 243/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1646049440.4384 - val_loss: 1778379421.8082\n",
      "Epoch 244/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1644652344.1096 - val_loss: 1804229030.5753\n",
      "Epoch 245/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1644837552.2192 - val_loss: 1799788712.3288\n",
      "Epoch 246/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1647962533.6986 - val_loss: 1808888973.8082\n",
      "Epoch 247/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1657944250.7397 - val_loss: 1785589984.4384\n",
      "Epoch 248/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1646442422.3562 - val_loss: 1806715185.0959\n",
      "Epoch 249/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1647419495.4521 - val_loss: 1800677014.7945\n",
      "Epoch 250/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1642045708.2740 - val_loss: 1791381606.9041\n",
      "Epoch 251/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1638189400.1096 - val_loss: 1786264245.4795\n",
      "Epoch 252/800\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 1641298098.4110 - val_loss: 1806642011.1781\n",
      "Epoch 253/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1635159488.0000 - val_loss: 1802596357.2603\n",
      "Epoch 254/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1642324975.3425 - val_loss: 1811413156.8219\n",
      "Epoch 255/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1631790327.2329 - val_loss: 1768542465.7534\n",
      "Epoch 256/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1637218645.9178 - val_loss: 1792564732.4932\n",
      "Epoch 257/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1633872255.1233 - val_loss: 1770733535.5616\n",
      "Epoch 258/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1628108828.0548 - val_loss: 1823085921.7534\n",
      "Epoch 259/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1630201635.9452 - val_loss: 1790462771.7260\n",
      "Epoch 260/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1627941435.6164 - val_loss: 1799022190.4658\n",
      "Epoch 261/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1627863328.4384 - val_loss: 1792900318.6849\n",
      "Epoch 262/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1630514743.2329 - val_loss: 1781403414.5753\n",
      "Epoch 263/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1626304064.8767 - val_loss: 1801252391.4521\n",
      "Epoch 264/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1630517249.7534 - val_loss: 1801416646.1370\n",
      "Epoch 265/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1623342518.3562 - val_loss: 1796617948.9315\n",
      "Epoch 266/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1621710360.5479 - val_loss: 1797459598.9041\n",
      "Epoch 267/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1624919886.9041 - val_loss: 1761283976.9863\n",
      "Epoch 268/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1615601728.0000 - val_loss: 1812953525.9178\n",
      "Epoch 269/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1614912732.9315 - val_loss: 1794894330.7397\n",
      "Epoch 270/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1613026865.9726 - val_loss: 1784645141.0411\n",
      "Epoch 271/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1616829965.1507 - val_loss: 1796732913.5342\n",
      "Epoch 272/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1613689377.3151 - val_loss: 1787301287.4521\n",
      "Epoch 273/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1613836575.1233 - val_loss: 1778739450.7397\n",
      "Epoch 274/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1617477012.6027 - val_loss: 1809570854.5753\n",
      "Epoch 275/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1607546304.8767 - val_loss: 1766994258.4110\n",
      "Epoch 276/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1610318648.1096 - val_loss: 1790475104.0000\n",
      "Epoch 277/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1607745208.1096 - val_loss: 1804600228.8219\n",
      "Epoch 278/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1609795680.4384 - val_loss: 1788920993.3151\n",
      "Epoch 279/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1604332666.7397 - val_loss: 1769522691.5068\n",
      "Epoch 280/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1606729806.0274 - val_loss: 1771508301.5890\n",
      "Epoch 281/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1608032526.0274 - val_loss: 1813687629.1507\n",
      "Epoch 282/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1605438987.3973 - val_loss: 1797833493.0411\n",
      "Epoch 283/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1603562808.1096 - val_loss: 1768135683.5068\n",
      "Epoch 284/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1608351549.3699 - val_loss: 1779873586.9589\n",
      "Epoch 285/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1600933823.1233 - val_loss: 1776203530.5205\n",
      "Epoch 286/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1602310299.1781 - val_loss: 1770583034.7397\n",
      "Epoch 287/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1596369614.9041 - val_loss: 1776920715.8356\n",
      "Epoch 288/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1600118581.4795 - val_loss: 1819870051.9452\n",
      "Epoch 289/800\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 1596264703.5616 - val_loss: 1785363171.0685\n",
      "Epoch 290/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1596949404.0548 - val_loss: 1783971302.5753\n",
      "Epoch 291/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1596151194.3014 - val_loss: 1783122986.0822\n",
      "Epoch 292/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1598215346.8493 - val_loss: 1773870225.5342\n",
      "Epoch 293/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1591663168.0000 - val_loss: 1775924027.1781\n",
      "Epoch 294/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1590551118.9041 - val_loss: 1793578211.9452\n",
      "Epoch 295/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1589126412.2740 - val_loss: 1789280504.9863\n",
      "Epoch 296/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1588723515.8356 - val_loss: 1786965607.4521\n",
      "Epoch 297/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1590259708.4932 - val_loss: 1766388883.2877\n",
      "Epoch 298/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1587524343.2329 - val_loss: 1794099999.7808\n",
      "Epoch 299/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1587981582.9041 - val_loss: 1793023658.0822\n",
      "Epoch 300/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1588088474.3014 - val_loss: 1785308629.9178\n",
      "Epoch 301/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1589645591.6712 - val_loss: 1762365231.3425\n",
      "Epoch 302/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1590368399.7808 - val_loss: 1805453954.6301\n",
      "Epoch 303/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1586267542.7945 - val_loss: 1770467039.5616\n",
      "Epoch 304/800\n",
      "1168/1168 [==============================] - 0s 41us/sample - loss: 1585381393.5342 - val_loss: 1795802785.3151\n",
      "Epoch 305/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1587823512.5479 - val_loss: 1784851536.6575\n",
      "Epoch 306/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1580714981.6986 - val_loss: 1768977236.8219\n",
      "Epoch 307/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1585082272.4384 - val_loss: 1787933468.0548\n",
      "Epoch 308/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1579289175.6712 - val_loss: 1766346773.0411\n",
      "Epoch 309/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1585410071.6712 - val_loss: 1776503466.9589\n",
      "Epoch 310/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1580874971.1781 - val_loss: 1782271479.2329\n",
      "Epoch 311/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1579273663.1233 - val_loss: 1772453959.0137\n",
      "Epoch 312/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1576340286.2466 - val_loss: 1770861009.8630\n",
      "Epoch 313/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1572380098.1918 - val_loss: 1766550079.2329\n",
      "Epoch 314/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1578219823.3425 - val_loss: 1767748685.1507\n",
      "Epoch 315/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1573209129.6438 - val_loss: 1804832513.7534\n",
      "Epoch 316/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1575004524.7123 - val_loss: 1789281511.4521\n",
      "Epoch 317/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1568976363.8356 - val_loss: 1782110372.8219\n",
      "Epoch 318/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1569395887.3425 - val_loss: 1772164998.1370\n",
      "Epoch 319/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1573076484.8219 - val_loss: 1765875273.6438\n",
      "Epoch 320/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1567207565.5890 - val_loss: 1795470629.0411\n",
      "Epoch 321/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1566678300.4932 - val_loss: 1767929745.6438\n",
      "Epoch 322/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1566751574.7945 - val_loss: 1772712558.4658\n",
      "Epoch 323/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1565131738.3014 - val_loss: 1779454398.2466\n",
      "Epoch 324/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1571706675.7260 - val_loss: 1764336889.8630\n",
      "Epoch 325/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1568186631.0137 - val_loss: 1743418781.4795\n",
      "Epoch 326/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1569775994.7397 - val_loss: 1761731610.3014\n",
      "Epoch 327/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1562726075.6164 - val_loss: 1767284178.4110\n",
      "Epoch 328/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1560419324.4932 - val_loss: 1781146818.6301\n",
      "Epoch 329/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1563260145.9726 - val_loss: 1754047191.6712\n",
      "Epoch 330/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1557827675.1781 - val_loss: 1764108355.5068\n",
      "Epoch 331/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1555120376.1096 - val_loss: 1777884437.0411\n",
      "Epoch 332/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1559530929.9726 - val_loss: 1750660858.0822\n",
      "Epoch 333/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1562570031.3425 - val_loss: 1778170765.1507\n",
      "Epoch 334/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1551720074.5205 - val_loss: 1764853731.9452\n",
      "Epoch 335/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1558695053.5890 - val_loss: 1754856423.0137\n",
      "Epoch 336/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1553471576.5479 - val_loss: 1785896991.5616\n",
      "Epoch 337/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1547523441.0959 - val_loss: 1758075966.2466\n",
      "Epoch 338/800\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 1555733113.8630 - val_loss: 1772514727.8904\n",
      "Epoch 339/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1552251965.3699 - val_loss: 1766760172.7123\n",
      "Epoch 340/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1552936477.3699 - val_loss: 1741164771.9452\n",
      "Epoch 341/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1547606127.3425 - val_loss: 1748773669.6986\n",
      "Epoch 342/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1550171899.1781 - val_loss: 1788146406.5753\n",
      "Epoch 343/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1549705688.5479 - val_loss: 1754084135.4521\n",
      "Epoch 344/800\n",
      "1168/1168 [==============================] - 0s 42us/sample - loss: 1544918306.1918 - val_loss: 1761574740.1644\n",
      "Epoch 345/800\n",
      "1168/1168 [==============================] - 0s 42us/sample - loss: 1548547040.8767 - val_loss: 1765434992.2192\n",
      "Epoch 346/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1548828888.5479 - val_loss: 1766861021.8082\n",
      "Epoch 347/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1546720780.2740 - val_loss: 1751845163.8356\n",
      "Epoch 348/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1547232218.3014 - val_loss: 1753963050.0822\n",
      "Epoch 349/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1539439393.3151 - val_loss: 1758675477.0411\n",
      "Epoch 350/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1540624183.2329 - val_loss: 1760614690.1918\n",
      "Epoch 351/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1539216749.5890 - val_loss: 1783389664.4384\n",
      "Epoch 352/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1536862937.4247 - val_loss: 1751999440.6575\n",
      "Epoch 353/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1538403807.5616 - val_loss: 1768066026.9589\n",
      "Epoch 354/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1535814796.2740 - val_loss: 1769747730.8493\n",
      "Epoch 355/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1538655850.9589 - val_loss: 1758215405.5890\n",
      "Epoch 356/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1531715235.0685 - val_loss: 1780713231.5616\n",
      "Epoch 357/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1537537507.9452 - val_loss: 1763292992.8767\n",
      "Epoch 358/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1537445404.9315 - val_loss: 1767099408.2192\n",
      "Epoch 359/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1528494481.5342 - val_loss: 1753402799.0137\n",
      "Epoch 360/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1524857167.3425 - val_loss: 1744086275.5068\n",
      "Epoch 361/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1522572582.1370 - val_loss: 1751180737.3151\n",
      "Epoch 362/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1521203222.7945 - val_loss: 1754433091.5068\n",
      "Epoch 363/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1520276996.3836 - val_loss: 1762696261.2603\n",
      "Epoch 364/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1519789909.0411 - val_loss: 1760590293.0411\n",
      "Epoch 365/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1522081787.6164 - val_loss: 1755469136.6575\n",
      "Epoch 366/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1520970420.6027 - val_loss: 1736348394.9589\n",
      "Epoch 367/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1518030024.7671 - val_loss: 1754742166.7945\n",
      "Epoch 368/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1521434083.0685 - val_loss: 1751933149.3699\n",
      "Epoch 369/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1516897344.0000 - val_loss: 1759125128.7671\n",
      "Epoch 370/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1510872440.9863 - val_loss: 1757191509.0411\n",
      "Epoch 371/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1516081663.1233 - val_loss: 1768318039.6712\n",
      "Epoch 372/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1510551125.0411 - val_loss: 1761395896.1096\n",
      "Epoch 373/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1506718645.4795 - val_loss: 1740840528.6575\n",
      "Epoch 374/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1505858931.7260 - val_loss: 1720981617.9726\n",
      "Epoch 375/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1509121259.8356 - val_loss: 1752899915.3973\n",
      "Epoch 376/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1507414129.0959 - val_loss: 1731145377.3151\n",
      "Epoch 377/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1499677411.9452 - val_loss: 1746609836.7123\n",
      "Epoch 378/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1515382186.0822 - val_loss: 1734804414.2466\n",
      "Epoch 379/800\n",
      "1168/1168 [==============================] - 0s 43us/sample - loss: 1512708118.3562 - val_loss: 1734492795.1781\n",
      "Epoch 380/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1503898806.3562 - val_loss: 1734044350.2466\n",
      "Epoch 381/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1500536041.2055 - val_loss: 1740577169.5342\n",
      "Epoch 382/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1497054506.9589 - val_loss: 1721056301.5890\n",
      "Epoch 383/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1500135610.7397 - val_loss: 1706374916.3836\n",
      "Epoch 384/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1499181571.5068 - val_loss: 1713835155.2877\n",
      "Epoch 385/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1489290016.4384 - val_loss: 1745013508.3836\n",
      "Epoch 386/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1492016884.6027 - val_loss: 1751430933.9178\n",
      "Epoch 387/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1493872234.9589 - val_loss: 1743022659.5068\n",
      "Epoch 388/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1488686367.5616 - val_loss: 1738864224.4384\n",
      "Epoch 389/800\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 1491076946.4110 - val_loss: 1722581507.5068\n",
      "Epoch 390/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1498811183.3425 - val_loss: 1757257411.0685\n",
      "Epoch 391/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1489884475.6164 - val_loss: 1738158553.4247\n",
      "Epoch 392/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1487973411.9452 - val_loss: 1715460388.8219\n",
      "Epoch 393/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1486297138.8493 - val_loss: 1710945399.2329\n",
      "Epoch 394/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1482775549.3699 - val_loss: 1738812857.8630\n",
      "Epoch 395/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1487239345.0959 - val_loss: 1721993292.2740\n",
      "Epoch 396/800\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 1482699441.9726 - val_loss: 1737749215.7808\n",
      "Epoch 397/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1481005607.4521 - val_loss: 1721713526.6849\n",
      "Epoch 398/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1478086389.4795 - val_loss: 1708402919.4521\n",
      "Epoch 399/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1481031722.9589 - val_loss: 1743413263.6712\n",
      "Epoch 400/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1473237744.2192 - val_loss: 1711139696.2192\n",
      "Epoch 401/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1474499174.5753 - val_loss: 1712252998.1370\n",
      "Epoch 402/800\n",
      "1168/1168 [==============================] - 0s 41us/sample - loss: 1473559220.6027 - val_loss: 1717324519.4521\n",
      "Epoch 403/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1476164642.1918 - val_loss: 1708622211.0685\n",
      "Epoch 404/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1472809126.5753 - val_loss: 1735472903.0137\n",
      "Epoch 405/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1471578926.4658 - val_loss: 1710380393.2055\n",
      "Epoch 406/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1475251267.5068 - val_loss: 1745220848.2192\n",
      "Epoch 407/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1474598056.3288 - val_loss: 1693084468.6027\n",
      "Epoch 408/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1478608027.1781 - val_loss: 1728918317.5890\n",
      "Epoch 409/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1469583497.6438 - val_loss: 1695566848.0000\n",
      "Epoch 410/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1467887946.5205 - val_loss: 1699221424.2192\n",
      "Epoch 411/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1464340668.4932 - val_loss: 1723466254.0274\n",
      "Epoch 412/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1461973394.8493 - val_loss: 1721315093.0411\n",
      "Epoch 413/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1458451464.7671 - val_loss: 1718367193.4247\n",
      "Epoch 414/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1458920319.1233 - val_loss: 1715784409.4247\n",
      "Epoch 415/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1457520635.6164 - val_loss: 1726702248.3288\n",
      "Epoch 416/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1460976707.0685 - val_loss: 1727660951.6712\n",
      "Epoch 417/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1453383060.1644 - val_loss: 1702369423.7808\n",
      "Epoch 418/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1461024658.4110 - val_loss: 1700397243.6164\n",
      "Epoch 419/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1453557683.7260 - val_loss: 1713996926.2466\n",
      "Epoch 420/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1450763384.9863 - val_loss: 1706197248.0000\n",
      "Epoch 421/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1453687829.9178 - val_loss: 1719507031.6712\n",
      "Epoch 422/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1458463730.8493 - val_loss: 1696913758.2466\n",
      "Epoch 423/800\n",
      "1168/1168 [==============================] - 0s 42us/sample - loss: 1458748391.4521 - val_loss: 1710005279.3425\n",
      "Epoch 424/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1449157687.6712 - val_loss: 1692589276.9315\n",
      "Epoch 425/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1443477534.6849 - val_loss: 1713382319.3425\n",
      "Epoch 426/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1443322813.3699 - val_loss: 1694130543.3425\n",
      "Epoch 427/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1439379192.9863 - val_loss: 1701087390.6849\n",
      "Epoch 428/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1441267626.0822 - val_loss: 1694457158.1370\n",
      "Epoch 429/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1437006517.9178 - val_loss: 1708972023.6712\n",
      "Epoch 430/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1435128088.5479 - val_loss: 1695095260.9315\n",
      "Epoch 431/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1435757550.4658 - val_loss: 1704613369.4247\n",
      "Epoch 432/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1437169117.8082 - val_loss: 1677777144.9863\n",
      "Epoch 433/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1434978157.5890 - val_loss: 1693905457.0959\n",
      "Epoch 434/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1430472801.3151 - val_loss: 1705290144.4384\n",
      "Epoch 435/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1430695335.4521 - val_loss: 1671935866.7397\n",
      "Epoch 436/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1434217181.8082 - val_loss: 1689753225.0959\n",
      "Epoch 437/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1426214421.9178 - val_loss: 1690537613.8082\n",
      "Epoch 438/800\n",
      "1168/1168 [==============================] - 0s 41us/sample - loss: 1430238388.1644 - val_loss: 1688990755.0685\n",
      "Epoch 439/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1427741310.2466 - val_loss: 1680901929.2055\n",
      "Epoch 440/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1423759709.8082 - val_loss: 1699403949.5890\n",
      "Epoch 441/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1424697462.3562 - val_loss: 1684335987.7260\n",
      "Epoch 442/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1420502684.7123 - val_loss: 1670802344.5479\n",
      "Epoch 443/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1425231068.0548 - val_loss: 1679696910.0274\n",
      "Epoch 444/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1421244890.3014 - val_loss: 1689847678.2466\n",
      "Epoch 445/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1415956074.9589 - val_loss: 1666358160.0000\n",
      "Epoch 446/800\n",
      "1168/1168 [==============================] - 0s 46us/sample - loss: 1418008851.7260 - val_loss: 1675281883.1781\n",
      "Epoch 447/800\n",
      "1168/1168 [==============================] - 0s 44us/sample - loss: 1412747971.0685 - val_loss: 1677746680.9863\n",
      "Epoch 448/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1414648035.0685 - val_loss: 1671835589.6986\n",
      "Epoch 449/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1415034039.2329 - val_loss: 1675567613.8082\n",
      "Epoch 450/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1414276337.0959 - val_loss: 1675330735.3425\n",
      "Epoch 451/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1409043209.6438 - val_loss: 1671759442.5205\n",
      "Epoch 452/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1406488313.8630 - val_loss: 1673579462.1370\n",
      "Epoch 453/800\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 1407072687.3425 - val_loss: 1677934156.2740\n",
      "Epoch 454/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1409829868.7123 - val_loss: 1690581667.0685\n",
      "Epoch 455/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1403164359.8904 - val_loss: 1674312308.6027\n",
      "Epoch 456/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1410116453.6986 - val_loss: 1658024809.2055\n",
      "Epoch 457/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1399255437.1507 - val_loss: 1682083222.7945\n",
      "Epoch 458/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1401721580.7123 - val_loss: 1712075066.7397\n",
      "Epoch 459/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1401372976.6575 - val_loss: 1693568846.9041\n",
      "Epoch 460/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1399672689.9726 - val_loss: 1680274319.7808\n",
      "Epoch 461/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1397916754.4110 - val_loss: 1683450902.7945\n",
      "Epoch 462/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1399533752.1096 - val_loss: 1680466796.7123\n",
      "Epoch 463/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1395806594.0822 - val_loss: 1678161920.0000\n",
      "Epoch 464/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1395805674.0822 - val_loss: 1659817670.1370\n",
      "Epoch 465/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1392141080.9863 - val_loss: 1660944299.8356\n",
      "Epoch 466/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1391468695.6712 - val_loss: 1672176843.3973\n",
      "Epoch 467/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1389536307.7260 - val_loss: 1667447063.2329\n",
      "Epoch 468/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1390078050.1918 - val_loss: 1663960793.4247\n",
      "Epoch 469/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1389975976.3288 - val_loss: 1674252610.6301\n",
      "Epoch 470/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1391200384.8767 - val_loss: 1670044991.1233\n",
      "Epoch 471/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1389010076.9315 - val_loss: 1663520130.6301\n",
      "Epoch 472/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1391125857.7534 - val_loss: 1672712102.5753\n",
      "Epoch 473/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1390676514.1918 - val_loss: 1672610076.0548\n",
      "Epoch 474/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1389812694.7945 - val_loss: 1650160527.7808\n",
      "Epoch 475/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1382396412.4932 - val_loss: 1673796298.9589\n",
      "Epoch 476/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1378813675.8356 - val_loss: 1654000092.9315\n",
      "Epoch 477/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1385943918.4658 - val_loss: 1658675460.3836\n",
      "Epoch 478/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1386215717.6986 - val_loss: 1650371663.7808\n",
      "Epoch 479/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1378180381.8082 - val_loss: 1676394920.3288\n",
      "Epoch 480/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1381694926.9041 - val_loss: 1680440888.1096\n",
      "Epoch 481/800\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 1372812984.5479 - val_loss: 1656745738.5205\n",
      "Epoch 482/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1375700063.5616 - val_loss: 1641850459.1781\n",
      "Epoch 483/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1376248327.8904 - val_loss: 1653647174.1370\n",
      "Epoch 484/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1375646459.6164 - val_loss: 1663515528.7671\n",
      "Epoch 485/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1370422855.0137 - val_loss: 1649148173.1507\n",
      "Epoch 486/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1380589214.6849 - val_loss: 1660333680.2192\n",
      "Epoch 487/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1368493473.3151 - val_loss: 1655847842.1918\n",
      "Epoch 488/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1372819915.3973 - val_loss: 1631841263.0137\n",
      "Epoch 489/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1365896841.6438 - val_loss: 1666440990.6849\n",
      "Epoch 490/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1368320931.0685 - val_loss: 1658344198.1370\n",
      "Epoch 491/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1363713635.0685 - val_loss: 1662696071.8904\n",
      "Epoch 492/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1362551467.8356 - val_loss: 1660627326.2466\n",
      "Epoch 493/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1364911133.8082 - val_loss: 1666491616.4384\n",
      "Epoch 494/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1363573836.2740 - val_loss: 1656031919.3425\n",
      "Epoch 495/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1366336775.0137 - val_loss: 1654779244.7123\n",
      "Epoch 496/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1373376818.8493 - val_loss: 1639795471.7808\n",
      "Epoch 497/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1366270262.7945 - val_loss: 1632574332.0548\n",
      "Epoch 498/800\n",
      "1168/1168 [==============================] - 0s 42us/sample - loss: 1355397980.0548 - val_loss: 1651838720.8767\n",
      "Epoch 499/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1359789073.5342 - val_loss: 1669939373.5890\n",
      "Epoch 500/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1353882215.4521 - val_loss: 1649400348.0548\n",
      "Epoch 501/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1354904982.7945 - val_loss: 1671157038.4658\n",
      "Epoch 502/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1353476767.5616 - val_loss: 1645093921.3151\n",
      "Epoch 503/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1351355710.2466 - val_loss: 1637010775.6712\n",
      "Epoch 504/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1355614794.5205 - val_loss: 1642340956.9315\n",
      "Epoch 505/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1348137216.4384 - val_loss: 1646013681.0959\n",
      "Epoch 506/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1348775969.3151 - val_loss: 1644359449.4247\n",
      "Epoch 507/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1345849211.6164 - val_loss: 1645544317.3699\n",
      "Epoch 508/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1349600213.0411 - val_loss: 1643954884.3836\n",
      "Epoch 509/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1343786739.7260 - val_loss: 1662642484.6027\n",
      "Epoch 510/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1343141135.7808 - val_loss: 1633565450.5205\n",
      "Epoch 511/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1342517657.4247 - val_loss: 1656332220.9315\n",
      "Epoch 512/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1347334945.3151 - val_loss: 1650553360.6575\n",
      "Epoch 513/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1344072937.2055 - val_loss: 1644296704.0000\n",
      "Epoch 514/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1343386767.3425 - val_loss: 1659034439.8904\n",
      "Epoch 515/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1342166787.5068 - val_loss: 1647827117.5890\n",
      "Epoch 516/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1341551786.9589 - val_loss: 1629354850.1918\n",
      "Epoch 517/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1346998844.4932 - val_loss: 1643958997.9178\n",
      "Epoch 518/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1337387314.8493 - val_loss: 1633485181.3699\n",
      "Epoch 519/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1343590734.0274 - val_loss: 1630758375.4521\n",
      "Epoch 520/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1335843182.4658 - val_loss: 1670078723.5068\n",
      "Epoch 521/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1339063607.2329 - val_loss: 1658540614.7945\n",
      "Epoch 522/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1334551987.7260 - val_loss: 1617070841.8630\n",
      "Epoch 523/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1340606944.8767 - val_loss: 1621492300.2740\n",
      "Epoch 524/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1334593237.0411 - val_loss: 1661365678.0274\n",
      "Epoch 525/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1335549597.8082 - val_loss: 1615763540.1644\n",
      "Epoch 526/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1333444553.6438 - val_loss: 1650212714.0822\n",
      "Epoch 527/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1332789104.6575 - val_loss: 1614560820.6027\n",
      "Epoch 528/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1331128319.1233 - val_loss: 1628813275.1781\n",
      "Epoch 529/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1328744657.5342 - val_loss: 1629910398.2466\n",
      "Epoch 530/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1321180590.4658 - val_loss: 1645580631.6712\n",
      "Epoch 531/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1324450686.2466 - val_loss: 1650414794.5205\n",
      "Epoch 532/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1324463801.8630 - val_loss: 1635916155.6164\n",
      "Epoch 533/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1320784555.8356 - val_loss: 1630783822.9041\n",
      "Epoch 534/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1320103652.3836 - val_loss: 1625051018.5205\n",
      "Epoch 535/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1318116170.5205 - val_loss: 1641955559.4521\n",
      "Epoch 536/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1328534714.3014 - val_loss: 1605154048.0000\n",
      "Epoch 537/800\n",
      "1168/1168 [==============================] - 0s 41us/sample - loss: 1318467404.2740 - val_loss: 1643222012.4932\n",
      "Epoch 538/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1325754896.6575 - val_loss: 1633795902.0274\n",
      "Epoch 539/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1313863776.8767 - val_loss: 1616690288.2192\n",
      "Epoch 540/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1314911275.8356 - val_loss: 1602033154.6301\n",
      "Epoch 541/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1309151210.0822 - val_loss: 1642233504.5479\n",
      "Epoch 542/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1309800032.4384 - val_loss: 1646281521.7534\n",
      "Epoch 543/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1308591740.4932 - val_loss: 1625574890.9589\n",
      "Epoch 544/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1307691432.3288 - val_loss: 1617251980.2740\n",
      "Epoch 545/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1306122141.8082 - val_loss: 1621665895.8904\n",
      "Epoch 546/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1303260073.2055 - val_loss: 1620013708.2740\n",
      "Epoch 547/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1306180771.5068 - val_loss: 1640162986.0822\n",
      "Epoch 548/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1303840544.4384 - val_loss: 1631766527.1233\n",
      "Epoch 549/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1305538369.7534 - val_loss: 1627699572.2740\n",
      "Epoch 550/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1305370994.8493 - val_loss: 1619907338.5205\n",
      "Epoch 551/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1299754317.1507 - val_loss: 1622328414.6849\n",
      "Epoch 552/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1302339533.1507 - val_loss: 1642105436.9315\n",
      "Epoch 553/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1305172145.0959 - val_loss: 1636361366.3562\n",
      "Epoch 554/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1299157820.4932 - val_loss: 1622595941.6986\n",
      "Epoch 555/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1301137978.7397 - val_loss: 1635418073.4247\n",
      "Epoch 556/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1295432332.7123 - val_loss: 1618908457.8630\n",
      "Epoch 557/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1297997367.2329 - val_loss: 1618269331.2877\n",
      "Epoch 558/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1291139501.5890 - val_loss: 1597212679.0137\n",
      "Epoch 559/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1296490536.3288 - val_loss: 1613565920.8767\n",
      "Epoch 560/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1293194669.5890 - val_loss: 1591412887.0137\n",
      "Epoch 561/800\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 423496320.000 - 0s 35us/sample - loss: 1292542016.8767 - val_loss: 1624059598.9041\n",
      "Epoch 562/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1287644722.8493 - val_loss: 1605907627.8356\n",
      "Epoch 563/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1291819591.8904 - val_loss: 1629677093.4795\n",
      "Epoch 564/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1289221269.4795 - val_loss: 1596080696.9863\n",
      "Epoch 565/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1291376767.1233 - val_loss: 1618483254.3562\n",
      "Epoch 566/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1285154735.3425 - val_loss: 1599813179.6164\n",
      "Epoch 567/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1281790049.3151 - val_loss: 1631191343.7808\n",
      "Epoch 568/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1299408569.8630 - val_loss: 1576278003.7260\n",
      "Epoch 569/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1282152071.0137 - val_loss: 1615972943.5616\n",
      "Epoch 570/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1279952506.7397 - val_loss: 1612021987.9452\n",
      "Epoch 571/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1280091930.7397 - val_loss: 1616906828.7123\n",
      "Epoch 572/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1285570954.5205 - val_loss: 1619082468.8219\n",
      "Epoch 573/800\n",
      "1168/1168 [==============================] - 0s 41us/sample - loss: 1282283012.3836 - val_loss: 1613825416.7671\n",
      "Epoch 574/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1281135055.7808 - val_loss: 1608357803.8356\n",
      "Epoch 575/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1279005973.0411 - val_loss: 1617157006.0274\n",
      "Epoch 576/800\n",
      "1168/1168 [==============================] - 0s 48us/sample - loss: 1276133778.4110 - val_loss: 1625817496.3014\n",
      "Epoch 577/800\n",
      "1168/1168 [==============================] - 0s 42us/sample - loss: 1274652996.3836 - val_loss: 1601473701.6986\n",
      "Epoch 578/800\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 2141583104.00 - 0s 39us/sample - loss: 1274414375.4521 - val_loss: 1603269449.6438\n",
      "Epoch 579/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1275860822.7945 - val_loss: 1611102823.4521\n",
      "Epoch 580/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1273085320.7671 - val_loss: 1611493556.1644\n",
      "Epoch 581/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1273525178.7397 - val_loss: 1615892450.1918\n",
      "Epoch 582/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1274654381.1507 - val_loss: 1613176860.0548\n",
      "Epoch 583/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1268159285.0411 - val_loss: 1614193551.7808\n",
      "Epoch 584/800\n",
      "1168/1168 [==============================] - 0s 44us/sample - loss: 1272125871.3425 - val_loss: 1613051128.9863\n",
      "Epoch 585/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 46us/sample - loss: 1267921031.0137 - val_loss: 1602394304.8767\n",
      "Epoch 586/800\n",
      "1168/1168 [==============================] - 0s 50us/sample - loss: 1268098982.1370 - val_loss: 1600038338.6301\n",
      "Epoch 587/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1275584922.3014 - val_loss: 1622878634.5205\n",
      "Epoch 588/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1264713776.2192 - val_loss: 1582915538.4110\n",
      "Epoch 589/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1257957428.6027 - val_loss: 1614902878.6849\n",
      "Epoch 590/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1262580991.5616 - val_loss: 1611840350.6849\n",
      "Epoch 591/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1258244529.0959 - val_loss: 1596943434.5205\n",
      "Epoch 592/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1257663413.4795 - val_loss: 1602605012.1644\n",
      "Epoch 593/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1259697746.8493 - val_loss: 1584240079.3425\n",
      "Epoch 594/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1258772863.1233 - val_loss: 1616849587.7260\n",
      "Epoch 595/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1256424701.3699 - val_loss: 1594298454.3562\n",
      "Epoch 596/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1254552514.6301 - val_loss: 1587599955.2877\n",
      "Epoch 597/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1253152440.1096 - val_loss: 1598252431.5616\n",
      "Epoch 598/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1256536534.3562 - val_loss: 1601664479.6164\n",
      "Epoch 599/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1257224918.7945 - val_loss: 1598307378.8493\n",
      "Epoch 600/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1249447940.3836 - val_loss: 1599165573.2603\n",
      "Epoch 601/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1251754806.3562 - val_loss: 1602530765.1507\n",
      "Epoch 602/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1251698453.9178 - val_loss: 1610352520.7671\n",
      "Epoch 603/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1247704973.1507 - val_loss: 1585948980.6027\n",
      "Epoch 604/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1244828101.2603 - val_loss: 1603192867.0685\n",
      "Epoch 605/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1247030976.0000 - val_loss: 1609499961.8630\n",
      "Epoch 606/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1247435207.8904 - val_loss: 1600918007.8904\n",
      "Epoch 607/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1246849071.3425 - val_loss: 1584867856.6575\n",
      "Epoch 608/800\n",
      "1168/1168 [==============================] - 0s 41us/sample - loss: 1245980670.2466 - val_loss: 1582605450.5205\n",
      "Epoch 609/800\n",
      "1168/1168 [==============================] - 0s 41us/sample - loss: 1252288544.8767 - val_loss: 1612949218.7397\n",
      "Epoch 610/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1247773894.1370 - val_loss: 1627766229.9178\n",
      "Epoch 611/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1246055635.7260 - val_loss: 1606132424.7671\n",
      "Epoch 612/800\n",
      "1168/1168 [==============================] - 0s 41us/sample - loss: 1240011524.3836 - val_loss: 1592220000.6027\n",
      "Epoch 613/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1242084179.2877 - val_loss: 1588863046.1370\n",
      "Epoch 614/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1239130854.5753 - val_loss: 1602470765.1507\n",
      "Epoch 615/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1237724515.0685 - val_loss: 1584409071.5068\n",
      "Epoch 616/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1233602433.7534 - val_loss: 1592284394.0822\n",
      "Epoch 617/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1237371433.6438 - val_loss: 1592141574.1370\n",
      "Epoch 618/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1236321969.0959 - val_loss: 1569179332.3836\n",
      "Epoch 619/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1235152644.3836 - val_loss: 1588881122.1918\n",
      "Epoch 620/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1230937125.2603 - val_loss: 1587942913.3151\n",
      "Epoch 621/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1229148021.9178 - val_loss: 1582252659.3973\n",
      "Epoch 622/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1232511708.0548 - val_loss: 1614783925.4795\n",
      "Epoch 623/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1226241045.9178 - val_loss: 1587964751.6438\n",
      "Epoch 624/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1225739225.8630 - val_loss: 1591923077.2603\n",
      "Epoch 625/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1224699881.2055 - val_loss: 1585324084.6027\n",
      "Epoch 626/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1223509323.3973 - val_loss: 1599243563.8356\n",
      "Epoch 627/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1228266311.0137 - val_loss: 1566250702.9041\n",
      "Epoch 628/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1224170631.8904 - val_loss: 1573808821.2603\n",
      "Epoch 629/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1223389482.9589 - val_loss: 1572936536.5479\n",
      "Epoch 630/800\n",
      "1168/1168 [==============================] - 0s 45us/sample - loss: 1222564188.9315 - val_loss: 1583838743.6712\n",
      "Epoch 631/800\n",
      "1168/1168 [==============================] - 0s 44us/sample - loss: 1222487332.8219 - val_loss: 1588197901.8082\n",
      "Epoch 632/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1222090360.5479 - val_loss: 1585775761.0959\n",
      "Epoch 633/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1218076949.9178 - val_loss: 1595755505.9726\n",
      "Epoch 634/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1215877179.6164 - val_loss: 1583364620.9315\n",
      "Epoch 635/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1215725507.9452 - val_loss: 1588441968.2192\n",
      "Epoch 636/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1219273685.0411 - val_loss: 1595941606.7945\n",
      "Epoch 637/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1213793863.8904 - val_loss: 1583179928.5479\n",
      "Epoch 638/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1211319377.5342 - val_loss: 1575411224.5479\n",
      "Epoch 639/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1209815454.6849 - val_loss: 1570860934.1370\n",
      "Epoch 640/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1210592186.3014 - val_loss: 1594236702.6849\n",
      "Epoch 641/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1209511727.3425 - val_loss: 1588750600.9863\n",
      "Epoch 642/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1212892955.1781 - val_loss: 1582843104.4384\n",
      "Epoch 643/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1214016277.0411 - val_loss: 1570696426.8493\n",
      "Epoch 644/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1206634812.4932 - val_loss: 1568389214.6849\n",
      "Epoch 645/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1208284445.8082 - val_loss: 1601402599.4521\n",
      "Epoch 646/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1200625948.9315 - val_loss: 1562727146.9589\n",
      "Epoch 647/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1208137344.8767 - val_loss: 1573365809.0959\n",
      "Epoch 648/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1205599244.2740 - val_loss: 1587669269.0411\n",
      "Epoch 649/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1204902155.3973 - val_loss: 1566986648.5479\n",
      "Epoch 650/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1203100565.0411 - val_loss: 1568656824.1096\n",
      "Epoch 651/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1201137761.7534 - val_loss: 1570579047.5616\n",
      "Epoch 652/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1202150883.0685 - val_loss: 1557880650.9589\n",
      "Epoch 653/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1205389760.0000 - val_loss: 1599043542.3562\n",
      "Epoch 654/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1198746639.7808 - val_loss: 1569147590.1370\n",
      "Epoch 655/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1203257376.8767 - val_loss: 1563970049.3151\n",
      "Epoch 656/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1194066933.4795 - val_loss: 1579479609.8630\n",
      "Epoch 657/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1193129828.8219 - val_loss: 1568338869.2603\n",
      "Epoch 658/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1199327462.5753 - val_loss: 1585293091.0685\n",
      "Epoch 659/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1196130746.7397 - val_loss: 1570320395.8356\n",
      "Epoch 660/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1194964067.9452 - val_loss: 1575532835.0685\n",
      "Epoch 661/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1191804107.3973 - val_loss: 1586549643.3973\n",
      "Epoch 662/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1187973246.2466 - val_loss: 1556995208.7671\n",
      "Epoch 663/800\n",
      "1168/1168 [==============================] - 0s 44us/sample - loss: 1189144413.3699 - val_loss: 1572156539.9452\n",
      "Epoch 664/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1195231845.2603 - val_loss: 1597123177.2055\n",
      "Epoch 665/800\n",
      "1168/1168 [==============================] - 0s 43us/sample - loss: 1192510715.8356 - val_loss: 1590535795.7260\n",
      "Epoch 666/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1185862786.6301 - val_loss: 1560979950.4658\n",
      "Epoch 667/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1186395389.3699 - val_loss: 1563605093.2603\n",
      "Epoch 668/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1187563213.1507 - val_loss: 1573135461.2603\n",
      "Epoch 669/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1181937667.0685 - val_loss: 1550416741.6986\n",
      "Epoch 670/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1187394055.0137 - val_loss: 1559124530.6301\n",
      "Epoch 671/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1187697004.2740 - val_loss: 1555727608.9863\n",
      "Epoch 672/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1182196633.8630 - val_loss: 1546319756.2740\n",
      "Epoch 673/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1179297731.9452 - val_loss: 1569191413.4795\n",
      "Epoch 674/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1175235562.0822 - val_loss: 1553659899.6164\n",
      "Epoch 675/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1176714078.2466 - val_loss: 1546120781.1507\n",
      "Epoch 676/800\n",
      "1168/1168 [==============================] - 0s 42us/sample - loss: 1181897738.0822 - val_loss: 1554117088.4384\n",
      "Epoch 677/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1178752739.0685 - val_loss: 1569644965.6986\n",
      "Epoch 678/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1173832159.5616 - val_loss: 1565943322.3014\n",
      "Epoch 679/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1172828314.7397 - val_loss: 1553179720.3288\n",
      "Epoch 680/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1172947826.8493 - val_loss: 1576729701.6986\n",
      "Epoch 681/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1184104907.8356 - val_loss: 1549426382.9041\n",
      "Epoch 682/800\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 1170937628.9315 - val_loss: 1566992263.0137\n",
      "Epoch 683/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1167362803.7260 - val_loss: 1561490410.9589\n",
      "Epoch 684/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1173032107.8356 - val_loss: 1557269460.1644\n",
      "Epoch 685/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1175258072.1096 - val_loss: 1576219690.0822\n",
      "Epoch 686/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1174040292.8219 - val_loss: 1578702629.6986\n",
      "Epoch 687/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1167096796.9315 - val_loss: 1551323109.6986\n",
      "Epoch 688/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1165067169.3151 - val_loss: 1553910387.7260\n",
      "Epoch 689/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1164378488.9863 - val_loss: 1567030946.4110\n",
      "Epoch 690/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1160980194.1918 - val_loss: 1544222863.7808\n",
      "Epoch 691/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1165203716.8219 - val_loss: 1544238023.8904\n",
      "Epoch 692/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1164205632.0000 - val_loss: 1554419599.7808\n",
      "Epoch 693/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1161298268.0548 - val_loss: 1564028837.6986\n",
      "Epoch 694/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1158035320.9863 - val_loss: 1557334675.2877\n",
      "Epoch 695/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1154823175.0137 - val_loss: 1555481971.7260\n",
      "Epoch 696/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1156720370.8493 - val_loss: 1556530982.5753\n",
      "Epoch 697/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1156316209.0959 - val_loss: 1575287409.0959\n",
      "Epoch 698/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1156503187.2877 - val_loss: 1561909020.0548\n",
      "Epoch 699/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1153215086.4658 - val_loss: 1568574065.9726\n",
      "Epoch 700/800\n",
      "1168/1168 [==============================] - 0s 42us/sample - loss: 1156450951.8904 - val_loss: 1522946024.3288\n",
      "Epoch 701/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1161784445.8082 - val_loss: 1543948680.7671\n",
      "Epoch 702/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1162995964.2740 - val_loss: 1580466468.2740\n",
      "Epoch 703/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1152818393.4247 - val_loss: 1566253350.5753\n",
      "Epoch 704/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1149900042.5205 - val_loss: 1546934216.7671\n",
      "Epoch 705/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1149087520.0000 - val_loss: 1565340952.5479\n",
      "Epoch 706/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1154753803.8356 - val_loss: 1540356194.1918\n",
      "Epoch 707/800\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 748791424.000 - 0s 35us/sample - loss: 1152339491.0685 - val_loss: 1544399684.3836\n",
      "Epoch 708/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1150832253.8082 - val_loss: 1555091434.0822\n",
      "Epoch 709/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1144027940.8219 - val_loss: 1556083808.4384\n",
      "Epoch 710/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1145142712.9863 - val_loss: 1545867548.4932\n",
      "Epoch 711/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1144242699.3973 - val_loss: 1544589679.6712\n",
      "Epoch 712/800\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 1144323466.5205 - val_loss: 1557444741.2603\n",
      "Epoch 713/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1140540494.0274 - val_loss: 1543201678.0274\n",
      "Epoch 714/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1142476781.5890 - val_loss: 1534753417.6438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 715/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1140132179.7260 - val_loss: 1532364742.1370\n",
      "Epoch 716/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1141508084.6027 - val_loss: 1530145058.8493\n",
      "Epoch 717/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1139437725.8082 - val_loss: 1539722569.6438\n",
      "Epoch 718/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1143556915.2877 - val_loss: 1541658609.9726\n",
      "Epoch 719/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1141359146.0822 - val_loss: 1554529084.4932\n",
      "Epoch 720/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1132670735.7808 - val_loss: 1540647592.3288\n",
      "Epoch 721/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1140019473.5342 - val_loss: 1551925051.6164\n",
      "Epoch 722/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1130706673.9726 - val_loss: 1538585926.1370\n",
      "Epoch 723/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1132904182.3562 - val_loss: 1533185276.4932\n",
      "Epoch 724/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1133803961.8630 - val_loss: 1565309608.3288\n",
      "Epoch 725/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1131521070.4658 - val_loss: 1543970916.8219\n",
      "Epoch 726/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1136348225.7534 - val_loss: 1549594222.6849\n",
      "Epoch 727/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1136291751.4521 - val_loss: 1570940553.6438\n",
      "Epoch 728/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1132085025.3151 - val_loss: 1547524140.1644\n",
      "Epoch 729/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1126858801.9726 - val_loss: 1552664928.4384\n",
      "Epoch 730/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1131058841.4247 - val_loss: 1555326111.5616\n",
      "Epoch 731/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1125626245.2603 - val_loss: 1522897506.1918\n",
      "Epoch 732/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1123882775.6712 - val_loss: 1561246979.5068\n",
      "Epoch 733/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1124056187.6164 - val_loss: 1539635553.3151\n",
      "Epoch 734/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1123374319.3425 - val_loss: 1540561039.7808\n",
      "Epoch 735/800\n",
      "1168/1168 [==============================] - 0s 42us/sample - loss: 1126065319.4521 - val_loss: 1539128641.7534\n",
      "Epoch 736/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1125246254.9041 - val_loss: 1555587222.7945\n",
      "Epoch 737/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1124229146.3014 - val_loss: 1515386147.0685\n",
      "Epoch 738/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1118662332.4932 - val_loss: 1543960800.4384\n",
      "Epoch 739/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1125266245.4795 - val_loss: 1526631869.3699\n",
      "Epoch 740/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1124102938.7397 - val_loss: 1524658204.0548\n",
      "Epoch 741/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1118711528.7671 - val_loss: 1544573916.9315\n",
      "Epoch 742/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1120055460.8219 - val_loss: 1532161879.3425\n",
      "Epoch 743/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1120824744.3288 - val_loss: 1558143448.5479\n",
      "Epoch 744/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1118239841.3151 - val_loss: 1550394068.6027\n",
      "Epoch 745/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1115984888.1096 - val_loss: 1540915230.6849\n",
      "Epoch 746/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1118998153.2055 - val_loss: 1537385116.9315\n",
      "Epoch 747/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1114530793.2055 - val_loss: 1531456352.4384\n",
      "Epoch 748/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1113177163.8356 - val_loss: 1544650132.1644\n",
      "Epoch 749/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1113904633.8630 - val_loss: 1556830279.8904\n",
      "Epoch 750/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1113227270.1370 - val_loss: 1545987159.6712\n",
      "Epoch 751/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1110019857.5342 - val_loss: 1532268873.6438\n",
      "Epoch 752/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1107317003.3973 - val_loss: 1532100196.8219\n",
      "Epoch 753/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1113280584.3288 - val_loss: 1526483683.9452\n",
      "Epoch 754/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1113324931.9452 - val_loss: 1548072031.5616\n",
      "Epoch 755/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1104671801.8630 - val_loss: 1520890800.6575\n",
      "Epoch 756/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1104385150.2466 - val_loss: 1537208320.0000\n",
      "Epoch 757/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1106310720.0000 - val_loss: 1525738225.5342\n",
      "Epoch 758/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1106904355.0685 - val_loss: 1567524253.2603\n",
      "Epoch 759/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1107013645.1507 - val_loss: 1522223556.8219\n",
      "Epoch 760/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1104065868.2740 - val_loss: 1512568021.9178\n",
      "Epoch 761/800\n",
      "1168/1168 [==============================] - 0s 33us/sample - loss: 1101205321.6438 - val_loss: 1535976370.8493\n",
      "Epoch 762/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1101756197.6986 - val_loss: 1543676317.8082\n",
      "Epoch 763/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1110226018.1918 - val_loss: 1512046700.7123\n",
      "Epoch 764/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1097408904.7671 - val_loss: 1541504905.5342\n",
      "Epoch 765/800\n",
      "1168/1168 [==============================] - 0s 41us/sample - loss: 1106127984.2192 - val_loss: 1517071729.0959\n",
      "Epoch 766/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1104100787.2877 - val_loss: 1572036122.3014\n",
      "Epoch 767/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1115595330.6301 - val_loss: 1521509027.7260\n",
      "Epoch 768/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1101411110.5753 - val_loss: 1537118409.6438\n",
      "Epoch 769/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1096350368.8767 - val_loss: 1543419813.4795\n",
      "Epoch 770/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1096880167.8904 - val_loss: 1543742361.6438\n",
      "Epoch 771/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1092591650.3014 - val_loss: 1538557455.5616\n",
      "Epoch 772/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1091011539.2877 - val_loss: 1545426590.6849\n",
      "Epoch 773/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1101435400.7671 - val_loss: 1547817678.0274\n",
      "Epoch 774/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1099953250.6301 - val_loss: 1536812454.1370\n",
      "Epoch 775/800\n",
      "1168/1168 [==============================] - 0s 40us/sample - loss: 1093480367.7808 - val_loss: 1522691008.2192\n",
      "Epoch 776/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1100090200.9863 - val_loss: 1565089830.5753\n",
      "Epoch 777/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1091516139.8356 - val_loss: 1529879159.1233\n",
      "Epoch 778/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1093868935.8904 - val_loss: 1527576231.4521\n",
      "Epoch 779/800\n",
      "1168/1168 [==============================] - 0s 39us/sample - loss: 1093094874.0822 - val_loss: 1543883206.1370\n",
      "Epoch 780/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1087916778.9589 - val_loss: 1528900981.0411\n",
      "Epoch 781/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1088453045.4795 - val_loss: 1546308543.1233\n",
      "Epoch 782/800\n",
      "1168/1168 [==============================] - 0s 38us/sample - loss: 1089377753.4247 - val_loss: 1538354065.5342\n",
      "Epoch 783/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1090998040.9863 - val_loss: 1514381813.4795\n",
      "Epoch 784/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1088482024.3288 - val_loss: 1522993643.6164\n",
      "Epoch 785/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1082319282.8493 - val_loss: 1536284772.3836\n",
      "Epoch 786/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1093770299.6164 - val_loss: 1523662970.3014\n",
      "Epoch 787/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1090150470.1370 - val_loss: 1513257397.6986\n",
      "Epoch 788/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1081216677.0411 - val_loss: 1526141027.5068\n",
      "Epoch 789/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1082305326.4658 - val_loss: 1505468710.5753\n",
      "Epoch 790/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1078491736.5479 - val_loss: 1535913741.5890\n",
      "Epoch 791/800\n",
      "1168/1168 [==============================] - 0s 37us/sample - loss: 1082267840.0000 - val_loss: 1511608390.1370\n",
      "Epoch 792/800\n",
      "1168/1168 [==============================] - 0s 36us/sample - loss: 1083925084.4932 - val_loss: 1531598163.2877\n",
      "Epoch 793/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1090364508.9315 - val_loss: 1527284503.4521\n",
      "Epoch 794/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1080677543.4521 - val_loss: 1512348938.6027\n",
      "Epoch 795/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1073983848.3288 - val_loss: 1531057727.1233\n",
      "Epoch 796/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1085275191.2329 - val_loss: 1512191551.1233\n",
      "Epoch 797/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1084662329.8630 - val_loss: 1538073700.6027\n",
      "Epoch 798/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1084247977.6438 - val_loss: 1564119332.8219\n",
      "Epoch 799/800\n",
      "1168/1168 [==============================] - 0s 35us/sample - loss: 1073830871.6712 - val_loss: 1507022815.1233\n",
      "Epoch 800/800\n",
      "1168/1168 [==============================] - 0s 34us/sample - loss: 1082049513.6438 - val_loss: 1511535192.3288\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=800,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32681.221687062192"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38878.466534954656"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5hcVZ3v//d316Vv6Vy7gVyARFCEAHIJCMPRA1646YA+ogcZR5mLceZ4nZ94hPn9RHGe8ajH43gcZmBwYARRRgYdDzIwAgIjOgJ2MGAgwQQJ0CSQJvf0tWrv7++PtbtTqVSnq5Pqrq7O5/U89dSutW/fqq7+rrVX7b22uTsiItL4onoHICIitaGELiIyTSihi4hME0roIiLThBK6iMg0oYQuIjJNKKGLNDgzczM7ut5xSP0poU9jZrbezN5Wx/3fYGbLK5R/IU1Cnygr/1Ra/oVJC3L3vv+Lmf2nmW03sy1m9gszO22y46g1M3vIzAbMbFfJ48f1jksmhhK6TKTzgbtHmfdb4ENlZR9MyyeVmc0E7gL+FpgLLASuAQbrEEtmAjb7MXefUfL4/VH2na2mbF/Gu7zUlhL6QcrMPmxm69LW6J1mtiAtNzP7GzPblLZWnzSz49N5F5rZ02a208xeMrMr9rH9E4Ft7t49yiK/AlrNbGm6/FKgJS0v3c47zWylmW1LW9Anlsy70syeTeN52szeXTLvcjP7uZl9zcy2mtlzZnbBKLG8DsDdb3P32N373f1ed38y3VYm3c6rZvY7M/toeiSRTefvcSSUHoHcWvL6X8zs5fTz/Nnwe07nfdvMrjOzu82sFzjHzJrS/b1gZq+Y2fVm1lKyzmfMbKOZbTCzPx7tbzAWMzvbzLrN7LNm9jLwT5XK0mUrfl/SeZ5+JmuBtfsbjxw4JfSDkJm9BfifwPuA+cDzwD+ns88F3kxIcrOB/wZsTufdCHzE3duB44EH9rGbC4F/GyOU7xBa5RBa67eUxXkKcBPwEWAe8A/AnWbWlC7yLPAmYBahRX2rmc0v2cQbgWeADuCrwI1mZhXi+C0Qm9nNZnaBmc0pm/9h4J3AycAy4JIx3le5e4DXAocAjwPfLZt/GfDXQDvwc+ArhM//JOBowhHD1QBmdj5wBfD2dJsH2qV2GOGo5EhgeaWyMb4vw95F+LyPO8B45EC4e90ehH/WTcCqKpZ9M+GfoQhcUjbvQ4SWwVrgQ/V8T1PpAawH3lah/EbgqyWvZwAFYDHwFkKCOwOIytZ7gZBcZ1ax74eBN40y7wvArcAR6TZz6fPhafkX0uWuA/6qbN1ngP86ynZXAhen05cD60rmtQIOHDbKuscC3wa60+/YncCh6bwHgD8rWfbcdFvZSp/z8PsbZT+z03Vnpa+/DdxSMt+AXuCokrIzgefS6ZuAL5fMe126vaNH2d9DQB+wreTxV+m8s4EhoLlk+Uplo35f0tcOvKXe33c9vO4t9G8T+lmr8QLhn/R7pYVmNhf4PKF1cDrw+QotLNnTAkIrCwB330VohS909weAa4G/A15Jf9icmS76HkLL+3kz+w8zO7PSxs1sNvB64D/3FYS7vwCsA74ErHX3F8sWORL4dNrdss3MthGS/nD30AdLumO2EY4aOkrWf7lkX33p5IxRYlnt7pe7+6J0OwuAb6SzFwClsT1fvv5o0u6aL6ddQzsIyZ+yOEu33UmofFaUvK9/T8v3N5ZPuPvsksfnSub1uPtA2fLlZaN+X0Z5D1IndU3o7v4zYEtpmZkdZWb/bmYrzOxhM3t9uux6D32aSdlmzgPuc/ct7r4VuI/qK4mD1QZCsgTAzNoIXRovAbj7N939VGApoQX4mbT8V+5+MaHr4EfA7aNs/zzgp+4eVxHLLcCnKetuSb0I/HVZMmp199vM7EjgW8DHgHnuPhtYRWjhHhB3X0NobByfFm0kVCTDjihbpZeQhIcdVjJ9GXAxoWtkFuEoiLI4S4c8fRXoB5aWvOdZ7j5cEY0Vy3hVGm61vGyf35d9bEcmWb1b6JXcAHw8TShXAH8/xvIL2bN10M2eLYeDXc7MmkseWcJRzh+Z2Ulpf/SXgEfdfb2ZnWZmbzSzHCFRDRD6l/Nm9gdmNsvdC8AOYLSE/Q5GP7ul3PcJXRiVKodvAX+WxmNm1mZm7zCzdqCNkER6AMzsj9idgMfFzF5vZp82s0Xp68OB9wOPpIvcDnzCzBalR39Xlm1iJXCpmeXMrLyPvZ1wtsxmQtL/0r5icfckfd9/Y2aHpPEsNLPzSmK53MyOM7NWwtHpRBv1+zIJ+5ZxmFIJ3cxmAL8H/IuZrST8CDZ/32tVbJGptbDb3YQW3/DjC+7+U+BzwA8ILb6jgEvT5WcSEspWwmH2ZuBr6bw/BNanXQd/BnygfGfpj45vJ3QTjMnDGSX3u3t/hXldhB8kr03jWUfodsPdnwb+N/BL4BXgBOAX1eyzgp2ELrtH0zNNHiG09j+dzv8W8BPgCcLvOD8sW/9zhM9wK+HH2dJuwVsIn+NLwNPsriT25bOE9/pI+lnfDxwD4O73ELqCHkiX2dcP08OutT3PQ19RxTojxvi+yBRi7vXNfWa2GLjL3Y9P+2qfcfdRk7iZfTtd/o709fuBs939I+nrfwAecvfbJjp22ZuZnQ5c6+6n1zuWiZJ+Z58Dcu5erG80IrtNqRa6u+8AnjOz98LIOdFvGGO1nwDnmtmc9HD43LRM6mcyugFEpExdW+hmdhvhNKkOwmHz5wmHkNcRulpywD+7+xctXIb9r8AcQr/uy+4+fFHKHwN/mW72r939nybzfcjBRS10marq3uUiIiK1MaW6XEREZP/VbSCdjo4OX7x4cb12LyLSkFasWPGqu3dWmle3hL548WK6urrqtXsRkYZkZqNeHawuFxGRaUIJXURkmlBCFxGZJnR3ERFpKIVCge7ubgYGygeJnF6am5tZtGgRuVyu6nWU0EWkoXR3d9Pe3s7ixYupfL+SxufubN68me7ubpYsWVL1eupyEZGGMjAwwLx586ZtMgcwM+bNmzfuoxAldBFpONM5mQ/bn/fYeAn9lafhp38FfVvGXlZE5CDSeAl9y7Pw8Ndgu+54JSKTb9u2bfz934913529XXjhhWzbtm0CItqt8RJ667zwrBa6iNTBaAk9jvd9x8W7776b2bNnT1RYwDgSenqz21+b2V0V5jWZ2ffNbJ2ZPZoOLzoxRhL65gnbhYjIaK688kqeffZZTjrpJE477TTOOeccLrvsMk444QQA3vWud3HqqaeydOlSbrjhhpH1Fi9ezKuvvsr69es59thj+fCHP8zSpUs599xz6e/f64Zd+2U8py1+ElhNuEVZuT8Btrr70WZ2KfAV4L/VIL69qYUuIqlrfvwUT2/YUdNtHrdgJp///aWjzv/yl7/MqlWrWLlyJQ899BDveMc7WLVq1cjphTfddBNz586lv7+f0047jfe85z3Mmzdvj22sXbuW2267jW9961u8733v4wc/+AEf+MBed3Qct6pa6OnNc98B/OMoi1wM3JxO3wG81SbqZ+jm9JClXwldROrv9NNP3+Nc8W9+85u84Q1v4IwzzuDFF19k7dq1e62zZMkSTjrpJABOPfVU1q9fX5NYqm2hfwP4H4Q7mFeyEHgRwN2LZrYdmAe8esARlstkoWkm9E/sjwsiMvXtqyU9Wdra2kamH3roIe6//35++ctf0traytlnn13xXPKmpqaR6UwmU7MulzFb6Gb2TmCTu+/rTuGVWuN73QrJzJabWZeZdfX09IwjzDK5Vij07f/6IiL7qb29nZ07d1act337dubMmUNraytr1qzhkUcemdTYqmmhnwVcZGYXAs3ATDO71d1LO3y6gcOBbjPLArOAvfpE3P0G4AaAZcuW7f+973ItSugiUhfz5s3jrLPO4vjjj6elpYVDDz10ZN7555/P9ddfz4knnsgxxxzDGWecMamxjZnQ3f0q4CoAMzsbuKIsmQPcCXwI+CVwCfCAT+TNSvNtUKjNIYqIyHh973vfq1je1NTEPffcU3HecD95R0cHq1atGim/4oorahbXfg/OZWZfBLrc/U7gRuA7ZraO0DK/tEbxVZZrgaHeCd2FiEijGVdCd/eHgIfS6atLygeA99YysH3KtaqFLiJSpvGuFIU0oauFLiJSqkETeota6CIiZRozoedbYUhnuYiIlGrMhK7z0EVE9tJwCf3l7QM8u7WAx4V6hyIiB6H9HT4X4Bvf+AZ9fRPXGG24hL7i+a38++rNEA/VOxQROQhN5YTecDeJ7mxvYi1ZLCmAOxwEt6ISkamjdPjct7/97RxyyCHcfvvtDA4O8u53v5trrrmG3t5e3ve+99Hd3U0cx3zuc5/jlVdeYcOGDZxzzjl0dHTw4IMP1jy2hkzoQ56GHRcgm69vQCJSP/dcCS//prbbPOwEuODLo84uHT733nvv5Y477uCxxx7D3bnooov42c9+Rk9PDwsWLODf/u3fgDDGy6xZs/j617/Ogw8+SEdHR21jTjVcl0tnexOF4XpI3S4iUkf33nsv9957LyeffDKnnHIKa9asYe3atZxwwgncf//9fPazn+Xhhx9m1qxZkxJPw7XQ2/IZyOTCCyV0kYPbPlrSk8Hdueqqq/jIRz6y17wVK1Zw9913c9VVV3Huuedy9dVXV9hCbTVcC93MyGTTsYR1pouITLLS4XPPO+88brrpJnbt2gXASy+9xKZNm9iwYQOtra184AMf4IorruDxxx/fa92J0HAtdIBsvgkGUAtdRCZd6fC5F1xwAZdddhlnnnkmADNmzODWW29l3bp1fOYznyGKInK5HNdddx0Ay5cv54ILLmD+/Pn6UXSYErqI1FP58Lmf/OQn93h91FFHcd555+213sc//nE+/vGPT1hcDdflApDNNYcJdbmIiIxoyISeG74fn1roIiIjGjKhN+X1o6jIwWwib4g2VezPe6zmJtHNZvaYmT1hZk+Z2TUVlrnczHrMbGX6+NNxRzIO2bxa6CIHq+bmZjZv3jytk7q7s3nzZpqbm8e1XjU/ig4Cb3H3XWaWA35uZve4e/ntrL/v7h8b1973U5RVQhc5WC1atIju7m56enrqHcqEam5uZtGiReNap5qbRDuwK32ZSx91rRqzuZDQ48IgmXoGIiKTLpfLsWTJknqHMSVV1YduZhkzWwlsAu5z90crLPYeM3vSzO4ws8NH2c5yM+sys64DqV0zuTB+S2FoYL+3ISIy3VSV0N09dveTgEXA6WZ2fNkiPwYWu/uJwP3AzaNs5wZ3X+buyzo7O/c76Gw2XPpfKBb3exsiItPNuM5ycfdtwEPA+WXlm919MH35LeDUmkQ3imzaQi8W1IcuIjKsmrNcOs1sdjrdArwNWFO2zPySlxcBq2sZZLl8PrTQiwWdtigiMqyas1zmAzebWYZQAdzu7neZ2ReBLne/E/iEmV0EFIEtwOUTFTBAbqTLRQldRGRYNWe5PAmcXKH86pLpq4Crahva6HK5tIWuPnQRkRENeaVoLu1Dj9VCFxEZ0ZAJfbgPXQldRGS3xkzow2e5KKGLiIxoyISezYWu/ySO6xyJiMjU0ZAJPZMNLfREoy2KiIxoyISey4YRXDzWWS4iIsMaMqEPt9BJlNBFRIY1ZELPZdM+9ER96CIiwxoyoWezWWI3UJeLiMiIxkzoGSMmwtXlIiIyoiETei6KiMmoD11EpERDJvRsxiiSwdWHLiIyojETehS6XFBCFxEZ0ZAJ3cxIiNTlIiJSoiETOkCRjFroIiIlGjahJ/pRVERkD9Xcgq7ZzB4zsyfM7Ckzu6bCMk1m9n0zW2dmj5rZ4okItlRs6nIRESlVTQt9EHiLu78BOAk438zOKFvmT4Ct7n408DfAV2ob5t4SdbmIiOxhzITuwa70ZS59eNliFwM3p9N3AG81M6tZlBUklgFXQhcRGVZVH7qZZcxsJbAJuM/dHy1bZCHwIoC7F4HtwLwK21luZl1m1tXT03NAgcdkMFeXi4jIsKoSurvH7n4SsAg43cyOL1ukUmu8vBWPu9/g7svcfVlnZ+f4oy3dlkWYulxEREaM6ywXd98GPAScXzarGzgcwMyywCxgSw3iGz0Wi9TlIiJSopqzXDrNbHY63QK8DVhTttidwIfS6UuAB9x9rxZ6LSVkME8mchciIg0lW8Uy84GbzSxDqABud/e7zOyLQJe73wncCHzHzNYRWuaXTljEKbcIUwtdRGTEmAnd3Z8ETq5QfnXJ9ADw3tqGtm+JRWqhi4iUaNgrRZ2MWugiIiUaN6FbhKEWuojIsIZO6JG6XERERjRwQleXi4hIqcZN6KiFLiJSqnETumXUhy4iUqKhE3qkLhcRkRENnNB1louISKmGTehEGfWhi4iUaNiE7hYRoS4XEZFhDZzQM0TqchERGdGwCR3TaIsiIqUaNqGHLhcldBGRYQ2b0IkyZJTQRURGNG5CVx+6iMgeqrlj0eFm9qCZrTazp8zskxWWOdvMtpvZyvRxdaVt1ZQG5xIR2UM1dywqAp9298fNrB1YYWb3ufvTZcs97O7vrH2Io4jUQhcRKTVmC93dN7r74+n0TmA1sHCiAxuTqQ9dRKTUuPrQzWwx4XZ0j1aYfaaZPWFm95jZ0hrEtm9qoYuI7KGaLhcAzGwG8APgU+6+o2z248CR7r7LzC4EfgS8tsI2lgPLAY444oj9DjpsLLTQk8SJIjuwbYmITANVtdDNLEdI5t919x+Wz3f3He6+K52+G8iZWUeF5W5w92Xuvqyzs/MAIw8t9Nj9wLYjIjJNVHOWiwE3Aqvd/eujLHNYuhxmdnq63c21DHQvUUSGhDhRQhcRgeq6XM4C/hD4jZmtTMv+EjgCwN2vBy4B/tzMikA/cKn7xDadzTJE5hTjBHKZidyViEhDGDOhu/vPgX12Urv7tcC1tQqqKlEIPY6LQG5Sdy0iMhU17pWiUQg9LhbrHIiIyNTQsAnd9mihi4hIwyZ0LPSbJ7FuciEiAg2c0KMoJPRiXKhzJCIiU0PDJnTShO5FtdBFRKCBE7qNtNDVhy4iAo2c0DNpCz1RQhcRgUZO6GkLPdaPoiIiwDRI6Im6XEREgGmR0NVCFxGBhk7ourBIRKRUwyb0aPhHUbXQRUSABk7oIz+K6iwXERFgGiR0tdBFRIKGTehRRn3oIiKlGjahj7TQE7XQRUSgulvQHW5mD5rZajN7ysw+WWEZM7Nvmtk6M3vSzE6ZmHB3y6QtdJ22KCISVHMLuiLwaXd/3MzagRVmdp+7P12yzAXAa9PHG4Hr0ueJowuLRET2MGYL3d03uvvj6fROYDWwsGyxi4FbPHgEmG1m82sebYnhFrq7WugiIjDOPnQzWwycDDxaNmsh8GLJ6272TvqY2XIz6zKzrp6envFFWibKhNDV5SIiElSd0M1sBvAD4FPuvqN8doVVfK8C9xvcfZm7L+vs7BxfpGWiKNwYWqMtiogEVSV0M8sRkvl33f2HFRbpBg4veb0I2HDg4Y1OV4qKiOypmrNcDLgRWO3uXx9lsTuBD6Znu5wBbHf3jTWMc++4hs9yUQtdRASo7iyXs4A/BH5jZivTsr8EjgBw9+uBu4ELgXVAH/BHtQ91T5mRG1wkE70rEZGGMGZCd/efU7mPvHQZBz5aq6CqEaWjLaoPXUQkaNgrRTPqQxcR2UPDJvQoO9xCV0IXEYFGTugay0VEZA8Nm9B3XymqPnQREWjghD48fC7qQxcRARo4oQ//KJqoy0VEBGjghD7cQjcNziUiAjRwQsfUQhcRKdW4CT09y8WU0EVEgEZO6KZL/0VESjVuQo/S0NWHLiICNHJCN11YJCJSqnETetqHjhK6iAjQyAk9baGry0VEJGjchK4WuojIHho3oactdHOd5SIiAtXdgu4mM9tkZqtGmX+2mW03s5Xp4+rah1lBepaLq8tFRASo7hZ03wauBW7ZxzIPu/s7axLROMREurBIRCQ1Zgvd3X8GbJmEWMYtJtKPoiIiqVr1oZ9pZk+Y2T1mtnS0hcxsuZl1mVlXT0/PAe80IaOELiKSqkVCfxw40t3fAPwt8KPRFnT3G9x9mbsv6+zsPOAdJxZhuvRfRASoQUJ39x3uviudvhvImVnHAUdWzb7V5SIiMuKAE7qZHWZmlk6fnm5z84FutxoJkcZDFxFJjXmWi5ndBpwNdJhZN/B5IAfg7tcDlwB/bmZFoB+41N19wiIukVgEOg9dRASoIqG7+/vHmH8t4bTGSZeQUQtdRCTVuFeKAm6RrhQVEUk1dEJXH7qIyG4NndDVQhcR2a2hE3pi6kMXERnW0Ald56GLiOzW0Akdy4CuFBURARo8oXukFrqIyLCGTuiYBucSERnW2Ak9ymhwLhGRVEMndEvPcomTSRlpQERkSmvohE6UISJhsKhuFxGRhk/oGRIGCup2ERFp6IRuUYaMqYUuIgLTIKFHaqGLiAANntCjkS4XtdBFRBo6oVsmS0RCvxK6iMjYCd3MbjKzTWa2apT5ZmbfNLN1ZvakmZ1S+zAry2ZCC317X2GydikiMmVV00L/NnD+PuZfALw2fSwHrjvwsKqTzWbJ4GztG5qsXYqITFljJnR3/xmwZR+LXAzc4sEjwGwzm1+rAPcln8sRkbClVwldRKQWfegLgRdLXnenZXsxs+Vm1mVmXT09PQe842w2R5aYbepyERGpSUK3CmUVr8V39xvcfZm7L+vs7DzwHedaaI0KbFYLXUSkJgm9Gzi85PUiYEMNtju2fCttNsimHQOTsjsRkamsFgn9TuCD6dkuZwDb3X1jDbY7tnwbzQyycbsSuohIdqwFzOw24Gygw8y6gc8DOQB3vx64G7gQWAf0AX80UcHuJddGkw/y8rbeSduliMhUNWZCd/f3jzHfgY/WLKLxyLcCMNDfy9beIea05esShojIVNDQV4qSCwm9lUHWvLyzzsGIiNRXYyf0fBsALTbAmpd31DkYEZH6mhYJfUFLzJqNaqGLyMGtwRP6DACWdmR4ontbnYMREamvxk7orXMBOLUjZs3LOzUEgIgc1Bo8oXcAsHROuPT/0d9trmc0IiJ11dgJvS0k9MPzfcxoyvIfvz3w8WFERBpVYyf0XAvk2sj0b+Etrz+E+55+hTipOIyMiMi019gJHaBtHvS9yvnHH8bm3iEefU7dLiJycGr8hN7aAb2vcs4xh9CWz/D9X7049joiItNQ4yf0tg7oe5WWfIbL3ngEdz6xgU07NViXiBx8Gj+ht86D3tDN8q6TF+IOP129qc5BiYhMvsZP6G0d0NsD7hw3fybHHNrOrY88TxgzTETk4NH4CX32kRAPws6NmBkf/L0jeWrDDrqe31rvyEREJlXjJ/R5R4fnzesAePfJC5ndmuPGh5+rY1AiIpOvqoRuZueb2TNmts7Mrqww/3Iz6zGzlenjT2sf6ijKEnprPssfvPEIfvL0y7ywuW/SwhARqbcxE7qZZYC/Ay4AjgPeb2bHVVj0++5+Uvr4xxrHObqZCyHbDJufHSn64JmLyUbGP/78d5MWhohIvVXTQj8dWOfuv3P3IeCfgYsnNqxxiCKYexT0PDNSdOjMZt677HC+++gLGiddRA4a1ST0hUDp1TrdaVm595jZk2Z2h5kdXpPoqnXEG+H5/4TC7vPPP3PuMcxszvL//esqEg0HICIHgWoSulUoK8+QPwYWu/uJwP3AzRU3ZLbczLrMrKunp4YDaR3zDij0wnP/MVI0py3PVRccS9fzW/nB492125eIyBRVTULvBkpb3IuADaULuPtmdx9MX34LOLXShtz9Bndf5u7LOjs79yfeypa8CfLtsPrOPYovOXURpx45hy/dvZoN2/prtz8RkSmomoT+K+C1ZrbEzPLApcAemdPM5pe8vAhYXbsQq5BtgmPOh1/fCi8+NlIcRcZXLzmRQuz82a0rGCjEkxqWiMhkGjOhu3sR+BjwE0Kivt3dnzKzL5rZRelinzCzp8zsCeATwOUTFfCozvuf0Dwb7vhjGNj9Q+hRnTP4+vvewJPd2/ncj1bpClIRmbasXglu2bJl3tXVVduNPvczuPn3Ycmb4YL/BYe8fmTW/773Gf72gXV85rxj+Og5R9d2vyIik8TMVrj7skrzGv9K0VJL3gxnfiwk9r8/A7pXjMz6i7e9jotPWsD/+skz3PLL9XULUURkokyvhA5w3l/DZbeDRfDtC+EX/weKg0SR8bX3voG3HXsoV//fp/inX2hoABGZXqZfQgd43XnwF6tg4TK472r421Ph8VvIeZFrLzuZ85YeyjU/fpprfvyUfigVkWljevWhV/Lbe+GhL8GGX0PLXHjd+cSvOYevrlvEP/xqK0s62vjqJSdy2uK5Ex+LiMgB2lcf+vRP6ABJAmvugt/cDut+CoUwaFehaQ73Fk/mgYHXccQJZ/EHF76NjpmtkxOTiMh+UEIvVRiAl1bAE9+DLc/hG1ZihV4A+ryJYtNsmha/kaaj3gQLToZDjoWmGZMfp4hIBftK6NnJDqbucs2w+KzwACyJ4dXfsumZX/JU13/Qt2UjJz/zCxb8tuTaqeZZIbm3zIFDjoOO14Ybaxx2AmRydXojIiJ7Ovha6GNYt2kX3330eX795BPM713DEtvI65q3cXJ2PbOsl9kDL+1eOMpCa0cYkz3XAjMOhUOXQvth0HkMzFwQKgERkRpRl8t+SBLn6Y07eOy5Lfxq/Rae3riDF7b00e69LLRXWWIbWdb0Iq/JbuYYf46ZvpMogpbCtj230zIXa5mLZfPh/qft88MVrU3toSsnPwPybeAeKoY5i0OrP8qECiPXCklRRwIiAiih18xAIeZ3Pb2s39zLc6/28uKWPjbtHOTl7QNs3N7P1r4hOtnOsdHzHGUbyFFksb3CfNtMS1SkLRqig+2000er9xGRVL3vYtMssIgkPwtvnoUZmDvk27BsDsu1Eg3uwJraoWU2I4NkZrIw1BtuAtIyB5IY4qEw/k1xAGYcFp5b5wEOUS6MXOkOnoTz+dvng6XbS4rQtyVUNhaFbRb7YWB7GCCteSYM7QrrZ3Iw1BeOVHp7dldemTxk8+H3jF0vhy6tTFPYvsfQvy28h0xTeC4OhP0Wh3ZXdrk2GNwZjq/XqTAAAAxTSURBVIwy+bBecSBsM98W3s9Qb4gxiiAupEdLFuY3z4LBHWGd4iC0dYZlsk1he/lWGNwVKtSo7Oxe992fh8gkUx96jTTnMhy3YCbHLZhZcf5AIWZL7xCv7hqkZ+cgOwYK7Ogvsqq/wI6BAjsHiiNlO/uHyPRvZnCgj7ahTURJgXnsYI7tJEvMIbaNHEUOta0MkSPujSiQod36mUUvLTbIkGfJ2TayxLQyyDzbzi5vIW8xGRLcjFYGyFFkF23MYictDI7EO0ieJoYokiGLzsffg2VCJQHhdFdPwiOJQwWWyYdHEofkb1GoDJyQ7KNM2IZFgIfpHRtgxiHhd5x8e1phWqhImmaEyjTKhgrXLFROUSbsJ8qGm6HnWkPZ4M50n82hzONQqTa1h/L8jFBhNbWHSrWtI1TE7YeGs748DhVWy+xQ+SXFsH4mF8pb54aYiwOhki4Ohf3HQyHObHNoLERpBZvE4X3m22D7SzD78N3vPx4M+8zkQhwDO8J2cq27K8bhSjLfFuZlmiAphLiyLWEbMw4r+RxawnY8Dp9PPATNc8I6cbperiXElxRCBV5M75fQOi98JkkhxB1ldjcYmmakn3E2zB/YEdYd2JZ+D2Io9IcGQHEQdr0SPusoG7YztCt0wxb6wv6bZ4dtx4XQiOnftrs8U/v0q4ReQ825DAtmt7Bgdsu4100SZ7CYsGuwyEAhpm8opr8QM1iIGSgmDBUTinHCUJywOXaKScJQ7BTjhEKcUIidYuzpdEKcOMXEieOEYuIkDsXEKcYxJAU8LjLoWZIkZjDJkI93EccxmWI/W5lJQkSCkUv6aY13EidOkoT97kiamRf3sCVuJaJIv+dpSfoY8CyZpEDeB9jODFoZYJvPYK7tpEAGgBwxeQrkKJK3Iju9BcfIEVMgQxMFttLOXHYQ4cywfgpkQ55MP6uIhDbCP+cOWslRJCbDoOdwjLwVmE0vvTQR4WRIKJJhcbSJQtQMcYF5uQHcsuyyNpqsSLsNkFiGVgZpsgIQMYM+spZgDk4Gz0R4NkuUd5qiIrnI8CjLjGQnOYokloMoIoOTsYQMTuQxBuSLfQzMOZpc0k+2OEh+sA+3DIaTSQpEu3oAI5sMkkQ5sskgnmvDoyyWFIg8JjIjigfADM+1kin04ZkcUaEPiwcxINP7Mm4ZvGUO3jSTqPeVkEzj9CikfyuWyYdEm2sJFUNhX/feNUZuf2CZUAntdTuEfawjlb3p0/DWq2u+WSX0KSKKjJZ8hpZ8pt6h1ESSOLE7ceIjlUt5WZzeScodHKcQO0PFUGmFeY47JA7uoVKKk1BpDRaTkZEzE4ehOGaomBAnECehEgvLOkm6z4FCTKaY0JSL2D4Yhxjc6UucTSXLxT5ceVWOOU6coThhoBAzOJSMVKSJe1pphvdRSBLqNrhn2fD/EQlJf+g6ykRGxowogmwU0WqDNFlMMdNCcwbyhW0MtRxK1hIyBk1RTGx5LJOlyQrkIsejPHlLyHiRXOS0ZBI8k2NG0suu3FxmJjuIDLKRkzXHsk200k8+GWQw245HOZoYJO9DeCZPW3E7caaZrMU0x73EuTZy+WYcaPIBoihLs/eR8SKGkbUYzzaTxDG5piZIEvJRTDaTwzNZsCzNvd2QbyMywnq5FiJzcoPb8GwzRNlQQWZbsGwTucJOonwTUXEQPMFIiFrmkCnshObZRP1biOIBrDiAkYQjg4Ed4aijZU5ohfdvDUdquZbdr4uD4ajMMmE6KcARZ07In10JXSZEFBkRRm561E8HxNNEP1QMST9JCJWHh0rE04qqdN7w0dqwxJ2+oThUVnFJJVle2ZRURvHI/FDJxQlp5RSmEw+VT6iIQiU6VHRymQX0DsUkJdvf/ZynmIQjwgHPADmKxYShwRCXezuxD5EkTSPvMY6dQpJQiHNAjiRJSHww3WaWxGMSn5FW4hkgP/yu0+fh1+O5HiQBFowyr61kelbJdPo7Du1lyw+XHzJSYgaRhYpxZDoyzDpGpkOFdgSZaO/fWz5gR/DnR43j7VRJCV1kgpkZuYyRy0zPoZNqabgSKSZO/1CMGSNHPYU4JPjSSswMBgoJkRkDxXiksnMc0m7G4Uql9EjLoeQIL1R6w92Vw+J0v8PrlR4l+kiFTFoR7z1dWplZ2Z08j5g7MVekK6GLyJQxfGSXzYTfpGR8qmoymNn5ZvaMma0zsysrzG8ys++n8x81s8W1DlRERPZtzIRuZhng74ALgOOA95vZcWWL/Qmw1d2PBv4G+EqtAxURkX2rpoV+OrDO3X/n7kPAPwMXly1zMXBzOn0H8FYzXXkhIjKZqknoC4EXS153p2UVl0lvKr0dmFe+ITNbbmZdZtbV09OzfxGLiEhF1ST0Si3t8rNrq1kGd7/B3Ze5+7LOzs5q4hMRkSpVk9C7gcNLXi8CNoy2jJllCSd3bqlFgCIiUp1qEvqvgNea2RIzywOXAneWLXMn8KF0+hLgAa/XqF8iIgepMc9Dd/eimX0M+AnhMq6b3P0pM/si0OXudwI3At8xs3WElvmlExm0iIjsrW7D55pZD/D8fq7eAbxaw3BqaarGprjGR3GNj+IanwOJ60h3r/gjZN0S+oEws67RxgOut6kam+IaH8U1PoprfCYqLg0uISIyTSihi4hME42a0G+odwD7MFVjU1zjo7jGR3GNz4TE1ZB96CIisrdGbaGLiEgZJXQRkWmi4RL6WGOzT/C+bzKzTWa2qqRsrpndZ2Zr0+c5abmZ2TfTOJ80s1MmMK7DzexBM1ttZk+Z2SenQmxm1mxmj5nZE2lc16TlS9Jx89em4+jn0/JJHVffzDJm9mszu2uqxGVm683sN2a20sy60rKp8B2bbWZ3mNma9Ht2Zr3jMrNj0s9p+LHDzD5V77jSff1F+p1fZWa3pf8LE//9cveGeRCuVH0WeA3hRoNPAMdN4v7fDJwCrCop+ypwZTp9JfCVdPpC4B7CwGVnAI9OYFzzgVPS6Xbgt4Sx6+saW7r9Gel0Dng03d/twKVp+fXAn6fT/x24Pp2+FPj+BP89/x/ge8Bd6eu6xwWsBzrKyqbCd+xm4E/T6TwweyrEVRJfBngZOLLecRFGn30OaCn5Xl0+Gd+vCf2QJ+CDOhP4Scnrq4CrJjmGxeyZ0J8B5qfT84Fn0ul/AN5fablJiPH/Am+fSrEBrcDjwBsJV8hly/+mhOElzkyns+lyNkHxLAJ+CrwFuCv9J58Kca1n74Re178jMDNNUDaV4iqL5VzgF1MhLnYPJz43/b7cBZw3Gd+vRutyqWZs9sl2qLtvBEifh28NXpdY08O1kwmt4brHlnZrrAQ2AfcRjrC2eRg3v3zfVY2rXyPfAP4Hu28tP2+KxOXAvWa2wsyWp2X1/ju+BugB/intovpHM2ubAnGVuhS4LZ2ua1zu/hLwNeAFYCPh+7KCSfh+NVpCr2rc9Sli0mM1sxnAD4BPufuOfS1aoWxCYnP32N1PIrSITweO3ce+JyUuM3snsMndV5QW1zuu1Fnufgrhlo8fNbM372PZyYorS+hqvM7dTwZ6CV0Z9Y4r7Cz0RV8E/MtYi1Yom4jv1xzCXdyWAAuANsLfc7R91yyuRkvo1YzNPtleMbP5AOnzprR8UmM1sxwhmX/X3X84lWIDcPdtwEOEvsvZFsbNL9/3ZI2rfxZwkZmtJ9xS8S2EFnu948LdN6TPm4B/JVSC9f47dgPd7v5o+voOQoKvd1zDLgAed/dX0tf1juttwHPu3uPuBeCHwO8xCd+vRkvo1YzNPtlKx4L/EKH/erj8g+kv62cA24cPA2vNzIwwhPFqd//6VInNzDrNbHY63UL4oq8GHiSMm18prgkfV9/dr3L3Re6+mPAdesDd/6DecZlZm5m1D08T+oVXUee/o7u/DLxoZsekRW8Fnq53XCXez+7uluH91zOuF4AzzKw1/d8c/rwm/vs1kT9UTMSD8Ev1bwl9sf/vJO/7NkKfWIFQq/4Joa/rp8Da9HluuqwBf5fG+Rtg2QTG9V8Ih2hPAivTx4X1jg04Efh1Gtcq4Oq0/DXAY8A6wmFyU1renL5el85/zST8Tc9m91kudY0r3f8T6eOp4e93vf+O6b5OArrSv+WPgDlTJK5WYDMwq6RsKsR1DbAm/d5/B2iajO+XLv0XEZkmGq3LRURERqGELiIyTSihi4hME0roIiLThBK6iMg0oYQuIjJNKKGLiEwT/z+OOd6FfcCegAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss / Mean Squared Error')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1461</td>\n",
       "      <td>141324.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1462</td>\n",
       "      <td>155842.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1463</td>\n",
       "      <td>205419.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1464</td>\n",
       "      <td>198624.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1465</td>\n",
       "      <td>168321.890625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id      SalePrice\n",
       "0  1461  141324.343750\n",
       "1  1462  155842.218750\n",
       "2  1463  205419.796875\n",
       "3  1464  198624.328125\n",
       "4  1465  168321.890625"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sale_price_keras = model.predict(test_new)\n",
    "sub['SalePrice'] = sale_price_keras\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
