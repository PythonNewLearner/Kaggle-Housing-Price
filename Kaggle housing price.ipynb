{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train.drop(['SalePrice','Id'],axis=1)\n",
    "test = test.drop(['Id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 79)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 79)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DropMissingValue(data:pd,threshold = 0.3) -> pd: # drop missing value that exceed threshold of number of observations\n",
    "    df_null = data.isnull().sum()\n",
    "    null_index = df_null[df_null >= threshold*data.shape[0]].index\n",
    "    data = data.drop(null_index,axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = DropMissingValue(train_features)\n",
    "test_new = DropMissingValue(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new.columns == test_new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill nan with mean for float numbers except for year,fill nan with mode for categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_null = train_new.isnull().sum().sort_values(ascending=False) != 0\n",
    "train_null_idx = train_null[train_null].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_null = test_new.isnull().sum().sort_values(ascending=False) != 0\n",
    "test_null_idx = test_null[test_null].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LotFrontage     float64\n",
       "GarageFinish     object\n",
       "GarageType       object\n",
       "GarageCond       object\n",
       "GarageQual       object\n",
       "GarageYrBlt     float64\n",
       "BsmtExposure     object\n",
       "BsmtFinType2     object\n",
       "BsmtFinType1     object\n",
       "BsmtCond         object\n",
       "BsmtQual         object\n",
       "MasVnrArea      float64\n",
       "MasVnrType       object\n",
       "Electrical       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new[train_null_idx].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LotFrontage', 'GarageFinish', 'GarageType', 'GarageCond', 'GarageQual',\n",
       "       'GarageYrBlt', 'BsmtExposure', 'BsmtFinType2', 'BsmtFinType1',\n",
       "       'BsmtCond', 'BsmtQual', 'MasVnrArea', 'MasVnrType', 'Electrical'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_null_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new[['LotFrontage','MasVnrArea']] = train_new[['LotFrontage','MasVnrArea']].fillna(train_new.mean().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new[[ 'GarageCond', 'GarageType', 'GarageYrBlt','GarageFinish', 'GarageQual', 'BsmtFinType2', 'BsmtExposure',\n",
    "           'BsmtFinType1', 'BsmtCond', 'BsmtQual', 'MasVnrType','Electrical']] = train_new[[ 'GarageCond', 'GarageType', 'GarageYrBlt','GarageFinish', 'GarageQual', 'BsmtFinType2', 'BsmtExposure',\n",
    "           'BsmtFinType1', 'BsmtCond', 'BsmtQual', 'MasVnrType','Electrical']].fillna(train_new.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LotFrontage', 'GarageFinish', 'GarageCond', 'GarageQual',\n",
       "       'GarageYrBlt', 'GarageType', 'BsmtCond', 'BsmtQual', 'BsmtExposure',\n",
       "       'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MasVnrArea', 'MSZoning',\n",
       "       'Functional', 'BsmtHalfBath', 'BsmtFullBath', 'Utilities',\n",
       "       'Exterior2nd', 'Exterior1st', 'KitchenQual', 'TotalBsmtSF',\n",
       "       'GarageCars', 'SaleType', 'BsmtUnfSF', 'GarageArea', 'BsmtFinSF2',\n",
       "       'BsmtFinSF1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_null_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new[['LotFrontage','MasVnrArea','BsmtHalfBath','BsmtFullBath','BsmtUnfSF','BsmtFinSF2','BsmtFinSF1','TotalBsmtSF',\n",
    "          'GarageArea']] = test_new[['LotFrontage','MasVnrArea','BsmtHalfBath','BsmtFullBath','BsmtUnfSF','BsmtFinSF2','BsmtFinSF1','TotalBsmtSF',\n",
    "          'GarageArea']].fillna(train_new.mean().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new[[ 'GarageCond', 'GarageQual', 'GarageYrBlt',\n",
    "       'GarageFinish', 'GarageType', 'BsmtCond', 'BsmtQual', 'BsmtExposure',\n",
    "       'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSZoning', 'Utilities', 'Functional', 'BsmtUnfSF',\n",
    "       'SaleType', 'Exterior2nd', 'Exterior1st', 'GarageCars', 'KitchenQual']] = test_new[[ 'GarageCond', 'GarageQual', 'GarageYrBlt',\n",
    "       'GarageFinish', 'GarageType', 'BsmtCond', 'BsmtQual', 'BsmtExposure',\n",
    "       'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSZoning', 'Utilities', 'Functional', 'BsmtUnfSF',\n",
    "       'SaleType', 'Exterior2nd', 'Exterior1st', 'GarageCars', 'KitchenQual']].fillna(train_new.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_new,test_new])  #Combine train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dummies(data:pd) -> pd:  # dummy all categorical features\n",
    "    object_features = data.dtypes[df.dtypes == object].index\n",
    "    dummies = pd.get_dummies(data[object_features])\n",
    "    df_new = pd.concat([data,dummies],axis=1)\n",
    "    df_new.drop(columns=object_features,inplace=True)\n",
    "    df_new = df_new.loc[:,~df_new.columns.duplicated()]\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = Dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 270)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the train and test back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = df_new.iloc[:1460,:]\n",
    "test_new = df_new.iloc[1460:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 270 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_ConLw  \\\n",
       "0          2003       196.0       706.0         0.0  ...               0   \n",
       "1          1976         0.0       978.0         0.0  ...               0   \n",
       "2          2002       162.0       486.0         0.0  ...               0   \n",
       "3          1970         0.0       216.0         0.0  ...               0   \n",
       "4          2000       350.0       655.0         0.0  ...               0   \n",
       "\n",
       "   SaleType_New  SaleType_Oth  SaleType_WD  SaleCondition_Abnorml  \\\n",
       "0             0             0            1                      0   \n",
       "1             0             0            1                      0   \n",
       "2             0             0            1                      0   \n",
       "3             0             0            1                      1   \n",
       "4             0             0            1                      0   \n",
       "\n",
       "   SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n",
       "0                      0                     0                     0   \n",
       "1                      0                     0                     0   \n",
       "2                      0                     0                     0   \n",
       "3                      0                     0                     0   \n",
       "4                      0                     0                     0   \n",
       "\n",
       "   SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                     1                      0  \n",
       "1                     1                      0  \n",
       "2                     1                      0  \n",
       "3                     0                      0  \n",
       "4                     1                      0  \n",
       "\n",
       "[5 rows x 270 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.09193234],\n",
       "       [-0.09193234,  1.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(train_new['OverallQual'],train_new['OverallCond'])  \n",
    "# these two features looks correlated as names implies, but they are not correlated much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YearBuilt: Original construction date<br>\n",
    "YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)<br>\n",
    "MoSold: Month Sold (MM)<br>\n",
    "YrSold: Year Sold (YYYY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "House's life and remodel  may be a critical feature to price the house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do something with YearBuilt and YearRemodAdd and MoSold: Month Sold (MM) and YrSold: Year Sold (YYYY)\n",
    "train_new['LifeBuilt'] = train_new['MoSold']/12 + train_new['YrSold'] - train_new['YearBuilt']\n",
    "train_new['LifeRemod'] = train_new['MoSold']/12 + train_new['YrSold'] - train_new['YearRemodAdd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new['LifeBuilt'] = test_new['MoSold']/12 + test_new['YrSold'] - test_new['YearBuilt']\n",
    "test_new['LifeRemod'] = test_new['MoSold']/12 + test_new['YrSold'] - test_new['YearRemodAdd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = train_new.drop(['MoSold','YrSold','YearBuilt','YearRemodAdd'],axis=1)\n",
    "test_new = test_new.drop(['MoSold','YrSold','YearBuilt','YearRemodAdd'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YearBuilt and YearRemodAdd : Result does not improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: <br>\n",
    "Try to do something with square feet : <br>\n",
    "\n",
    "BsmtFinSF2: Type 2 finished square feet<br>\n",
    "BsmtUnfSF: Unfinished square feet of basement area<br>\n",
    "TotalBsmtSF: Total square feet of basement area<br>\n",
    "\n",
    "1stFlrSF: First Floor square feet<br>\n",
    "2ndFlrSF: Second floor square feet<br>\n",
    "LowQualFinSF: Low quality finished square feet (all floors)<br>\n",
    "GrLivArea: Above grade (ground) living area square feet<br>\n",
    "\n",
    "GarageArea: Size of garage in square feet<br>\n",
    "\n",
    "WoodDeckSF: Wood deck area in square feet<br>\n",
    "\n",
    "OpenPorchSF: Open porch area in square feet<br>\n",
    "\n",
    "EnclosedPorch: Enclosed porch area in square feet<br>\n",
    "3SsnPorch: Three season porch area in square feet<br>\n",
    "ScreenPorch: Screen porch area in square feet<br>\n",
    "PoolArea: Pool area in square feet<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_new\n",
    "y = train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:47:23] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimator = [20,30,50,100,120,200,300,500,900,1000]\n",
    "booster = ['gbtree','gblinear']\n",
    "base_score = [0.25,0.5,0.75,1]\n",
    "max_depth = [2,3,5,10,15]\n",
    "learning_rate = [0.05,0.1,0.15,0.2,0.25,0.3]\n",
    "min_childweight = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparametergrid = {'n_estimator':n_estimator,\n",
    "                      'max_depth':max_depth,\n",
    "                     'learning_rate':learning_rate,\n",
    "                     'min_child_weight':min_childweight,\n",
    "                     'booster':booster,\n",
    "                     'base_score':base_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cv = RandomizedSearchCV(estimator=xgb_model,\n",
    "                              param_distributions=hyperparametergrid,\n",
    "                              cv=5,scoring='neg_mean_squared_error',return_train_score=True,verbose=5,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[03:48:08] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-132730198.049, test=-1341603838.696), total=   0.7s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[03:48:09] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-116062842.246, test=-1284177520.759), total=   0.7s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[03:48:09] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-151579874.982, test=-488709312.193), total=   0.7s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[03:48:10] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-122207540.544, test=-652517596.729), total=   0.7s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5 \n",
      "[03:48:11] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    2.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=3, learning_rate=0.15, booster=gbtree, base_score=0.5, score=(train=-137026727.376, test=-571003819.738), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1 \n",
      "[03:48:12] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1, score=(train=-932906349.119, test=-3453422528.952), total=   0.5s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1 \n",
      "[03:48:12] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1, score=(train=-1265842193.316, test=-1488989103.533), total=   0.5s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1 \n",
      "[03:48:13] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1, score=(train=-1381329099.586, test=-997399938.479), total=   0.5s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1 \n",
      "[03:48:13] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1, score=(train=-1408081830.083, test=-894696595.201), total=   0.5s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1 \n",
      "[03:48:14] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=5, learning_rate=0.25, booster=gblinear, base_score=1, score=(train=-1421447866.968, test=-936159189.177), total=   0.5s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25 \n",
      "[03:48:14] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25, score=(train=-60644810.169, test=-1299446760.748), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25 \n",
      "[03:48:15] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25, score=(train=-60237641.975, test=-1582751720.997), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25 \n",
      "[03:48:15] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25, score=(train=-79882865.071, test=-470238595.948), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25 \n",
      "[03:48:16] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25, score=(train=-59462794.658, test=-845875248.696), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25 \n",
      "[03:48:17] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.3, booster=gbtree, base_score=0.25, score=(train=-65017853.661, test=-641494564.105), total=   0.7s\n",
      "[CV] n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[03:48:18] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-947240165.940, test=-3508354392.235), total=   0.7s\n",
      "[CV] n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[03:48:18] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-1302184626.529, test=-1487504639.254), total=   0.7s\n",
      "[CV] n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[03:48:19] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-1415439978.430, test=-1004528940.668), total=   0.8s\n",
      "[CV] n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[03:48:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-1442792259.140, test=-906754444.949), total=   0.8s\n",
      "[CV] n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5 \n",
      "[03:48:21] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=900, min_child_weight=2, max_depth=15, learning_rate=0.15, booster=gblinear, base_score=0.5, score=(train=-1451069406.599, test=-944333759.488), total=   0.7s\n",
      "[CV] n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[03:48:21] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-959461839.697, test=-3521032068.509), total=   0.7s\n",
      "[CV] n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[03:48:22] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-1332557702.211, test=-1484463289.584), total=   0.7s\n",
      "[CV] n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[03:48:23] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-1444301066.118, test=-1017729169.342), total=   0.7s\n",
      "[CV] n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[03:48:24] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-1470892001.614, test=-925897628.867), total=   0.7s\n",
      "[CV] n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75 \n",
      "[03:48:24] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=100, min_child_weight=2, max_depth=2, learning_rate=0.1, booster=gblinear, base_score=0.75, score=(train=-1476842873.346, test=-932170815.054), total=   0.7s\n",
      "[CV] n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[03:48:25] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-207471383.478, test=-1560326426.634), total=   0.8s\n",
      "[CV] n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[03:48:26] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-212316068.064, test=-1270807195.378), total=   0.8s\n",
      "[CV] n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[03:48:27] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-261571856.661, test=-507289221.464), total=   0.8s\n",
      "[CV] n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[03:48:28] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-209989063.340, test=-702663949.775), total=   0.6s\n",
      "[CV] n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1 \n",
      "[03:48:28] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=200, min_child_weight=4, max_depth=2, learning_rate=0.2, booster=gbtree, base_score=1, score=(train=-225810569.067, test=-550492962.139), total=   0.6s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75 \n",
      "[03:48:29] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75, score=(train=-4880672.386, test=-1327554233.112), total=   1.0s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75 \n",
      "[03:48:30] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75, score=(train=-4284875.800, test=-1627543453.225), total=   1.0s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75 \n",
      "[03:48:31] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75, score=(train=-4037269.861, test=-672648617.554), total=   1.0s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75 \n",
      "[03:48:32] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75, score=(train=-4548722.770, test=-736454578.739), total=   1.0s\n",
      "[CV] n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75 \n",
      "[03:48:33] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=50, min_child_weight=4, max_depth=5, learning_rate=0.3, booster=gbtree, base_score=0.75, score=(train=-4688949.588, test=-720515135.144), total=   1.0s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5 \n",
      "[03:48:34] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5, score=(train=-928366620.845, test=-3429646114.781), total=   0.5s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5 \n",
      "[03:48:34] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5, score=(train=-1255048894.097, test=-1491536176.717), total=   0.5s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5 \n",
      "[03:48:35] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5, score=(train=-1371979416.807, test=-995553287.504), total=   0.5s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5 \n",
      "[03:48:35] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5, score=(train=-1397637775.479, test=-895249567.591), total=   0.5s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5 \n",
      "[03:48:36] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=5, learning_rate=0.3, booster=gblinear, base_score=0.5, score=(train=-1412944042.650, test=-930798978.865), total=   0.5s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[03:48:36] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-302867684.874, test=-1673665362.240), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[03:48:37] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-294135491.192, test=-1671540734.189), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[03:48:38] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-371583296.954, test=-556182945.489), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[03:48:38] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-310042239.749, test=-696061766.164), total=   0.7s\n",
      "[CV] n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25 \n",
      "[03:48:39] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=300, min_child_weight=5, max_depth=3, learning_rate=0.05, booster=gbtree, base_score=0.25, score=(train=-305916963.561, test=-703135175.539), total=   0.7s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[03:48:40] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-322012469.468, test=-1529370818.882), total=   0.6s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[03:48:40] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-296014164.242, test=-1570394487.711), total=   0.6s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[03:48:41] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-378477007.714, test=-453373559.820), total=   0.6s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[03:48:42] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-311814158.239, test=-610217201.738), total=   0.6s\n",
      "[CV] n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75 \n",
      "[03:48:42] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[CV]  n_estimator=500, min_child_weight=1, max_depth=2, learning_rate=0.1, booster=gbtree, base_score=0.75, score=(train=-343971226.442, test=-801502089.433), total=   0.6s\n",
      "[03:48:43] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   34.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                          colsample_bylevel=1,\n",
       "                                          colsample_bynode=1,\n",
       "                                          colsample_bytree=1, gamma=0,\n",
       "                                          importance_type='gain',\n",
       "                                          learning_rate=0.1, max_delta_step=0,\n",
       "                                          max_depth=3, min_child_weight=1,\n",
       "                                          missing=None, n_estimators=100,\n",
       "                                          n_jobs=1, nthread=None,\n",
       "                                          objective='reg:linear',\n",
       "                                          random_st...\n",
       "                   param_distributions={'base_score': [0.25, 0.5, 0.75, 1],\n",
       "                                        'booster': ['gbtree', 'gblinear'],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2,\n",
       "                                                          0.25, 0.3],\n",
       "                                        'max_depth': [2, 3, 5, 10, 15],\n",
       "                                        'min_child_weight': [1, 2, 3, 4, 5],\n",
       "                                        'n_estimator': [20, 30, 50, 100, 120,\n",
       "                                                        200, 300, 500, 900,\n",
       "                                                        1000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=123, refit=True,\n",
       "                   return_train_score=True, scoring='neg_mean_squared_error',\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.15, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=4, missing=None, n_estimator=50,\n",
       "             n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize model using hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_opt = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.15, max_delta_step=0,\n",
    "             max_depth=3, min_child_weight=4, missing=None, n_estimator=50,\n",
    "             n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
    "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "             seed=None, silent=None, subsample=1, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:49:00] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.15, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=4, missing=None, n_estimator=50,\n",
       "             n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_opt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using Optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xgb_model_opt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = sqrt(mean_squared_error(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24234.138023652016\n"
     ]
    }
   ],
   "source": [
    "print(RMSE)  # error is higher than yesterday after doing the features of YearBuilt,YearRemodAdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1461</td>\n",
       "      <td>169277.052498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1462</td>\n",
       "      <td>187758.393989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1463</td>\n",
       "      <td>183583.683570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1464</td>\n",
       "      <td>179317.477511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1465</td>\n",
       "      <td>150730.079977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id      SalePrice\n",
       "0  1461  169277.052498\n",
       "1  1462  187758.393989\n",
       "2  1463  183583.683570\n",
       "3  1464  179317.477511\n",
       "4  1465  150730.079977"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "saleprice = xgb_model_opt.predict(test_new )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['SalePrice'] = saleprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1461</td>\n",
       "      <td>122163.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1462</td>\n",
       "      <td>153987.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1463</td>\n",
       "      <td>179107.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1464</td>\n",
       "      <td>190180.546875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1465</td>\n",
       "      <td>193584.218750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id      SalePrice\n",
       "0  1461  122163.390625\n",
       "1  1462  153987.625000\n",
       "2  1463  179107.515625\n",
       "3  1464  190180.546875\n",
       "4  1465  193584.218750"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "def regress():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(60,input_shape=(input_shape,),activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dense(25,activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dense(5,activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "    return model\n",
    "#model = regress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = SGD(lr=0.2, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=regress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1460 samples\n",
      "Epoch 1/1100\n",
      "1460/1460 [==============================] - 0s 89us/sample - loss: 38717034383.7808\n",
      "Epoch 2/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 37962948215.2329\n",
      "Epoch 3/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 36571636216.9863\n",
      "Epoch 4/1100\n",
      "1460/1460 [==============================] - 0s 24us/sample - loss: 33897488552.3288\n",
      "Epoch 5/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 29368817467.6164\n",
      "Epoch 6/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 22999837625.8630\n",
      "Epoch 7/1100\n",
      "1460/1460 [==============================] - 0s 23us/sample - loss: 16073836291.5068\n",
      "Epoch 8/1100\n",
      "1460/1460 [==============================] - 0s 23us/sample - loss: 11672886243.9452\n",
      "Epoch 9/1100\n",
      "1460/1460 [==============================] - 0s 23us/sample - loss: 9380891760.2192\n",
      "Epoch 10/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 8344879384.5479\n",
      "Epoch 11/1100\n",
      "1460/1460 [==============================] - 0s 23us/sample - loss: 7785184764.4932\n",
      "Epoch 12/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 7367448509.3699\n",
      "Epoch 13/1100\n",
      "1460/1460 [==============================] - 0s 24us/sample - loss: 6839487137.3151\n",
      "Epoch 14/1100\n",
      "1460/1460 [==============================] - 0s 23us/sample - loss: 6377763964.4932\n",
      "Epoch 15/1100\n",
      "1460/1460 [==============================] - 0s 23us/sample - loss: 5959058929.9726\n",
      "Epoch 16/1100\n",
      "1460/1460 [==============================] - 0s 25us/sample - loss: 5575259353.4247\n",
      "Epoch 17/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 5254704769.7534\n",
      "Epoch 18/1100\n",
      "1460/1460 [==============================] - 0s 23us/sample - loss: 4911521944.5479\n",
      "Epoch 19/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 4602669357.5890\n",
      "Epoch 20/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 4323348052.1644\n",
      "Epoch 21/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 4057271001.4247\n",
      "Epoch 22/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 3856215316.1644\n",
      "Epoch 23/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 3625712576.8767\n",
      "Epoch 24/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 3469002601.2055\n",
      "Epoch 25/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 3292713231.7808\n",
      "Epoch 26/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 3173586894.9041\n",
      "Epoch 27/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 3025409038.0274\n",
      "Epoch 28/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 2919393283.5068\n",
      "Epoch 29/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 2827384900.3836\n",
      "Epoch 30/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 2742275282.4110\n",
      "Epoch 31/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 2667135044.3836\n",
      "Epoch 32/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 2615230278.1370\n",
      "Epoch 33/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 2557235799.6712\n",
      "Epoch 34/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 2498357248.0000\n",
      "Epoch 35/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 2463084621.1507\n",
      "Epoch 36/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 2418230394.7397\n",
      "Epoch 37/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 2380972870.1370\n",
      "Epoch 38/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 2347911974.5753\n",
      "Epoch 39/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 2306847505.5342\n",
      "Epoch 40/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 2275852852.6027\n",
      "Epoch 41/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 2263528565.4795\n",
      "Epoch 42/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 2234569137.0959\n",
      "Epoch 43/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 2216332459.8356\n",
      "Epoch 44/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 2196957366.3562\n",
      "Epoch 45/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 2181116245.9178\n",
      "Epoch 46/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 2161782538.5205\n",
      "Epoch 47/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 2166012046.0274\n",
      "Epoch 48/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 2131321399.2329\n",
      "Epoch 49/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 2117611314.8493\n",
      "Epoch 50/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 2101982947.9452\n",
      "Epoch 51/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 2087026362.7397\n",
      "Epoch 52/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 2078447568.6575\n",
      "Epoch 53/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 2063841230.9041\n",
      "Epoch 54/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 2049997543.4521\n",
      "Epoch 55/1100\n",
      "1460/1460 [==============================] - 0s 23us/sample - loss: 2035686296.5479\n",
      "Epoch 56/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 2029541594.3014\n",
      "Epoch 57/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 2019431602.8493\n",
      "Epoch 58/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1999477070.9041\n",
      "Epoch 59/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1987783608.9863\n",
      "Epoch 60/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1974878129.9726\n",
      "Epoch 61/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1967817924.3836\n",
      "Epoch 62/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1952301987.0685\n",
      "Epoch 63/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1938762995.7260\n",
      "Epoch 64/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1936688980.1644\n",
      "Epoch 65/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1921678449.9726\n",
      "Epoch 66/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1905318389.4795\n",
      "Epoch 67/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1895621765.2603\n",
      "Epoch 68/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1890659282.4110\n",
      "Epoch 69/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1884361528.1096\n",
      "Epoch 70/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1871863250.4110\n",
      "Epoch 71/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1859869930.0822\n",
      "Epoch 72/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1857351210.0822\n",
      "Epoch 73/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1850088983.6712\n",
      "Epoch 74/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1842326945.3151\n",
      "Epoch 75/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1835043919.7808\n",
      "Epoch 76/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1823363459.5068\n",
      "Epoch 77/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1819210059.3973\n",
      "Epoch 78/1100\n",
      "1460/1460 [==============================] - 0s 25us/sample - loss: 1813759936.8767\n",
      "Epoch 79/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1833669760.0000\n",
      "Epoch 80/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1804332319.5616\n",
      "Epoch 81/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1804505608.7671\n",
      "Epoch 82/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1804959070.6849\n",
      "Epoch 83/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1793552810.0822\n",
      "Epoch 84/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1783086655.7808\n",
      "Epoch 85/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1782934577.0959\n",
      "Epoch 86/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1775163139.5068\n",
      "Epoch 87/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1768553761.3151\n",
      "Epoch 88/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1761138647.6712\n",
      "Epoch 89/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1754080423.4521\n",
      "Epoch 90/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1751867339.3973\n",
      "Epoch 91/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1748878381.5890\n",
      "Epoch 92/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1742294507.8356\n",
      "Epoch 93/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1748925228.7123\n",
      "Epoch 94/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1741818555.6164\n",
      "Epoch 95/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1734643901.3699\n",
      "Epoch 96/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1738571372.7123\n",
      "Epoch 97/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1724049667.5068\n",
      "Epoch 98/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1708400376.9863\n",
      "Epoch 99/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1712200497.0959\n",
      "Epoch 100/1100\n",
      "1460/1460 [==============================] - 0s 26us/sample - loss: 1715571123.7260\n",
      "Epoch 101/1100\n",
      "1460/1460 [==============================] - 0s 24us/sample - loss: 1706051740.9315\n",
      "Epoch 102/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1697706936.9863\n",
      "Epoch 103/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1692819853.5890\n",
      "Epoch 104/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1685382938.3014\n",
      "Epoch 105/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1681896262.1370\n",
      "Epoch 106/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1678989347.0685\n",
      "Epoch 107/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1673968602.7397\n",
      "Epoch 108/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1669501578.5205\n",
      "Epoch 109/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1670087752.3288\n",
      "Epoch 110/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1666913001.2055\n",
      "Epoch 111/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1666884298.5205\n",
      "Epoch 112/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1659772198.5753\n",
      "Epoch 113/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1658125850.3014\n",
      "Epoch 114/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1647294459.6164\n",
      "Epoch 115/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1640076842.0822\n",
      "Epoch 116/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1650582203.6164\n",
      "Epoch 117/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1642238710.3562\n",
      "Epoch 118/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1648667777.7534\n",
      "Epoch 119/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1644298089.2055\n",
      "Epoch 120/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1684148290.6301\n",
      "Epoch 121/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1659245194.5205\n",
      "Epoch 122/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1637739069.3699\n",
      "Epoch 123/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1621397139.2877\n",
      "Epoch 124/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1612971729.0959\n",
      "Epoch 125/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1608816263.0137\n",
      "Epoch 126/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1606738128.6575\n",
      "Epoch 127/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1615395960.9863\n",
      "Epoch 128/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1618911300.3836\n",
      "Epoch 129/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1599563562.0822\n",
      "Epoch 130/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1597256908.2740\n",
      "Epoch 131/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1598696928.4384\n",
      "Epoch 132/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1593154650.3014\n",
      "Epoch 133/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1584289043.2877\n",
      "Epoch 134/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1589104520.7671\n",
      "Epoch 135/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1587034059.3973\n",
      "Epoch 136/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1582396588.7123\n",
      "Epoch 137/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1582606760.3288\n",
      "Epoch 138/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1570117658.3014\n",
      "Epoch 139/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1564741400.5479\n",
      "Epoch 140/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1570810785.3151\n",
      "Epoch 141/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1563994211.5068\n",
      "Epoch 142/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1561530045.3699\n",
      "Epoch 143/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1563999899.6164\n",
      "Epoch 144/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1562262953.2055\n",
      "Epoch 145/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1550753897.2055\n",
      "Epoch 146/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1554145866.5205\n",
      "Epoch 147/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1561925719.6712\n",
      "Epoch 148/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1557677063.0137\n",
      "Epoch 149/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1542988400.2192\n",
      "Epoch 150/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1533100366.9041\n",
      "Epoch 151/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1531320428.7123\n",
      "Epoch 152/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1533503379.2877\n",
      "Epoch 153/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1537884110.9041\n",
      "Epoch 154/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1527668839.4521\n",
      "Epoch 155/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1525353134.0274\n",
      "Epoch 156/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1517438687.5616\n",
      "Epoch 157/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1517261115.6164\n",
      "Epoch 158/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1516894336.0000\n",
      "Epoch 159/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1507797767.0137\n",
      "Epoch 160/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1512688376.9863\n",
      "Epoch 161/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1555212214.3562\n",
      "Epoch 162/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1538579399.8904\n",
      "Epoch 163/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1503143642.3014\n",
      "Epoch 164/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1488330890.5205\n",
      "Epoch 165/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1485965774.9041\n",
      "Epoch 166/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1488086086.1370\n",
      "Epoch 167/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1486335315.2877\n",
      "Epoch 168/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1478491279.7808\n",
      "Epoch 169/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1478464469.9178\n",
      "Epoch 170/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1488715502.4658\n",
      "Epoch 171/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1473292147.7260\n",
      "Epoch 172/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1476673081.4247\n",
      "Epoch 173/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1467948418.6301\n",
      "Epoch 174/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1478321879.6712\n",
      "Epoch 175/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1464357283.0685\n",
      "Epoch 176/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1455999619.5068\n",
      "Epoch 177/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1450808593.5342\n",
      "Epoch 178/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1449389175.2329\n",
      "Epoch 179/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1462059569.0959\n",
      "Epoch 180/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1467033989.2603\n",
      "Epoch 181/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1451357414.5753\n",
      "Epoch 182/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1441718752.4384\n",
      "Epoch 183/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1431371258.7397\n",
      "Epoch 184/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1437709353.2055\n",
      "Epoch 185/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1430871473.0959\n",
      "Epoch 186/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1449404843.8356\n",
      "Epoch 187/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1576760788.1644\n",
      "Epoch 188/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1456491719.0137\n",
      "Epoch 189/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1422036869.2603\n",
      "Epoch 190/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1410999890.4110\n",
      "Epoch 191/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1408194286.4658\n",
      "Epoch 192/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1416288462.0274\n",
      "Epoch 193/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1405162503.8904\n",
      "Epoch 194/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1394798293.9178\n",
      "Epoch 195/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1415725101.5890\n",
      "Epoch 196/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1391689199.3425\n",
      "Epoch 197/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1395269179.6164\n",
      "Epoch 198/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1391759268.8219\n",
      "Epoch 199/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1392517412.8219\n",
      "Epoch 200/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1386760973.1507\n",
      "Epoch 201/1100\n",
      "1460/1460 [==============================] - 0s 16us/sample - loss: 1382512932.8219\n",
      "Epoch 202/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1385861933.5890\n",
      "Epoch 203/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1372575528.3288\n",
      "Epoch 204/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1379071112.7671\n",
      "Epoch 205/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1372759224.1096\n",
      "Epoch 206/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1369316737.7534\n",
      "Epoch 207/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1361043947.8356\n",
      "Epoch 208/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1356434855.0137\n",
      "Epoch 209/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1352455816.7671\n",
      "Epoch 210/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1366988321.3151\n",
      "Epoch 211/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1349926726.1370\n",
      "Epoch 212/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1397336239.3425\n",
      "Epoch 213/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1343032710.1370\n",
      "Epoch 214/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1347960870.5753\n",
      "Epoch 215/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1349659572.6027\n",
      "Epoch 216/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1344402996.6027\n",
      "Epoch 217/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1333564019.7260\n",
      "Epoch 218/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1332067146.5205\n",
      "Epoch 219/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1325483116.7123\n",
      "Epoch 220/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1321717607.4521\n",
      "Epoch 221/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1329309105.9726\n",
      "Epoch 222/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1319794940.4932\n",
      "Epoch 223/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1320457578.9589\n",
      "Epoch 224/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1332087473.0959\n",
      "Epoch 225/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1305331084.2740\n",
      "Epoch 226/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1316013827.5068\n",
      "Epoch 227/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1313994039.2329\n",
      "Epoch 228/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1307874356.1644\n",
      "Epoch 229/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1305465671.0137\n",
      "Epoch 230/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1301099452.4932\n",
      "Epoch 231/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1293218339.0685\n",
      "Epoch 232/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1300694303.5616\n",
      "Epoch 233/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1293374986.5205\n",
      "Epoch 234/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1293221499.6164\n",
      "Epoch 235/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1290401965.5890\n",
      "Epoch 236/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1282374307.9452\n",
      "Epoch 237/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1279919009.3151\n",
      "Epoch 238/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1278441603.5068\n",
      "Epoch 239/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1285969983.1233\n",
      "Epoch 240/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1289711326.6849\n",
      "Epoch 241/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1272406068.6027\n",
      "Epoch 242/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1275517589.9178\n",
      "Epoch 243/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1266405816.1096\n",
      "Epoch 244/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 1263330230.3562\n",
      "Epoch 245/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1261430528.0000\n",
      "Epoch 246/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1261144952.1096\n",
      "Epoch 247/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1253931508.6027\n",
      "Epoch 248/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1253305963.8356\n",
      "Epoch 249/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1266913595.6164\n",
      "Epoch 250/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1264891279.7808\n",
      "Epoch 251/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1254548882.4110\n",
      "Epoch 252/1100\n",
      "1460/1460 [==============================] - 0s 16us/sample - loss: 1254860086.3562\n",
      "Epoch 253/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1259259042.1918\n",
      "Epoch 254/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1250008609.3151\n",
      "Epoch 255/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1235390584.9863\n",
      "Epoch 256/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1242028850.8493\n",
      "Epoch 257/1100\n",
      "1460/1460 [==============================] - 0s 16us/sample - loss: 1230261714.4110\n",
      "Epoch 258/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1236411686.5753\n",
      "Epoch 259/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1243770547.7260\n",
      "Epoch 260/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1240274617.8630\n",
      "Epoch 261/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1224887730.8493\n",
      "Epoch 262/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1234016843.8356\n",
      "Epoch 263/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1247558720.8767\n",
      "Epoch 264/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1235535453.8082\n",
      "Epoch 265/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1217735655.4521\n",
      "Epoch 266/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1213945740.2740\n",
      "Epoch 267/1100\n",
      "1460/1460 [==============================] - 0s 16us/sample - loss: 1214538887.0137\n",
      "Epoch 268/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1233265272.1096\n",
      "Epoch 269/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1207921041.5342\n",
      "Epoch 270/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1200822570.5205\n",
      "Epoch 271/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1201385905.9726\n",
      "Epoch 272/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1199938070.7945\n",
      "Epoch 273/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1198742473.6438\n",
      "Epoch 274/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1193868091.6164\n",
      "Epoch 275/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1198761507.5068\n",
      "Epoch 276/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1189080789.9178\n",
      "Epoch 277/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1197940817.9726\n",
      "Epoch 278/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1188766091.3973\n",
      "Epoch 279/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1192055288.9863\n",
      "Epoch 280/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1190604592.2192\n",
      "Epoch 281/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1182523430.5753\n",
      "Epoch 282/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1223865434.3014\n",
      "Epoch 283/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1199970737.0959\n",
      "Epoch 284/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1182324187.6164\n",
      "Epoch 285/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1182210245.2603\n",
      "Epoch 286/1100\n",
      "1460/1460 [==============================] - 0s 16us/sample - loss: 1188532481.7534\n",
      "Epoch 287/1100\n",
      "1460/1460 [==============================] - 0s 16us/sample - loss: 1170854286.9041\n",
      "Epoch 288/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1165150653.3699\n",
      "Epoch 289/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1165246375.4521\n",
      "Epoch 290/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1169107690.9589\n",
      "Epoch 291/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1163118567.4521\n",
      "Epoch 292/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1159791478.3562\n",
      "Epoch 293/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1173464324.8219\n",
      "Epoch 294/1100\n",
      "1460/1460 [==============================] - ETA: 0s - loss: 3143946240.00 - 0s 18us/sample - loss: 1166916216.1096\n",
      "Epoch 295/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1160075822.4658\n",
      "Epoch 296/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1162968398.9041\n",
      "Epoch 297/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1156523883.3973\n",
      "Epoch 298/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1149262992.6575\n",
      "Epoch 299/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1153642830.9041\n",
      "Epoch 300/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1147222399.1233\n",
      "Epoch 301/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1136438504.7671\n",
      "Epoch 302/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1156311020.7123\n",
      "Epoch 303/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1149228756.1644\n",
      "Epoch 304/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1140829678.4658\n",
      "Epoch 305/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1132341174.3562\n",
      "Epoch 306/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1130417934.0274\n",
      "Epoch 307/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1139259597.1507\n",
      "Epoch 308/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1149586594.1918\n",
      "Epoch 309/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1131451323.6164\n",
      "Epoch 310/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1124993948.0548\n",
      "Epoch 311/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1126263151.3425\n",
      "Epoch 312/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1121430489.8630\n",
      "Epoch 313/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1122537137.9726\n",
      "Epoch 314/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1124953888.4384\n",
      "Epoch 315/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1117688046.4658\n",
      "Epoch 316/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1118740876.2740\n",
      "Epoch 317/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1115490828.7123\n",
      "Epoch 318/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1111329644.7123\n",
      "Epoch 319/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1129430247.4521\n",
      "Epoch 320/1100\n",
      "1460/1460 [==============================] - 0s 23us/sample - loss: 1132517100.7123\n",
      "Epoch 321/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1108841941.0411\n",
      "Epoch 322/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1107932942.0274\n",
      "Epoch 323/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1142760924.9315\n",
      "Epoch 324/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1117130159.3425\n",
      "Epoch 325/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1105159307.3973\n",
      "Epoch 326/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1104744165.6986\n",
      "Epoch 327/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1176873303.6712\n",
      "Epoch 328/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1126259630.0274\n",
      "Epoch 329/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1109854990.0274\n",
      "Epoch 330/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1091542673.5342\n",
      "Epoch 331/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1096352398.4658\n",
      "Epoch 332/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1087147058.8493\n",
      "Epoch 333/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1099721037.1507\n",
      "Epoch 334/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1089385454.4658\n",
      "Epoch 335/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1083474446.9041\n",
      "Epoch 336/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1084934684.9315\n",
      "Epoch 337/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1093994307.0685\n",
      "Epoch 338/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1075995495.4521\n",
      "Epoch 339/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1092194603.8356\n",
      "Epoch 340/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1096915939.9452\n",
      "Epoch 341/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1084577115.1781\n",
      "Epoch 342/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1090312227.0685\n",
      "Epoch 343/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1071957170.8493\n",
      "Epoch 344/1100\n",
      "1460/1460 [==============================] - 0s 16us/sample - loss: 1070543184.6575\n",
      "Epoch 345/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1076350096.6575\n",
      "Epoch 346/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1064540642.1918\n",
      "Epoch 347/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1083860711.4521\n",
      "Epoch 348/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1062068154.7397\n",
      "Epoch 349/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1072100744.7671\n",
      "Epoch 350/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1061036757.9178\n",
      "Epoch 351/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1059472161.3151\n",
      "Epoch 352/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1062215210.0822\n",
      "Epoch 353/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1053522857.2055\n",
      "Epoch 354/1100\n",
      "1460/1460 [==============================] - 0s 16us/sample - loss: 1082938755.5068\n",
      "Epoch 355/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1058103687.0137\n",
      "Epoch 356/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1070637094.1370\n",
      "Epoch 357/1100\n",
      "1460/1460 [==============================] - ETA: 0s - loss: 1186698880.00 - 0s 18us/sample - loss: 1059630604.2740\n",
      "Epoch 358/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1105356494.0274\n",
      "Epoch 359/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1066811522.1918\n",
      "Epoch 360/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1059352116.6027\n",
      "Epoch 361/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1043938556.4932\n",
      "Epoch 362/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1047975076.3836\n",
      "Epoch 363/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1041273434.3014\n",
      "Epoch 364/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1042918108.9315\n",
      "Epoch 365/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1034258623.1233\n",
      "Epoch 366/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1073780342.3562\n",
      "Epoch 367/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 1043040597.9178\n",
      "Epoch 368/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1047136212.1644\n",
      "Epoch 369/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1032280123.6164\n",
      "Epoch 370/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1030256868.8219\n",
      "Epoch 371/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1044867943.4521\n",
      "Epoch 372/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1037858498.6301\n",
      "Epoch 373/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1023815664.2192\n",
      "Epoch 374/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1056867886.9041\n",
      "Epoch 375/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1039612899.5068\n",
      "Epoch 376/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1038404200.3288\n",
      "Epoch 377/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1017579527.0137\n",
      "Epoch 378/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1016939954.8493\n",
      "Epoch 379/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1021347079.0137\n",
      "Epoch 380/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1015175599.3425\n",
      "Epoch 381/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1018587946.0822\n",
      "Epoch 382/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1013045710.9041\n",
      "Epoch 383/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1016745203.7260\n",
      "Epoch 384/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1022194477.5890\n",
      "Epoch 385/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1015925886.6849\n",
      "Epoch 386/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1017844662.3562\n",
      "Epoch 387/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1013262014.2466\n",
      "Epoch 388/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1007052930.6301\n",
      "Epoch 389/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1011951958.3562\n",
      "Epoch 390/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1003446054.5753\n",
      "Epoch 391/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1005798666.5205\n",
      "Epoch 392/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1006964227.5068\n",
      "Epoch 393/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1000682424.9863\n",
      "Epoch 394/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 1003571987.2877\n",
      "Epoch 395/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 998579720.7671\n",
      "Epoch 396/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 993705838.4658\n",
      "Epoch 397/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1020728770.6301\n",
      "Epoch 398/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1008732351.1233\n",
      "Epoch 399/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1017051103.1233\n",
      "Epoch 400/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 1002024160.4384\n",
      "Epoch 401/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 986692325.6986\n",
      "Epoch 402/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 990542265.8630\n",
      "Epoch 403/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 1002265939.2877\n",
      "Epoch 404/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 1005059941.6986\n",
      "Epoch 405/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 1006501281.3151\n",
      "Epoch 406/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 984922396.9315\n",
      "Epoch 407/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 976446258.8493\n",
      "Epoch 408/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 980266446.0274\n",
      "Epoch 409/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 979847101.3699\n",
      "Epoch 410/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 983131884.7123\n",
      "Epoch 411/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 979622366.6849\n",
      "Epoch 412/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 973218534.5753\n",
      "Epoch 413/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 17us/sample - loss: 971407715.9452\n",
      "Epoch 414/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 979127045.2603\n",
      "Epoch 415/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 978755862.7945\n",
      "Epoch 416/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 978786984.3288\n",
      "Epoch 417/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 976383612.4932\n",
      "Epoch 418/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 994162091.8356\n",
      "Epoch 419/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 969396997.2603\n",
      "Epoch 420/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 979397205.4795\n",
      "Epoch 421/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 965569993.2055\n",
      "Epoch 422/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 987960739.9452\n",
      "Epoch 423/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 966404687.7808\n",
      "Epoch 424/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 962179806.6849\n",
      "Epoch 425/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 962596026.7397\n",
      "Epoch 426/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 963221321.2055\n",
      "Epoch 427/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 954915638.7945\n",
      "Epoch 428/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 960489218.6301\n",
      "Epoch 429/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 967401994.5205\n",
      "Epoch 430/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 952831666.8493\n",
      "Epoch 431/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 973826503.8904\n",
      "Epoch 432/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 959749493.4795\n",
      "Epoch 433/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 947588611.5068\n",
      "Epoch 434/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 959065307.6164\n",
      "Epoch 435/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 954346424.1096\n",
      "Epoch 436/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 953929926.1370\n",
      "Epoch 437/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 953069123.0685\n",
      "Epoch 438/1100\n",
      "1460/1460 [==============================] - 0s 16us/sample - loss: 943194557.3699\n",
      "Epoch 439/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 945773176.1096\n",
      "Epoch 440/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 943877703.4521\n",
      "Epoch 441/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 944703711.5616\n",
      "Epoch 442/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 943339637.4795\n",
      "Epoch 443/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 942084618.5205\n",
      "Epoch 444/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 941284753.5342\n",
      "Epoch 445/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 937564660.6027\n",
      "Epoch 446/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 954901393.0959\n",
      "Epoch 447/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 934790619.1781\n",
      "Epoch 448/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 943120150.7945\n",
      "Epoch 449/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 941331936.0000\n",
      "Epoch 450/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 931072538.3014\n",
      "Epoch 451/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 933169620.1644\n",
      "Epoch 452/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 931466013.8082\n",
      "Epoch 453/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 940429130.5205\n",
      "Epoch 454/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 936673312.4384\n",
      "Epoch 455/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 924312928.4384\n",
      "Epoch 456/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 942350918.1370\n",
      "Epoch 457/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 948543955.2877\n",
      "Epoch 458/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 927953403.1781\n",
      "Epoch 459/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 923922245.6986\n",
      "Epoch 460/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 920406216.7671\n",
      "Epoch 461/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 917799608.1096\n",
      "Epoch 462/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 919406021.2603\n",
      "Epoch 463/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 918671796.6027\n",
      "Epoch 464/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 924698267.1781\n",
      "Epoch 465/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 917851271.8904\n",
      "Epoch 466/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 918404279.2329\n",
      "Epoch 467/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 913584839.4521\n",
      "Epoch 468/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 925694467.5068\n",
      "Epoch 469/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 903624267.3973\n",
      "Epoch 470/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 913796520.7671\n",
      "Epoch 471/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 905025924.8219\n",
      "Epoch 472/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 931100588.7123\n",
      "Epoch 473/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 921473187.0685\n",
      "Epoch 474/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 916547896.9863\n",
      "Epoch 475/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 907894411.3973\n",
      "Epoch 476/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 908242441.2055\n",
      "Epoch 477/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 907086120.3288\n",
      "Epoch 478/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 902302569.2055\n",
      "Epoch 479/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 917567140.8219\n",
      "Epoch 480/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 899486835.7260\n",
      "Epoch 481/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 900731455.1233\n",
      "Epoch 482/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 906321601.7534\n",
      "Epoch 483/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 924646414.9041\n",
      "Epoch 484/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 894591344.2192\n",
      "Epoch 485/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 916479000.5479\n",
      "Epoch 486/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 904415095.2329\n",
      "Epoch 487/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 904012209.0959\n",
      "Epoch 488/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 901503612.4932\n",
      "Epoch 489/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 898978601.2055\n",
      "Epoch 490/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 892645688.1096\n",
      "Epoch 491/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 890012145.5342\n",
      "Epoch 492/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 890401141.0411\n",
      "Epoch 493/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 886934605.1507\n",
      "Epoch 494/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 887841937.5342\n",
      "Epoch 495/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 891331601.5342\n",
      "Epoch 496/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 18us/sample - loss: 881692956.0548\n",
      "Epoch 497/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 884416370.8493\n",
      "Epoch 498/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 889087879.0137\n",
      "Epoch 499/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 877376149.9178\n",
      "Epoch 500/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 902790852.3836\n",
      "Epoch 501/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 886414521.4247\n",
      "Epoch 502/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 903823342.4658\n",
      "Epoch 503/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 902228986.7397\n",
      "Epoch 504/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 877349393.5342\n",
      "Epoch 505/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 875233146.3014\n",
      "Epoch 506/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 873620160.8767\n",
      "Epoch 507/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 880705183.5616\n",
      "Epoch 508/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 883527020.7123\n",
      "Epoch 509/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 879950441.2055\n",
      "Epoch 510/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 878210261.0411\n",
      "Epoch 511/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 885314731.8356\n",
      "Epoch 512/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 864560245.4795\n",
      "Epoch 513/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 868020271.7808\n",
      "Epoch 514/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 881625265.9726\n",
      "Epoch 515/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 868119755.3973\n",
      "Epoch 516/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 866469540.8219\n",
      "Epoch 517/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 896235288.5479\n",
      "Epoch 518/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 883194190.9041\n",
      "Epoch 519/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 880039817.6438\n",
      "Epoch 520/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 864484487.8904\n",
      "Epoch 521/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 874687622.5753\n",
      "Epoch 522/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 872523256.9863\n",
      "Epoch 523/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 864089151.5616\n",
      "Epoch 524/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 853740760.5479\n",
      "Epoch 525/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 858585859.5068\n",
      "Epoch 526/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 855361867.3973\n",
      "Epoch 527/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 860958488.5479\n",
      "Epoch 528/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 854416497.0959\n",
      "Epoch 529/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 859750409.6438\n",
      "Epoch 530/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 863248354.1918\n",
      "Epoch 531/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 849372209.0959\n",
      "Epoch 532/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 881819608.9863\n",
      "Epoch 533/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 857061426.8493\n",
      "Epoch 534/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 850482814.2466\n",
      "Epoch 535/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 851014082.6301\n",
      "Epoch 536/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 875225666.6301\n",
      "Epoch 537/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 859013232.2192\n",
      "Epoch 538/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 847862803.2877\n",
      "Epoch 539/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 847676324.8219\n",
      "Epoch 540/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 845584384.0000\n",
      "Epoch 541/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 845733714.4110\n",
      "Epoch 542/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 840112782.0274\n",
      "Epoch 543/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 838448678.5753\n",
      "Epoch 544/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 850951658.0822\n",
      "Epoch 545/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 851119667.7260\n",
      "Epoch 546/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 833219045.6986\n",
      "Epoch 547/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 833130605.1507\n",
      "Epoch 548/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 832729896.3288\n",
      "Epoch 549/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 857559985.9726\n",
      "Epoch 550/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 838090547.7260\n",
      "Epoch 551/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 832525611.8356\n",
      "Epoch 552/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 827900260.8219\n",
      "Epoch 553/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 832248201.2055\n",
      "Epoch 554/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 828751076.8219\n",
      "Epoch 555/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 839207101.3699\n",
      "Epoch 556/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 826875334.1370\n",
      "Epoch 557/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 824201279.1233\n",
      "Epoch 558/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 822509816.9863\n",
      "Epoch 559/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 829844008.3288\n",
      "Epoch 560/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 830121319.4521\n",
      "Epoch 561/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 841994094.9041\n",
      "Epoch 562/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 823225137.0959\n",
      "Epoch 563/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 819624032.4384\n",
      "Epoch 564/1100\n",
      "1460/1460 [==============================] - 0s 23us/sample - loss: 818764181.0411\n",
      "Epoch 565/1100\n",
      "1460/1460 [==============================] - 0s 23us/sample - loss: 815627067.6164\n",
      "Epoch 566/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 816969191.4521\n",
      "Epoch 567/1100\n",
      "1460/1460 [==============================] - 0s 23us/sample - loss: 827577335.2329\n",
      "Epoch 568/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 821786463.5616\n",
      "Epoch 569/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 809751724.2740\n",
      "Epoch 570/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 839551104.0000\n",
      "Epoch 571/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 814352475.3973\n",
      "Epoch 572/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 815028351.1233\n",
      "Epoch 573/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 809174891.8356\n",
      "Epoch 574/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 807668486.1370\n",
      "Epoch 575/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 805642858.0822\n",
      "Epoch 576/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 806642139.1781\n",
      "Epoch 577/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 813136752.2192\n",
      "Epoch 578/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 805944426.5205\n",
      "Epoch 579/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 21us/sample - loss: 807974376.3288\n",
      "Epoch 580/1100\n",
      "1460/1460 [==============================] - 0s 24us/sample - loss: 800966618.3014\n",
      "Epoch 581/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 814539398.5753\n",
      "Epoch 582/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 805296510.2466\n",
      "Epoch 583/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 796177038.0274\n",
      "Epoch 584/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 800693397.9178\n",
      "Epoch 585/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 809746377.2055\n",
      "Epoch 586/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 798011127.4521\n",
      "Epoch 587/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 808508454.1370\n",
      "Epoch 588/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 806273693.8082\n",
      "Epoch 589/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 794553032.7671\n",
      "Epoch 590/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 797101396.1644\n",
      "Epoch 591/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 802123943.4521\n",
      "Epoch 592/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 821617377.3151\n",
      "Epoch 593/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 829544681.6438\n",
      "Epoch 594/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 835366193.0959\n",
      "Epoch 595/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 818517873.9726\n",
      "Epoch 596/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 820446146.6301\n",
      "Epoch 597/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 840348798.2466\n",
      "Epoch 598/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 803715964.4932\n",
      "Epoch 599/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 804138268.0548\n",
      "Epoch 600/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 863779948.2740\n",
      "Epoch 601/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 785496344.5479\n",
      "Epoch 602/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 782827067.6164\n",
      "Epoch 603/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 787638109.8082\n",
      "Epoch 604/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 783925711.7808\n",
      "Epoch 605/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 786688661.0411\n",
      "Epoch 606/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 787010069.9178\n",
      "Epoch 607/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 775088473.4247\n",
      "Epoch 608/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 776370885.2603\n",
      "Epoch 609/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 782676423.0137\n",
      "Epoch 610/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 773911510.3562\n",
      "Epoch 611/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 772058865.9726\n",
      "Epoch 612/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 786439993.8630\n",
      "Epoch 613/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 777842922.9589\n",
      "Epoch 614/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 776262667.8356\n",
      "Epoch 615/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 770487792.2192\n",
      "Epoch 616/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 771367368.7671\n",
      "Epoch 617/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 778676508.0548\n",
      "Epoch 618/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 767966740.1644\n",
      "Epoch 619/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 771007236.3836\n",
      "Epoch 620/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 778001532.4932\n",
      "Epoch 621/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 775704397.1507\n",
      "Epoch 622/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 768427399.8904\n",
      "Epoch 623/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 767824466.4110\n",
      "Epoch 624/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 782382527.1233\n",
      "Epoch 625/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 763364110.0274\n",
      "Epoch 626/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 777280490.9589\n",
      "Epoch 627/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 763656770.1918\n",
      "Epoch 628/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 761441545.6438\n",
      "Epoch 629/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 763754439.8904\n",
      "Epoch 630/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 761443286.3562\n",
      "Epoch 631/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 768759345.9726\n",
      "Epoch 632/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 760256580.3836\n",
      "Epoch 633/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 751549090.6301\n",
      "Epoch 634/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 753885726.6849\n",
      "Epoch 635/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 753116486.1370\n",
      "Epoch 636/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 757367578.3014\n",
      "Epoch 637/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 762196247.2329\n",
      "Epoch 638/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 750606326.7945\n",
      "Epoch 639/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 752437344.6575\n",
      "Epoch 640/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 750341651.2877\n",
      "Epoch 641/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 782041205.0411\n",
      "Epoch 642/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 749928120.1096\n",
      "Epoch 643/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 820509908.1644\n",
      "Epoch 644/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 748607340.7123\n",
      "Epoch 645/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 751260366.9041\n",
      "Epoch 646/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 759484231.0137\n",
      "Epoch 647/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 748371693.5890\n",
      "Epoch 648/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 747876458.9589\n",
      "Epoch 649/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 739651180.7123\n",
      "Epoch 650/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 736548923.6164\n",
      "Epoch 651/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 742805763.9452\n",
      "Epoch 652/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 736945318.5753\n",
      "Epoch 653/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 744931762.8493\n",
      "Epoch 654/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 743554576.6575\n",
      "Epoch 655/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 736938954.0822\n",
      "Epoch 656/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 735142873.4247\n",
      "Epoch 657/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 740768212.1644\n",
      "Epoch 658/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 737051409.5342\n",
      "Epoch 659/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 732403551.5616\n",
      "Epoch 660/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 739559322.3014\n",
      "Epoch 661/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 729578263.2329\n",
      "Epoch 662/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 20us/sample - loss: 729335332.8219\n",
      "Epoch 663/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 727327368.3288\n",
      "Epoch 664/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 736353292.2740\n",
      "Epoch 665/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 731445710.0274\n",
      "Epoch 666/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 729468523.8356\n",
      "Epoch 667/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 735597665.3151\n",
      "Epoch 668/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 726106381.1507\n",
      "Epoch 669/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 759980501.9178\n",
      "Epoch 670/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 737187047.0137\n",
      "Epoch 671/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 744236240.6575\n",
      "Epoch 672/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 766940265.2055\n",
      "Epoch 673/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 739376946.4110\n",
      "Epoch 674/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 726505781.0411\n",
      "Epoch 675/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 728240990.6849\n",
      "Epoch 676/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 746523892.6027\n",
      "Epoch 677/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 724583377.0959\n",
      "Epoch 678/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 718210903.6712\n",
      "Epoch 679/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 716798871.6712\n",
      "Epoch 680/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 710741841.5342\n",
      "Epoch 681/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 736376828.4932\n",
      "Epoch 682/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 727213732.8219\n",
      "Epoch 683/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 718337807.3425\n",
      "Epoch 684/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 714865019.6164\n",
      "Epoch 685/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 743105513.2055\n",
      "Epoch 686/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 719800181.0411\n",
      "Epoch 687/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 719695971.0685\n",
      "Epoch 688/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 716531110.5753\n",
      "Epoch 689/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 730112814.4658\n",
      "Epoch 690/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 710834657.3151\n",
      "Epoch 691/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 717261826.1918\n",
      "Epoch 692/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 706931364.8219\n",
      "Epoch 693/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 729395345.5342\n",
      "Epoch 694/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 717491322.7397\n",
      "Epoch 695/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 706393360.6575\n",
      "Epoch 696/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 723744206.9041\n",
      "Epoch 697/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 705517120.4384\n",
      "Epoch 698/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 698700398.9041\n",
      "Epoch 699/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 702609580.7123\n",
      "Epoch 700/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 707052881.5342\n",
      "Epoch 701/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 714518960.6575\n",
      "Epoch 702/1100\n",
      "1460/1460 [==============================] - ETA: 0s - loss: 540631296.000 - 0s 18us/sample - loss: 716698912.8767\n",
      "Epoch 703/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 726003503.7808\n",
      "Epoch 704/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 697540205.1507\n",
      "Epoch 705/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 704112895.1233\n",
      "Epoch 706/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 706522371.5068\n",
      "Epoch 707/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 703310185.2055\n",
      "Epoch 708/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 708649874.4110\n",
      "Epoch 709/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 711945534.4658\n",
      "Epoch 710/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 694427278.0274\n",
      "Epoch 711/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 703713780.6027\n",
      "Epoch 712/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 700920183.2329\n",
      "Epoch 713/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 694379338.5205\n",
      "Epoch 714/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 694932142.4658\n",
      "Epoch 715/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 702403463.8904\n",
      "Epoch 716/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 692555533.3699\n",
      "Epoch 717/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 691554914.1918\n",
      "Epoch 718/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 692814640.2192\n",
      "Epoch 719/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 686393476.3836\n",
      "Epoch 720/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 687427762.8493\n",
      "Epoch 721/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 765894121.2055\n",
      "Epoch 722/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 692344240.2192\n",
      "Epoch 723/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 682874798.4658\n",
      "Epoch 724/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 679888957.3699\n",
      "Epoch 725/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 678275558.5753\n",
      "Epoch 726/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 703421117.3699\n",
      "Epoch 727/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 689292098.1918\n",
      "Epoch 728/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 687648777.6438\n",
      "Epoch 729/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 692782797.1507\n",
      "Epoch 730/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 716173853.8082\n",
      "Epoch 731/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 714252216.1096\n",
      "Epoch 732/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 679685108.6027\n",
      "Epoch 733/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 681064330.5205\n",
      "Epoch 734/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 680775392.4384\n",
      "Epoch 735/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 698147043.0685\n",
      "Epoch 736/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 691984456.7671\n",
      "Epoch 737/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 684673371.1781\n",
      "Epoch 738/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 717734129.0959\n",
      "Epoch 739/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 707324435.2877\n",
      "Epoch 740/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 680343038.2466\n",
      "Epoch 741/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 676612569.8630\n",
      "Epoch 742/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 674162856.3288\n",
      "Epoch 743/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 671201225.6438\n",
      "Epoch 744/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 711837162.9589\n",
      "Epoch 745/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 20us/sample - loss: 685701501.3699\n",
      "Epoch 746/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 668869005.5890\n",
      "Epoch 747/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 677580621.1507\n",
      "Epoch 748/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 664401613.1507\n",
      "Epoch 749/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 667267623.4521\n",
      "Epoch 750/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 670110529.7534\n",
      "Epoch 751/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 688051172.8219\n",
      "Epoch 752/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 663788456.3288\n",
      "Epoch 753/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 665565473.3151\n",
      "Epoch 754/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 668410751.1233\n",
      "Epoch 755/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 672800198.3562\n",
      "Epoch 756/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 667198825.2055\n",
      "Epoch 757/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 670916681.6438\n",
      "Epoch 758/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 663299233.3151\n",
      "Epoch 759/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 663545569.3151\n",
      "Epoch 760/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 661987838.9041\n",
      "Epoch 761/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 654911317.4795\n",
      "Epoch 762/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 663778644.1644\n",
      "Epoch 763/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 656533262.9041\n",
      "Epoch 764/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 659094790.1370\n",
      "Epoch 765/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 671373585.5342\n",
      "Epoch 766/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 663334353.9726\n",
      "Epoch 767/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 656737406.6849\n",
      "Epoch 768/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 659356567.6712\n",
      "Epoch 769/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 656487987.2877\n",
      "Epoch 770/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 655018199.6712\n",
      "Epoch 771/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 667888106.3014\n",
      "Epoch 772/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 657360584.7671\n",
      "Epoch 773/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 660754768.6575\n",
      "Epoch 774/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 660456826.7397\n",
      "Epoch 775/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 651243124.6027\n",
      "Epoch 776/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 670079143.0137\n",
      "Epoch 777/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 666763560.3288\n",
      "Epoch 778/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 656922237.3699\n",
      "Epoch 779/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 667483723.3973\n",
      "Epoch 780/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 660214268.4932\n",
      "Epoch 781/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 661765600.0000\n",
      "Epoch 782/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 657147604.1644\n",
      "Epoch 783/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 669245341.8082\n",
      "Epoch 784/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 650982129.9726\n",
      "Epoch 785/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 656238155.3973\n",
      "Epoch 786/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 645817496.5479\n",
      "Epoch 787/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 642294479.7808\n",
      "Epoch 788/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 649606613.9178\n",
      "Epoch 789/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 648230931.2877\n",
      "Epoch 790/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 646348506.3014\n",
      "Epoch 791/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 642144261.2603\n",
      "Epoch 792/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 641026247.8904\n",
      "Epoch 793/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 660679629.1507\n",
      "Epoch 794/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 643813749.9178\n",
      "Epoch 795/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 636952634.3014\n",
      "Epoch 796/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 634872385.7534\n",
      "Epoch 797/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 656811644.4932\n",
      "Epoch 798/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 632512141.1507\n",
      "Epoch 799/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 644993427.2877\n",
      "Epoch 800/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 634146574.0274\n",
      "Epoch 801/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 637919857.0959\n",
      "Epoch 802/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 629139707.6164\n",
      "Epoch 803/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 649621198.6849\n",
      "Epoch 804/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 656307747.9452\n",
      "Epoch 805/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 631023085.5890\n",
      "Epoch 806/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 625623953.5342\n",
      "Epoch 807/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 636739162.7397\n",
      "Epoch 808/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 629379361.3151\n",
      "Epoch 809/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 637602008.1096\n",
      "Epoch 810/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 624938289.0959\n",
      "Epoch 811/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 636726929.0959\n",
      "Epoch 812/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 633853830.1370\n",
      "Epoch 813/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 622886168.5479\n",
      "Epoch 814/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 624625429.9178\n",
      "Epoch 815/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 623853514.5205\n",
      "Epoch 816/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 635779454.2466\n",
      "Epoch 817/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 623874470.5753\n",
      "Epoch 818/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 627973937.0959\n",
      "Epoch 819/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 650544284.0548\n",
      "Epoch 820/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 642129200.2192\n",
      "Epoch 821/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 625171412.8219\n",
      "Epoch 822/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 640120577.3151\n",
      "Epoch 823/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 638318658.6301\n",
      "Epoch 824/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 650695230.2466\n",
      "Epoch 825/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 621870078.6849\n",
      "Epoch 826/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 619896964.3836\n",
      "Epoch 827/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 615023427.9452\n",
      "Epoch 828/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 18us/sample - loss: 622475693.5890\n",
      "Epoch 829/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 623211150.9041\n",
      "Epoch 830/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 616981501.3699\n",
      "Epoch 831/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 629304537.8630\n",
      "Epoch 832/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 619194955.7260\n",
      "Epoch 833/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 613226139.1781\n",
      "Epoch 834/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 616836545.7534\n",
      "Epoch 835/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 615541814.7945\n",
      "Epoch 836/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 613393729.3151\n",
      "Epoch 837/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 612497520.2192\n",
      "Epoch 838/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 632790104.5479\n",
      "Epoch 839/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 631056616.3288\n",
      "Epoch 840/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 611606682.3014\n",
      "Epoch 841/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 614758837.4795\n",
      "Epoch 842/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 626933696.8767\n",
      "Epoch 843/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 635971854.4658\n",
      "Epoch 844/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 627898151.4521\n",
      "Epoch 845/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 617027100.0548\n",
      "Epoch 846/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 617416128.2192\n",
      "Epoch 847/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 611850880.0000\n",
      "Epoch 848/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 607698982.5753\n",
      "Epoch 849/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 620426686.6849\n",
      "Epoch 850/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 604659015.4521\n",
      "Epoch 851/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 608624540.9315\n",
      "Epoch 852/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 604533052.9315\n",
      "Epoch 853/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 606653205.0411\n",
      "Epoch 854/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 613347591.4521\n",
      "Epoch 855/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 617906439.0137\n",
      "Epoch 856/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 601478153.6438\n",
      "Epoch 857/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 605420510.6849\n",
      "Epoch 858/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 605263305.6438\n",
      "Epoch 859/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 598669180.9315\n",
      "Epoch 860/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 597831845.9178\n",
      "Epoch 861/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 595803630.4658\n",
      "Epoch 862/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 593995320.1096\n",
      "Epoch 863/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 599597276.9315\n",
      "Epoch 864/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 600751484.4932\n",
      "Epoch 865/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 594004916.1644\n",
      "Epoch 866/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 595670879.3425\n",
      "Epoch 867/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 594536516.8219\n",
      "Epoch 868/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 599345939.7260\n",
      "Epoch 869/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 602780120.9863\n",
      "Epoch 870/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 592020172.7123\n",
      "Epoch 871/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 589831995.6164\n",
      "Epoch 872/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 597456137.6438\n",
      "Epoch 873/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 587550693.0411\n",
      "Epoch 874/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 596326880.0000\n",
      "Epoch 875/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 601755266.6301\n",
      "Epoch 876/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 594515403.8356\n",
      "Epoch 877/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 588266688.0000\n",
      "Epoch 878/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 588202635.3973\n",
      "Epoch 879/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 591177557.2603\n",
      "Epoch 880/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 588309722.3014\n",
      "Epoch 881/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 584104960.8767\n",
      "Epoch 882/1100\n",
      "1460/1460 [==============================] - 0s 16us/sample - loss: 584685911.6712\n",
      "Epoch 883/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 591334267.8356\n",
      "Epoch 884/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 583983766.7945\n",
      "Epoch 885/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 591038755.0685\n",
      "Epoch 886/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 596167392.8767\n",
      "Epoch 887/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 605294403.0685\n",
      "Epoch 888/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 591916558.0274\n",
      "Epoch 889/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 594363223.2329\n",
      "Epoch 890/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 586508348.4932\n",
      "Epoch 891/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 589891929.8630\n",
      "Epoch 892/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 581078533.2603\n",
      "Epoch 893/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 587903244.2740\n",
      "Epoch 894/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 577377415.8904\n",
      "Epoch 895/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 592021468.2740\n",
      "Epoch 896/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 605589033.6438\n",
      "Epoch 897/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 578552956.4932\n",
      "Epoch 898/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 571838855.0137\n",
      "Epoch 899/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 586203027.2877\n",
      "Epoch 900/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 585690177.3151\n",
      "Epoch 901/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 588611160.1096\n",
      "Epoch 902/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 570398250.5205\n",
      "Epoch 903/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 578060524.7123\n",
      "Epoch 904/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 593012390.5753\n",
      "Epoch 905/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 584420237.1507\n",
      "Epoch 906/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 572190883.9452\n",
      "Epoch 907/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 569981694.2466\n",
      "Epoch 908/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 570955259.6164\n",
      "Epoch 909/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 579576906.5205\n",
      "Epoch 910/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 574677002.5205\n",
      "Epoch 911/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 19us/sample - loss: 582557948.0548\n",
      "Epoch 912/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 578550080.8767\n",
      "Epoch 913/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 562136572.4932\n",
      "Epoch 914/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 587810011.1781\n",
      "Epoch 915/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 566618515.7260\n",
      "Epoch 916/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 566735089.9726\n",
      "Epoch 917/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 571500242.8493\n",
      "Epoch 918/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 578355859.9452\n",
      "Epoch 919/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 564860058.3014\n",
      "Epoch 920/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 569854005.6986\n",
      "Epoch 921/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 571031310.0274\n",
      "Epoch 922/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 567495137.3151\n",
      "Epoch 923/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 594217094.5753\n",
      "Epoch 924/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 574714165.9178\n",
      "Epoch 925/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 567274318.9041\n",
      "Epoch 926/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 562013118.4658\n",
      "Epoch 927/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 563568924.9315\n",
      "Epoch 928/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 560840580.1644\n",
      "Epoch 929/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 562982743.6712\n",
      "Epoch 930/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 572350738.8493\n",
      "Epoch 931/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 559322133.9178\n",
      "Epoch 932/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 556989078.7945\n",
      "Epoch 933/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 555519849.2055\n",
      "Epoch 934/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 554513001.2055\n",
      "Epoch 935/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 566110760.3288\n",
      "Epoch 936/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 553944850.1918\n",
      "Epoch 937/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 551328180.6027\n",
      "Epoch 938/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 552006837.2603\n",
      "Epoch 939/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 555065412.3836\n",
      "Epoch 940/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 598818838.7945\n",
      "Epoch 941/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 563370908.0548\n",
      "Epoch 942/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 561413967.3425\n",
      "Epoch 943/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 546322915.9452\n",
      "Epoch 944/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 552911173.6986\n",
      "Epoch 945/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 551403303.4521\n",
      "Epoch 946/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 556384412.9315\n",
      "Epoch 947/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 550462322.8493\n",
      "Epoch 948/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 541532419.9452\n",
      "Epoch 949/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 546261739.8356\n",
      "Epoch 950/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 554488244.6027\n",
      "Epoch 951/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 541768629.4795\n",
      "Epoch 952/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 543620426.0822\n",
      "Epoch 953/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 548893962.0822\n",
      "Epoch 954/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 542331052.4932\n",
      "Epoch 955/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 544506378.9589\n",
      "Epoch 956/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 542928865.3151\n",
      "Epoch 957/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 539946237.3699\n",
      "Epoch 958/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 540862088.7671\n",
      "Epoch 959/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 544314121.4247\n",
      "Epoch 960/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 545824050.8493\n",
      "Epoch 961/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 551777763.9452\n",
      "Epoch 962/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 541018529.3151\n",
      "Epoch 963/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 543088513.7534\n",
      "Epoch 964/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 538441244.9315\n",
      "Epoch 965/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 549859974.5753\n",
      "Epoch 966/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 536837236.6027\n",
      "Epoch 967/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 536646341.6986\n",
      "Epoch 968/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 545822350.4658\n",
      "Epoch 969/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 537008874.9589\n",
      "Epoch 970/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 532863988.6027\n",
      "Epoch 971/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 538797375.3425\n",
      "Epoch 972/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 534772084.8219\n",
      "Epoch 973/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 538115170.1918\n",
      "Epoch 974/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 540073258.5205\n",
      "Epoch 975/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 531791745.7534\n",
      "Epoch 976/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 533707021.1507\n",
      "Epoch 977/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 536749732.8219\n",
      "Epoch 978/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 537850535.8904\n",
      "Epoch 979/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 530038290.8493\n",
      "Epoch 980/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 531559245.5890\n",
      "Epoch 981/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 529821816.9863\n",
      "Epoch 982/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 521688125.8082\n",
      "Epoch 983/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 524408852.1644\n",
      "Epoch 984/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 525901828.3836\n",
      "Epoch 985/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 523286421.4795\n",
      "Epoch 986/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 530121154.1918\n",
      "Epoch 987/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 525595089.2055\n",
      "Epoch 988/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 539381204.1644\n",
      "Epoch 989/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 542338604.9315\n",
      "Epoch 990/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 539169114.3014\n",
      "Epoch 991/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 534264416.4384\n",
      "Epoch 992/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 528757163.8356\n",
      "Epoch 993/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 535851507.7260\n",
      "Epoch 994/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 0s 19us/sample - loss: 519334975.5616\n",
      "Epoch 995/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 530856588.7123\n",
      "Epoch 996/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 525055427.5068\n",
      "Epoch 997/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 515338751.1233\n",
      "Epoch 998/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 524981417.2055\n",
      "Epoch 999/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 521093223.8904\n",
      "Epoch 1000/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 526596149.9178\n",
      "Epoch 1001/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 522008428.7123\n",
      "Epoch 1002/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 527731825.0959\n",
      "Epoch 1003/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 522935826.8493\n",
      "Epoch 1004/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 541157969.9726\n",
      "Epoch 1005/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 523302106.7397\n",
      "Epoch 1006/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 508833183.1233\n",
      "Epoch 1007/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 510487535.7808\n",
      "Epoch 1008/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 511726230.7945\n",
      "Epoch 1009/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 507837575.8904\n",
      "Epoch 1010/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 511503243.3973\n",
      "Epoch 1011/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 503661745.9726\n",
      "Epoch 1012/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 507617176.1096\n",
      "Epoch 1013/1100\n",
      "1460/1460 [==============================] - 0s 23us/sample - loss: 510842255.1233\n",
      "Epoch 1014/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 503576276.1644\n",
      "Epoch 1015/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 522599093.0411\n",
      "Epoch 1016/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 523464398.9041\n",
      "Epoch 1017/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 508068830.6849\n",
      "Epoch 1018/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 503460551.8904\n",
      "Epoch 1019/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 520459648.4384\n",
      "Epoch 1020/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 536500887.6712\n",
      "Epoch 1021/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 498993459.5068\n",
      "Epoch 1022/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 501388593.5342\n",
      "Epoch 1023/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 500223260.4932\n",
      "Epoch 1024/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 514197517.5890\n",
      "Epoch 1025/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 509486803.2877\n",
      "Epoch 1026/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 558515726.4658\n",
      "Epoch 1027/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 548329029.6986\n",
      "Epoch 1028/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 498596275.0685\n",
      "Epoch 1029/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 497956285.3699\n",
      "Epoch 1030/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 500586631.8904\n",
      "Epoch 1031/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 498879112.3288\n",
      "Epoch 1032/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 495446385.5342\n",
      "Epoch 1033/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 499617323.8356\n",
      "Epoch 1034/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 499778410.9589\n",
      "Epoch 1035/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 492208583.0137\n",
      "Epoch 1036/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 491385923.9452\n",
      "Epoch 1037/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 494120501.9178\n",
      "Epoch 1038/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 496837433.4247\n",
      "Epoch 1039/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 491459576.1096\n",
      "Epoch 1040/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 498534862.0274\n",
      "Epoch 1041/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 521141370.3014\n",
      "Epoch 1042/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 504971044.8219\n",
      "Epoch 1043/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 498470138.7397\n",
      "Epoch 1044/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 492180652.7123\n",
      "Epoch 1045/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 493163546.3014\n",
      "Epoch 1046/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 488569927.8904\n",
      "Epoch 1047/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 486568327.8904\n",
      "Epoch 1048/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 490581941.4795\n",
      "Epoch 1049/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 487779563.8356\n",
      "Epoch 1050/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 487358392.5479\n",
      "Epoch 1051/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 501477567.1233\n",
      "Epoch 1052/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 510996006.5753\n",
      "Epoch 1053/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 488832077.1507\n",
      "Epoch 1054/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 534931527.0137\n",
      "Epoch 1055/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 515521344.2192\n",
      "Epoch 1056/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 491789812.6027\n",
      "Epoch 1057/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 515571035.1781\n",
      "Epoch 1058/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 492460679.0137\n",
      "Epoch 1059/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 500737252.3836\n",
      "Epoch 1060/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 524447372.2740\n",
      "Epoch 1061/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 506351096.9863\n",
      "Epoch 1062/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 499472828.0548\n",
      "Epoch 1063/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 479453885.3699\n",
      "Epoch 1064/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 485423231.5616\n",
      "Epoch 1065/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 473954762.0822\n",
      "Epoch 1066/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 474864187.6164\n",
      "Epoch 1067/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 473754794.0822\n",
      "Epoch 1068/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 477978531.0685\n",
      "Epoch 1069/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 480916554.5205\n",
      "Epoch 1070/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 481899021.1507\n",
      "Epoch 1071/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 481276520.9863\n",
      "Epoch 1072/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 476167132.9315\n",
      "Epoch 1073/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 484593677.3699\n",
      "Epoch 1074/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 504324713.2055\n",
      "Epoch 1075/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 472062133.4795\n",
      "Epoch 1076/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 494577476.3836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1077/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 487387527.4521\n",
      "Epoch 1078/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 467207521.7534\n",
      "Epoch 1079/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 476018550.3562\n",
      "Epoch 1080/1100\n",
      "1460/1460 [==============================] - 0s 22us/sample - loss: 466592519.6712\n",
      "Epoch 1081/1100\n",
      "1460/1460 [==============================] - 0s 21us/sample - loss: 466376941.5890\n",
      "Epoch 1082/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 479890666.0822\n",
      "Epoch 1083/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 476899900.0548\n",
      "Epoch 1084/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 461608166.5753\n",
      "Epoch 1085/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 479073461.4795\n",
      "Epoch 1086/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 465989689.8630\n",
      "Epoch 1087/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 462053869.1507\n",
      "Epoch 1088/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 457350575.3425\n",
      "Epoch 1089/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 461998008.7671\n",
      "Epoch 1090/1100\n",
      "1460/1460 [==============================] - 0s 20us/sample - loss: 460527246.0274\n",
      "Epoch 1091/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 458911472.4384\n",
      "Epoch 1092/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 462985373.8082\n",
      "Epoch 1093/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 460198300.9315\n",
      "Epoch 1094/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 475946497.7534\n",
      "Epoch 1095/1100\n",
      "1460/1460 [==============================] - 0s 17us/sample - loss: 480565073.0959\n",
      "Epoch 1096/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 459122056.7671\n",
      "Epoch 1097/1100\n",
      "1460/1460 [==============================] - 0s 19us/sample - loss: 464141041.0959\n",
      "Epoch 1098/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 457607351.8904\n",
      "Epoch 1099/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 469480241.9726\n",
      "Epoch 1100/1100\n",
      "1460/1460 [==============================] - 0s 18us/sample - loss: 456724007.0137\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(epochs = [500,600,800,1000,1100,1200],\n",
    "                  batch_size = [ 60, 80, 100])\n",
    "                \n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1,cv=5)\n",
    "grid_result = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 80, 'epochs': 1100}"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1100\n",
      "1168/1168 [==============================] - 0s 141us/sample - loss: 38893501257.6438 - val_loss: 37274883731.2877\n",
      "Epoch 2/1100\n",
      "1168/1168 [==============================] - 0s 18us/sample - loss: 38036205876.6027 - val_loss: 36338817949.8082\n",
      "Epoch 3/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 36782893027.9452 - val_loss: 34902650066.4110\n",
      "Epoch 4/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 34794994786.1918 - val_loss: 32528813322.5205\n",
      "Epoch 5/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 31647038506.0822 - val_loss: 28864099847.0137\n",
      "Epoch 6/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 27126067003.6164 - val_loss: 23875979123.7260\n",
      "Epoch 7/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 21640977590.3562 - val_loss: 18021553713.0959\n",
      "Epoch 8/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 16001356421.2603 - val_loss: 12618317066.5205\n",
      "Epoch 9/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 12157954545.9726 - val_loss: 8755657194.9589\n",
      "Epoch 10/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 10046894430.6849 - val_loss: 6942335410.8493\n",
      "Epoch 11/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 9544532970.9589 - val_loss: 6038095198.6849\n",
      "Epoch 12/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 8919587022.9041 - val_loss: 5803362118.1370\n",
      "Epoch 13/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 8310924870.1370 - val_loss: 5425673223.0137\n",
      "Epoch 14/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 7777595925.0411 - val_loss: 5361850683.6164\n",
      "Epoch 15/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 7272915813.6986 - val_loss: 5096196418.6301\n",
      "Epoch 16/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 6700666950.1370 - val_loss: 4681769275.6164\n",
      "Epoch 17/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 6266596278.3562 - val_loss: 4472093731.0685\n",
      "Epoch 18/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 5823564691.2877 - val_loss: 4226824079.7808\n",
      "Epoch 19/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 5414852183.6712 - val_loss: 3977998693.6986\n",
      "Epoch 20/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 5022697827.9452 - val_loss: 3921338290.8493\n",
      "Epoch 21/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 4663359686.1370 - val_loss: 3721102371.0685\n",
      "Epoch 22/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 4333536021.0411 - val_loss: 3545748711.4521\n",
      "Epoch 23/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 4042183716.8219 - val_loss: 3404583408.2192\n",
      "Epoch 24/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 3776265547.3973 - val_loss: 3270784164.8219\n",
      "Epoch 25/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 3572651188.6027 - val_loss: 3204570841.4247\n",
      "Epoch 26/1100\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1486029568.00 - 0s 24us/sample - loss: 3343028672.8767 - val_loss: 3101419456.8767\n",
      "Epoch 27/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 3200584170.9589 - val_loss: 3000873756.0548\n",
      "Epoch 28/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 3038263276.7123 - val_loss: 2919857797.2603\n",
      "Epoch 29/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 2913077542.5753 - val_loss: 2869785722.7397\n",
      "Epoch 30/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 2797397942.3562 - val_loss: 2796755960.9863\n",
      "Epoch 31/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 2696711764.1644 - val_loss: 2730020835.9452\n",
      "Epoch 32/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 2592374945.3151 - val_loss: 2679435874.1918\n",
      "Epoch 33/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 2524201830.5753 - val_loss: 2630676939.3973\n",
      "Epoch 34/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 2469292573.8082 - val_loss: 2580806224.6575\n",
      "Epoch 35/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 2412199623.8904 - val_loss: 2542678324.6027\n",
      "Epoch 36/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 2376614714.7397 - val_loss: 2509339811.0685\n",
      "Epoch 37/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 2343259576.1096 - val_loss: 2475576614.5753\n",
      "Epoch 38/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 2319891156.1644 - val_loss: 2446426812.4932\n",
      "Epoch 39/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 2288784098.1918 - val_loss: 2423627050.0822\n",
      "Epoch 40/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 2269794256.6575 - val_loss: 2398209939.2877\n",
      "Epoch 41/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 2251492917.4795 - val_loss: 2380694338.6301\n",
      "Epoch 42/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 2240378289.0959 - val_loss: 2362560355.9452\n",
      "Epoch 43/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 2236430528.8767 - val_loss: 2345253386.5205\n",
      "Epoch 44/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 2203627356.0548 - val_loss: 2324608359.4521\n",
      "Epoch 45/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 2188707650.6301 - val_loss: 2311619496.3288\n",
      "Epoch 46/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 2177951570.4110 - val_loss: 2297058184.7671\n",
      "Epoch 47/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 2176083982.0274 - val_loss: 2286998554.3014\n",
      "Epoch 48/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 2165135330.1918 - val_loss: 2267372309.0411\n",
      "Epoch 49/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 2135778057.6438 - val_loss: 2255991173.2603\n",
      "Epoch 50/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 2127173643.3973 - val_loss: 2240581679.3425\n",
      "Epoch 51/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 2110204892.9315 - val_loss: 2226501309.3699\n",
      "Epoch 52/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 2102265100.2740 - val_loss: 2214969701.6986\n",
      "Epoch 53/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 2102055103.1233 - val_loss: 2206633799.0137\n",
      "Epoch 54/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 2088362488.9863 - val_loss: 2193242527.5616\n",
      "Epoch 55/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 2065429072.6575 - val_loss: 2182839671.2329\n",
      "Epoch 56/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 2054936759.2329 - val_loss: 2170320531.2877\n",
      "Epoch 57/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 2047752707.5068 - val_loss: 2157624874.0822\n",
      "Epoch 58/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 2032595287.6712 - val_loss: 2148177155.5068\n",
      "Epoch 59/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 2031764879.7808 - val_loss: 2140329205.4795\n",
      "Epoch 60/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 2020616039.4521 - val_loss: 2131518627.0685\n",
      "Epoch 61/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 2009262951.4521 - val_loss: 2122071380.1644\n",
      "Epoch 62/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 2003941227.8356 - val_loss: 2112246373.6986\n",
      "Epoch 63/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1993052351.1233 - val_loss: 2103580868.3836\n",
      "Epoch 64/1100\n",
      "1168/1168 [==============================] - 0s 18us/sample - loss: 1990396510.6849 - val_loss: 2102704075.3973\n",
      "Epoch 65/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1978656565.4795 - val_loss: 2090711131.1781\n",
      "Epoch 66/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1961730861.5890 - val_loss: 2083210723.9452\n",
      "Epoch 67/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1958872056.1096 - val_loss: 2078736085.9178\n",
      "Epoch 68/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1941880130.6301 - val_loss: 2069491498.9589\n",
      "Epoch 69/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1941157435.6164 - val_loss: 2062215318.7945\n",
      "Epoch 70/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1932217224.7671 - val_loss: 2055460941.1507\n",
      "Epoch 71/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1938916919.2329 - val_loss: 2051366054.5753\n",
      "Epoch 72/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1929563619.9452 - val_loss: 2046741298.8493\n",
      "Epoch 73/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1912085200.6575 - val_loss: 2039977640.3288\n",
      "Epoch 74/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1909118661.2603 - val_loss: 2031951819.3973\n",
      "Epoch 75/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1907267633.9726 - val_loss: 2030203825.0959\n",
      "Epoch 76/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1893465474.6301 - val_loss: 2021788098.6301\n",
      "Epoch 77/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1902924771.9452 - val_loss: 2015903677.3699\n",
      "Epoch 78/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1877892540.4932 - val_loss: 2014479859.7260\n",
      "Epoch 79/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1884586576.6575 - val_loss: 2006808896.8767\n",
      "Epoch 80/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1884381089.3151 - val_loss: 2004598212.3836\n",
      "Epoch 81/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1873667329.7534 - val_loss: 2001203329.7534\n",
      "Epoch 82/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1880116313.4247 - val_loss: 1997025024.0000\n",
      "Epoch 83/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1877977851.6164 - val_loss: 2000750237.8082\n",
      "Epoch 84/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1850374169.4247 - val_loss: 1987258439.8904\n",
      "Epoch 85/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1854852074.0822 - val_loss: 1986050344.3288\n",
      "Epoch 86/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1846599620.3836 - val_loss: 1984983281.0959\n",
      "Epoch 87/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1854710835.7260 - val_loss: 1977749975.6712\n",
      "Epoch 88/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1835169565.8082 - val_loss: 1974516174.9041\n",
      "Epoch 89/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1834131221.0411 - val_loss: 1971580871.8904\n",
      "Epoch 90/1100\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1662877568.00 - 0s 20us/sample - loss: 1837110084.3836 - val_loss: 1970116397.5890\n",
      "Epoch 91/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1824763151.7808 - val_loss: 1967904850.4110\n",
      "Epoch 92/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1825444388.8219 - val_loss: 1966239957.9178\n",
      "Epoch 93/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1817178389.9178 - val_loss: 1962317965.1507\n",
      "Epoch 94/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1823028607.1233 - val_loss: 1964747153.5342\n",
      "Epoch 95/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1807773792.4384 - val_loss: 1962160790.7945\n",
      "Epoch 96/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1809576921.4247 - val_loss: 1953897570.1918\n",
      "Epoch 97/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1800700387.0685 - val_loss: 1956268003.9452\n",
      "Epoch 98/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1801569071.3425 - val_loss: 1949635954.8493\n",
      "Epoch 99/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1804352332.2740 - val_loss: 1949278001.0959\n",
      "Epoch 100/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1792732021.4795 - val_loss: 1946174197.4795\n",
      "Epoch 101/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1794040653.1507 - val_loss: 1946401391.3425\n",
      "Epoch 102/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1793967952.6575 - val_loss: 1944907376.2192\n",
      "Epoch 103/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1785043877.6986 - val_loss: 1942371170.1918\n",
      "Epoch 104/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1783802341.6986 - val_loss: 1940018507.3973\n",
      "Epoch 105/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1780591571.2877 - val_loss: 1935836345.8630\n",
      "Epoch 106/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1777395269.2603 - val_loss: 1935685005.1507\n",
      "Epoch 107/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1772618523.1781 - val_loss: 1935899590.1370\n",
      "Epoch 108/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1798838072.9863 - val_loss: 1932852592.2192\n",
      "Epoch 109/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1773465275.6164 - val_loss: 1931727668.6027\n",
      "Epoch 110/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1766898858.0822 - val_loss: 1929714696.7671\n",
      "Epoch 111/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1780527453.3699 - val_loss: 1929556650.0822\n",
      "Epoch 112/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1770285510.1370 - val_loss: 1926574586.7397\n",
      "Epoch 113/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1777333368.9863 - val_loss: 1930240005.2603\n",
      "Epoch 114/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1757451233.3151 - val_loss: 1936513244.9315\n",
      "Epoch 115/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1770798231.6712 - val_loss: 1925867220.1644\n",
      "Epoch 116/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1760814753.3151 - val_loss: 1921210974.6849\n",
      "Epoch 117/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1751122135.6712 - val_loss: 1922017087.1233\n",
      "Epoch 118/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1751033040.6575 - val_loss: 1919442386.4110\n",
      "Epoch 119/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1756071356.4932 - val_loss: 1917226548.6027\n",
      "Epoch 120/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1756670013.3699 - val_loss: 1918644921.8630\n",
      "Epoch 121/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1745082366.2466 - val_loss: 1917317134.0274\n",
      "Epoch 122/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1744841169.5342 - val_loss: 1916668493.1507\n",
      "Epoch 123/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1741418097.0959 - val_loss: 1914077093.2603\n",
      "Epoch 124/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1750345670.1370 - val_loss: 1915263156.6027\n",
      "Epoch 125/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1731698517.0411 - val_loss: 1913920690.8493\n",
      "Epoch 126/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1735389831.8904 - val_loss: 1911533575.0137\n",
      "Epoch 127/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1739654182.5753 - val_loss: 1910473892.8219\n",
      "Epoch 128/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1737327592.3288 - val_loss: 1909904326.1370\n",
      "Epoch 129/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1735187775.1233 - val_loss: 1908977445.6986\n",
      "Epoch 130/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1755057192.3288 - val_loss: 1906858988.7123\n",
      "Epoch 131/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1733471311.7808 - val_loss: 1910452356.3836\n",
      "Epoch 132/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1761753786.7397 - val_loss: 1904394467.9452\n",
      "Epoch 133/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1753751956.1644 - val_loss: 1903139708.4932\n",
      "Epoch 134/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1723182996.1644 - val_loss: 1903984401.5342\n",
      "Epoch 135/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1732742855.8904 - val_loss: 1902866754.1918\n",
      "Epoch 136/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1730964769.3151 - val_loss: 1900881812.1644\n",
      "Epoch 137/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1717435972.3836 - val_loss: 1899834932.6027\n",
      "Epoch 138/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1715438978.6301 - val_loss: 1897344249.8630\n",
      "Epoch 139/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1721001996.2740 - val_loss: 1896779255.2329\n",
      "Epoch 140/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1709335569.5342 - val_loss: 1896849953.3151\n",
      "Epoch 141/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1717523683.9452 - val_loss: 1895864075.3973\n",
      "Epoch 142/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1715750193.9726 - val_loss: 1893759126.7945\n",
      "Epoch 143/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1734405892.3836 - val_loss: 1894911349.4795\n",
      "Epoch 144/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1742096849.5342 - val_loss: 1890928145.5342\n",
      "Epoch 145/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1715326318.4658 - val_loss: 1891708472.1096\n",
      "Epoch 146/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1702326904.9863 - val_loss: 1886584442.7397\n",
      "Epoch 147/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1696596622.9041 - val_loss: 1889450822.1370\n",
      "Epoch 148/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1702391523.5068 - val_loss: 1890593732.3836\n",
      "Epoch 149/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1703886371.9452 - val_loss: 1885891955.7260\n",
      "Epoch 150/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1694119969.3151 - val_loss: 1885504708.3836\n",
      "Epoch 151/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1692612476.4932 - val_loss: 1885415045.2603\n",
      "Epoch 152/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1690872989.8082 - val_loss: 1886888044.7123\n",
      "Epoch 153/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1689717409.3151 - val_loss: 1884176149.9178\n",
      "Epoch 154/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1682713833.2055 - val_loss: 1883118383.3425\n",
      "Epoch 155/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1693718599.8904 - val_loss: 1881301105.9726\n",
      "Epoch 156/1100\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 1698876975.3425 - val_loss: 1878744506.3014\n",
      "Epoch 157/1100\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 1684202059.3973 - val_loss: 1881222312.3288\n",
      "Epoch 158/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1680210103.2329 - val_loss: 1881503111.0137\n",
      "Epoch 159/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1681261235.7260 - val_loss: 1877293848.5479\n",
      "Epoch 160/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1687475668.1644 - val_loss: 1890352487.4521\n",
      "Epoch 161/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1680283313.0959 - val_loss: 1878551218.8493\n",
      "Epoch 162/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1698948141.5890 - val_loss: 1875609214.2466\n",
      "Epoch 163/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1710137771.8356 - val_loss: 1888807201.3151\n",
      "Epoch 164/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1701950998.7945 - val_loss: 1880966086.1370\n",
      "Epoch 165/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1662452181.0411 - val_loss: 1884281561.4247\n",
      "Epoch 166/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1671612945.5342 - val_loss: 1870155681.3151\n",
      "Epoch 167/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1663998543.7808 - val_loss: 1871848791.6712\n",
      "Epoch 168/1100\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 1695861283.9452 - val_loss: 1882923176.3288\n",
      "Epoch 169/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1693642208.4384 - val_loss: 1872703540.6027\n",
      "Epoch 170/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1660708217.8630 - val_loss: 1870584884.6027\n",
      "Epoch 171/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1659353712.2192 - val_loss: 1873049855.1233\n",
      "Epoch 172/1100\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 1660223123.2877 - val_loss: 1866284877.1507\n",
      "Epoch 173/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1655953329.0959 - val_loss: 1866847772.0548\n",
      "Epoch 174/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1656577663.1233 - val_loss: 1864780088.1096\n",
      "Epoch 175/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1662396364.2740 - val_loss: 1864187351.6712\n",
      "Epoch 176/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1648354704.6575 - val_loss: 1878158793.6438\n",
      "Epoch 177/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1676261269.0411 - val_loss: 1864661255.0137\n",
      "Epoch 178/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1661311505.9726 - val_loss: 1864548571.1781\n",
      "Epoch 179/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1640027711.1233 - val_loss: 1863525077.0411\n",
      "Epoch 180/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1647268231.0137 - val_loss: 1861575690.5205\n",
      "Epoch 181/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1653873169.5342 - val_loss: 1862073154.6301\n",
      "Epoch 182/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1669682633.6438 - val_loss: 1858450358.3562\n",
      "Epoch 183/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1652198933.9178 - val_loss: 1860274607.3425\n",
      "Epoch 184/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1648244697.4247 - val_loss: 1862252551.0137\n",
      "Epoch 185/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1664250653.8082 - val_loss: 1857978602.9589\n",
      "Epoch 186/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1641180782.4658 - val_loss: 1857337095.0137\n",
      "Epoch 187/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1670600181.4795 - val_loss: 1867490851.0685\n",
      "Epoch 188/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1664235894.3562 - val_loss: 1861902083.5068\n",
      "Epoch 189/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1629476518.1370 - val_loss: 1864523574.3562\n",
      "Epoch 190/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1632412664.9863 - val_loss: 1855989893.2603\n",
      "Epoch 191/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1661836191.5616 - val_loss: 1858242417.0959\n",
      "Epoch 192/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1655670019.5068 - val_loss: 1852221722.3014\n",
      "Epoch 193/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1640909090.1918 - val_loss: 1853149094.5753\n",
      "Epoch 194/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1627172131.0685 - val_loss: 1848108449.3151\n",
      "Epoch 195/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1621506038.3562 - val_loss: 1846808328.7671\n",
      "Epoch 196/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1632254698.0822 - val_loss: 1850939174.5753\n",
      "Epoch 197/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1642620328.3288 - val_loss: 1852732349.3699\n",
      "Epoch 198/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1660990521.8630 - val_loss: 1861699114.0822\n",
      "Epoch 199/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1621691800.5479 - val_loss: 1852620360.7671\n",
      "Epoch 200/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1633519177.6438 - val_loss: 1847330165.4795\n",
      "Epoch 201/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1612667182.4658 - val_loss: 1842567127.6712\n",
      "Epoch 202/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1615625450.0822 - val_loss: 1842393793.7534\n",
      "Epoch 203/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1611645598.6849 - val_loss: 1846377235.2877\n",
      "Epoch 204/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1608507124.6027 - val_loss: 1850223079.4521\n",
      "Epoch 205/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1616962558.2466 - val_loss: 1839828174.9041\n",
      "Epoch 206/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1612692953.4247 - val_loss: 1840188321.3151\n",
      "Epoch 207/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1618753351.0137 - val_loss: 1843247256.5479\n",
      "Epoch 208/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1620995150.9041 - val_loss: 1844027623.4521\n",
      "Epoch 209/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1607479393.3151 - val_loss: 1837833935.7808\n",
      "Epoch 210/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1627605900.2740 - val_loss: 1836623581.8082\n",
      "Epoch 211/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1600070095.7808 - val_loss: 1848221376.0000\n",
      "Epoch 212/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1622046651.6164 - val_loss: 1835232394.5205\n",
      "Epoch 213/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1591816276.1644 - val_loss: 1850457187.9452\n",
      "Epoch 214/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1607370784.4384 - val_loss: 1833153697.3151\n",
      "Epoch 215/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1607611630.4658 - val_loss: 1834585116.9315\n",
      "Epoch 216/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1595901703.8904 - val_loss: 1833245387.3973\n",
      "Epoch 217/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1598162591.5616 - val_loss: 1831932203.8356\n",
      "Epoch 218/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1600398855.0137 - val_loss: 1840252896.4384\n",
      "Epoch 219/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1609766001.0959 - val_loss: 1831047829.9178\n",
      "Epoch 220/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1606308586.9589 - val_loss: 1835411316.6027\n",
      "Epoch 221/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1627367682.6301 - val_loss: 1828234755.5068\n",
      "Epoch 222/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1582180485.2603 - val_loss: 1835180218.7397\n",
      "Epoch 223/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1589808611.9452 - val_loss: 1831158141.3699\n",
      "Epoch 224/1100\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 1582205136.6575 - val_loss: 1826010474.9589\n",
      "Epoch 225/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1599429363.7260 - val_loss: 1833529774.4658\n",
      "Epoch 226/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1584536372.6027 - val_loss: 1825062719.1233\n",
      "Epoch 227/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1581532257.7534 - val_loss: 1827452263.4521\n",
      "Epoch 228/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1580986056.7671 - val_loss: 1827418027.8356\n",
      "Epoch 229/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1579408067.5068 - val_loss: 1821683762.8493\n",
      "Epoch 230/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1594101675.8356 - val_loss: 1821778982.5753\n",
      "Epoch 231/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1587785235.2877 - val_loss: 1822204835.0685\n",
      "Epoch 232/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1579190282.5205 - val_loss: 1826928061.3699\n",
      "Epoch 233/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1570348918.3562 - val_loss: 1821022313.2055\n",
      "Epoch 234/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1582328904.7671 - val_loss: 1820867114.0822\n",
      "Epoch 235/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1574346904.5479 - val_loss: 1819978243.5068\n",
      "Epoch 236/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1564909546.9589 - val_loss: 1829443917.1507\n",
      "Epoch 237/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1573179503.3425 - val_loss: 1819904037.6986\n",
      "Epoch 238/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1582093610.9589 - val_loss: 1815247202.1918\n",
      "Epoch 239/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1566465422.0274 - val_loss: 1818731185.9726\n",
      "Epoch 240/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1562972412.4932 - val_loss: 1815615256.5479\n",
      "Epoch 241/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1564304874.0822 - val_loss: 1816609829.6986\n",
      "Epoch 242/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1569025308.0548 - val_loss: 1811884266.9589\n",
      "Epoch 243/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1576928630.3562 - val_loss: 1811613047.2329\n",
      "Epoch 244/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1561497361.5342 - val_loss: 1812122674.8493\n",
      "Epoch 245/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1559858741.4795 - val_loss: 1812395984.6575\n",
      "Epoch 246/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1563494868.1644 - val_loss: 1811685386.5205\n",
      "Epoch 247/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1558096404.1644 - val_loss: 1810415546.7397\n",
      "Epoch 248/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1568547173.6986 - val_loss: 1809044617.6438\n",
      "Epoch 249/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1556789568.0000 - val_loss: 1810029536.4384\n",
      "Epoch 250/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1560740958.6849 - val_loss: 1813694208.0000\n",
      "Epoch 251/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1552032164.8219 - val_loss: 1808966098.4110\n",
      "Epoch 252/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1560447266.1918 - val_loss: 1814800732.0548\n",
      "Epoch 253/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1548058053.2603 - val_loss: 1816338898.4110\n",
      "Epoch 254/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1553089954.1918 - val_loss: 1804889150.2466\n",
      "Epoch 255/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1557395893.9178 - val_loss: 1810236505.4247\n",
      "Epoch 256/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1544475435.8356 - val_loss: 1806422531.5068\n",
      "Epoch 257/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1545598611.2877 - val_loss: 1806103376.6575\n",
      "Epoch 258/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1550933131.3973 - val_loss: 1803379831.2329\n",
      "Epoch 259/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1544881262.4658 - val_loss: 1800950948.8219\n",
      "Epoch 260/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1545587214.9041 - val_loss: 1802038706.8493\n",
      "Epoch 261/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1549676771.0685 - val_loss: 1801656487.8904\n",
      "Epoch 262/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1541204947.2877 - val_loss: 1801655339.8356\n",
      "Epoch 263/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1541200833.7534 - val_loss: 1808582170.3014\n",
      "Epoch 264/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1544369826.1918 - val_loss: 1802808877.5890\n",
      "Epoch 265/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1543872171.8356 - val_loss: 1803962876.4932\n",
      "Epoch 266/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1536523081.6438 - val_loss: 1802597004.2740\n",
      "Epoch 267/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1538354083.0685 - val_loss: 1796032547.0685\n",
      "Epoch 268/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1533754548.6027 - val_loss: 1797667994.3014\n",
      "Epoch 269/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1525863963.1781 - val_loss: 1802604114.4110\n",
      "Epoch 270/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1537493461.9178 - val_loss: 1803750591.1233\n",
      "Epoch 271/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1527903153.9726 - val_loss: 1798178160.2192\n",
      "Epoch 272/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1545515313.9726 - val_loss: 1796925247.1233\n",
      "Epoch 273/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1528383018.9589 - val_loss: 1803178930.8493\n",
      "Epoch 274/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1528257541.2603 - val_loss: 1791747175.4521\n",
      "Epoch 275/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1524370896.6575 - val_loss: 1796871560.7671\n",
      "Epoch 276/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1543241532.4932 - val_loss: 1790943761.5342\n",
      "Epoch 277/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1518882639.7808 - val_loss: 1806913692.0548\n",
      "Epoch 278/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1524016318.2466 - val_loss: 1792723406.0274\n",
      "Epoch 279/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1540582776.9863 - val_loss: 1804990432.4384\n",
      "Epoch 280/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1536406910.2466 - val_loss: 1791070913.7534\n",
      "Epoch 281/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1521796886.7945 - val_loss: 1790036984.1096\n",
      "Epoch 282/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1518854047.5616 - val_loss: 1791879501.1507\n",
      "Epoch 283/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1515379967.1233 - val_loss: 1792875158.7945\n",
      "Epoch 284/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1531815584.4384 - val_loss: 1796669816.9863\n",
      "Epoch 285/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1521217752.5479 - val_loss: 1788328290.1918\n",
      "Epoch 286/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1526522186.5205 - val_loss: 1806255716.8219\n",
      "Epoch 287/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1527849574.5753 - val_loss: 1792692613.2603\n",
      "Epoch 288/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1641213088.4384 - val_loss: 1786395686.5753\n",
      "Epoch 289/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1552170781.8082 - val_loss: 1794562980.8219\n",
      "Epoch 290/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1575840402.4110 - val_loss: 1785495893.9178\n",
      "Epoch 291/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1530620138.0822 - val_loss: 1791446533.2603\n",
      "Epoch 292/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1509944485.6986 - val_loss: 1789075748.8219\n",
      "Epoch 293/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1532548925.3699 - val_loss: 1784358926.0274\n",
      "Epoch 294/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1504333304.1096 - val_loss: 1780810506.5205\n",
      "Epoch 295/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1512566058.9589 - val_loss: 1780200532.1644\n",
      "Epoch 296/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1498715633.0959 - val_loss: 1788322914.1918\n",
      "Epoch 297/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1506180315.1781 - val_loss: 1782912394.5205\n",
      "Epoch 298/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1516063377.5342 - val_loss: 1781625491.2877\n",
      "Epoch 299/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1494314488.1096 - val_loss: 1788772586.9589\n",
      "Epoch 300/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1503611300.8219 - val_loss: 1781925945.8630\n",
      "Epoch 301/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1507282131.2877 - val_loss: 1782514763.3973\n",
      "Epoch 302/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1508212019.7260 - val_loss: 1782746380.2740\n",
      "Epoch 303/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1527595421.8082 - val_loss: 1780335367.0137\n",
      "Epoch 304/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1493658604.7123 - val_loss: 1786755566.4658\n",
      "Epoch 305/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1499624996.8219 - val_loss: 1777107356.9315\n",
      "Epoch 306/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1495090999.2329 - val_loss: 1779555620.8219\n",
      "Epoch 307/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1531168948.6027 - val_loss: 1787726291.2877\n",
      "Epoch 308/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1501292195.9452 - val_loss: 1779436915.7260\n",
      "Epoch 309/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1492848130.6301 - val_loss: 1778113475.5068\n",
      "Epoch 310/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1501818735.3425 - val_loss: 1776677056.8767\n",
      "Epoch 311/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1511578040.1096 - val_loss: 1778003396.3836\n",
      "Epoch 312/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1494944534.7945 - val_loss: 1774369870.9041\n",
      "Epoch 313/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1493065341.3699 - val_loss: 1777867688.3288\n",
      "Epoch 314/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1495651271.4521 - val_loss: 1776575104.0000\n",
      "Epoch 315/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1490956894.6849 - val_loss: 1777323707.6164\n",
      "Epoch 316/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1501222920.7671 - val_loss: 1773869980.0548\n",
      "Epoch 317/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1493777113.4247 - val_loss: 1773027158.7945\n",
      "Epoch 318/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1480699242.0822 - val_loss: 1772088236.7123\n",
      "Epoch 319/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1482025016.1096 - val_loss: 1775051395.5068\n",
      "Epoch 320/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1496519904.4384 - val_loss: 1778117228.7123\n",
      "Epoch 321/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1505296757.4795 - val_loss: 1772253461.0411\n",
      "Epoch 322/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1485738595.9452 - val_loss: 1774614122.9589\n",
      "Epoch 323/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1483011289.4247 - val_loss: 1773028811.3973\n",
      "Epoch 324/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1475756899.0685 - val_loss: 1771844195.0685\n",
      "Epoch 325/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1483161778.8493 - val_loss: 1788282187.3973\n",
      "Epoch 326/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1488083058.8493 - val_loss: 1769650193.5342\n",
      "Epoch 327/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1479381902.0274 - val_loss: 1769328233.2055\n",
      "Epoch 328/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1480719296.8767 - val_loss: 1767460464.2192\n",
      "Epoch 329/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1472914834.4110 - val_loss: 1769668769.3151\n",
      "Epoch 330/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1471460988.4932 - val_loss: 1775151509.0411\n",
      "Epoch 331/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1470468129.7534 - val_loss: 1769697181.8082\n",
      "Epoch 332/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1468520695.2329 - val_loss: 1765750829.5890\n",
      "Epoch 333/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1473691425.3151 - val_loss: 1765435633.9726\n",
      "Epoch 334/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1466090030.9041 - val_loss: 1771920797.8082\n",
      "Epoch 335/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1481456212.1644 - val_loss: 1763205326.0274\n",
      "Epoch 336/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1467028976.2192 - val_loss: 1777771530.5205\n",
      "Epoch 337/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1461269403.1781 - val_loss: 1762729912.1096\n",
      "Epoch 338/1100\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 1475030730.5205 - val_loss: 1763873167.7808\n",
      "Epoch 339/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1476821291.8356 - val_loss: 1770126744.5479\n",
      "Epoch 340/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1470500763.1781 - val_loss: 1764299624.3288\n",
      "Epoch 341/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1471584270.0274 - val_loss: 1761899807.5616\n",
      "Epoch 342/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1479820245.0411 - val_loss: 1765511560.7671\n",
      "Epoch 343/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1464395448.1096 - val_loss: 1760643521.7534\n",
      "Epoch 344/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1479139604.1644 - val_loss: 1764914070.7945\n",
      "Epoch 345/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1477695543.2329 - val_loss: 1765168787.2877\n",
      "Epoch 346/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1512552698.3014 - val_loss: 1768751335.4521\n",
      "Epoch 347/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1481786727.0137 - val_loss: 1760747747.9452\n",
      "Epoch 348/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1458020353.7534 - val_loss: 1758690013.8082\n",
      "Epoch 349/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1447302655.5616 - val_loss: 1773537848.9863\n",
      "Epoch 350/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1464446983.8904 - val_loss: 1757919088.2192\n",
      "Epoch 351/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1452383411.7260 - val_loss: 1768101270.7945\n",
      "Epoch 352/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1456577941.0411 - val_loss: 1756318684.9315\n",
      "Epoch 353/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1466751051.3973 - val_loss: 1765816199.8904\n",
      "Epoch 354/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1469553664.8767 - val_loss: 1756480704.8767\n",
      "Epoch 355/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1456135736.9863 - val_loss: 1755586363.6164\n",
      "Epoch 356/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1458764255.5616 - val_loss: 1758944910.0274\n",
      "Epoch 357/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1454239697.5342 - val_loss: 1757585302.7945\n",
      "Epoch 358/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1467734877.8082 - val_loss: 1763303464.3288\n",
      "Epoch 359/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1504902507.3973 - val_loss: 1758324518.5753\n",
      "Epoch 360/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1456500365.1507 - val_loss: 1758442495.1233\n",
      "Epoch 361/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1448783917.5890 - val_loss: 1754588540.4932\n",
      "Epoch 362/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1439936595.7260 - val_loss: 1772992564.6027\n",
      "Epoch 363/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1456142241.3151 - val_loss: 1751474125.1507\n",
      "Epoch 364/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1459974216.7671 - val_loss: 1753181255.8904\n",
      "Epoch 365/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1453003396.3836 - val_loss: 1757772877.1507\n",
      "Epoch 366/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1478709143.6712 - val_loss: 1749175243.3973\n",
      "Epoch 367/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1470259024.6575 - val_loss: 1767602465.3151\n",
      "Epoch 368/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1443490688.8767 - val_loss: 1755301639.0137\n",
      "Epoch 369/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1443294399.1233 - val_loss: 1756037152.4384\n",
      "Epoch 370/1100\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 1443099618.1918 - val_loss: 1761007149.5890\n",
      "Epoch 371/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1449937918.2466 - val_loss: 1755015455.5616\n",
      "Epoch 372/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1446253724.9315 - val_loss: 1754839502.9041\n",
      "Epoch 373/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1445792180.6027 - val_loss: 1752980458.9589\n",
      "Epoch 374/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1438262623.5616 - val_loss: 1751568899.5068\n",
      "Epoch 375/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1452215999.1233 - val_loss: 1750926656.8767\n",
      "Epoch 376/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1441382262.3562 - val_loss: 1750967406.4658\n",
      "Epoch 377/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1443308807.4521 - val_loss: 1751311937.7534\n",
      "Epoch 378/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1439547202.6301 - val_loss: 1750758205.3699\n",
      "Epoch 379/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1454545223.8904 - val_loss: 1755116540.4932\n",
      "Epoch 380/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1437554585.4247 - val_loss: 1746818952.7671\n",
      "Epoch 381/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1442294464.8767 - val_loss: 1746760297.2055\n",
      "Epoch 382/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1442925048.1096 - val_loss: 1748307693.5890\n",
      "Epoch 383/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1459192223.5616 - val_loss: 1747806193.9726\n",
      "Epoch 384/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1436499750.5753 - val_loss: 1747320708.3836\n",
      "Epoch 385/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1438231741.3699 - val_loss: 1748719098.7397\n",
      "Epoch 386/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1433933784.5479 - val_loss: 1746931985.5342\n",
      "Epoch 387/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1433395215.7808 - val_loss: 1756743965.8082\n",
      "Epoch 388/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1447451152.6575 - val_loss: 1747051709.3699\n",
      "Epoch 389/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1441095298.6301 - val_loss: 1744750104.5479\n",
      "Epoch 390/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1447011803.1781 - val_loss: 1747014803.2877\n",
      "Epoch 391/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1436719224.9863 - val_loss: 1747513557.9178\n",
      "Epoch 392/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1432078863.7808 - val_loss: 1746479502.0274\n",
      "Epoch 393/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1426179403.3973 - val_loss: 1749791030.3562\n",
      "Epoch 394/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1425640221.8082 - val_loss: 1744835073.7534\n",
      "Epoch 395/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1422266120.3288 - val_loss: 1746580444.0548\n",
      "Epoch 396/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1422907125.4795 - val_loss: 1746374703.3425\n",
      "Epoch 397/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1425293318.1370 - val_loss: 1745266626.6301\n",
      "Epoch 398/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1423146535.4521 - val_loss: 1755002460.9315\n",
      "Epoch 399/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1428102475.3973 - val_loss: 1745061810.8493\n",
      "Epoch 400/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1431435326.6849 - val_loss: 1746070415.7808\n",
      "Epoch 401/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1422313241.4247 - val_loss: 1755676003.9452\n",
      "Epoch 402/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1421893033.2055 - val_loss: 1745073747.2877\n",
      "Epoch 403/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1433607917.1507 - val_loss: 1741956402.8493\n",
      "Epoch 404/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1416949421.5890 - val_loss: 1750518049.3151\n",
      "Epoch 405/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1444957606.5753 - val_loss: 1742343920.2192\n",
      "Epoch 406/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1421840409.4247 - val_loss: 1753054204.4932\n",
      "Epoch 407/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1426285489.0959 - val_loss: 1743238686.6849\n",
      "Epoch 408/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1429605839.7808 - val_loss: 1742433718.3562\n",
      "Epoch 409/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1429102684.0548 - val_loss: 1746427778.6301\n",
      "Epoch 410/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1428480497.9726 - val_loss: 1750345226.5205\n",
      "Epoch 411/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1426371412.1644 - val_loss: 1745009131.8356\n",
      "Epoch 412/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1420788472.5479 - val_loss: 1745673128.3288\n",
      "Epoch 413/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1441984297.6438 - val_loss: 1746249266.8493\n",
      "Epoch 414/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1414897052.9315 - val_loss: 1742637183.1233\n",
      "Epoch 415/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1430382428.0548 - val_loss: 1739757036.7123\n",
      "Epoch 416/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1413557010.4110 - val_loss: 1740205087.5616\n",
      "Epoch 417/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1411146357.4795 - val_loss: 1737956408.1096\n",
      "Epoch 418/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1432732079.3425 - val_loss: 1740759271.4521\n",
      "Epoch 419/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1421665344.0000 - val_loss: 1739756023.2329\n",
      "Epoch 420/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1406140501.9178 - val_loss: 1737685090.1918\n",
      "Epoch 421/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1409667490.1918 - val_loss: 1738117005.1507\n",
      "Epoch 422/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1409813923.9452 - val_loss: 1751023478.3562\n",
      "Epoch 423/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1415752220.0548 - val_loss: 1740700933.2603\n",
      "Epoch 424/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1430659024.6575 - val_loss: 1739938332.0548\n",
      "Epoch 425/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1408224714.5205 - val_loss: 1743982111.5616\n",
      "Epoch 426/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1414220099.0685 - val_loss: 1755427280.6575\n",
      "Epoch 427/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1406367929.8630 - val_loss: 1741487729.9726\n",
      "Epoch 428/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1412722117.2603 - val_loss: 1742816090.3014\n",
      "Epoch 429/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1404186178.6301 - val_loss: 1739672591.7808\n",
      "Epoch 430/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1402447522.1918 - val_loss: 1736831814.1370\n",
      "Epoch 431/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1405546760.7671 - val_loss: 1739971676.9315\n",
      "Epoch 432/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1404363969.7534 - val_loss: 1738428855.2329\n",
      "Epoch 433/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1401944032.4384 - val_loss: 1735776140.2740\n",
      "Epoch 434/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1402003268.3836 - val_loss: 1758724800.0000\n",
      "Epoch 435/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1436363955.7260 - val_loss: 1742423861.4795\n",
      "Epoch 436/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1418031146.9589 - val_loss: 1750511044.3836\n",
      "Epoch 437/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1404227243.8356 - val_loss: 1744181630.2466\n",
      "Epoch 438/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1404960751.3425 - val_loss: 1741534527.1233\n",
      "Epoch 439/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1405977651.7260 - val_loss: 1734067005.3699\n",
      "Epoch 440/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1396701918.6849 - val_loss: 1756731413.0411\n",
      "Epoch 441/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1396760423.4521 - val_loss: 1733838325.4795\n",
      "Epoch 442/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1422555820.7123 - val_loss: 1754104914.4110\n",
      "Epoch 443/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1397211373.5890 - val_loss: 1734464270.9041\n",
      "Epoch 444/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1395571151.7808 - val_loss: 1735363384.1096\n",
      "Epoch 445/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1405611384.9863 - val_loss: 1734042304.8767\n",
      "Epoch 446/1100\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 812534656.000 - 0s 21us/sample - loss: 1409894452.6027 - val_loss: 1735649132.7123\n",
      "Epoch 447/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1405276132.8219 - val_loss: 1738233042.4110\n",
      "Epoch 448/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1405352280.5479 - val_loss: 1734239372.2740\n",
      "Epoch 449/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1402758911.1233 - val_loss: 1745264797.8082\n",
      "Epoch 450/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1401473639.4521 - val_loss: 1734857177.4247\n",
      "Epoch 451/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1411894695.4521 - val_loss: 1737366776.1096\n",
      "Epoch 452/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1398965155.9452 - val_loss: 1731745834.9589\n",
      "Epoch 453/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1417885548.7123 - val_loss: 1733853941.4795\n",
      "Epoch 454/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1407717932.7123 - val_loss: 1738949917.8082\n",
      "Epoch 455/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1425575849.2055 - val_loss: 1736107676.0548\n",
      "Epoch 456/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1411840828.0548 - val_loss: 1739046706.8493\n",
      "Epoch 457/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1411682110.2466 - val_loss: 1739696906.5205\n",
      "Epoch 458/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1396340032.0000 - val_loss: 1734088334.9041\n",
      "Epoch 459/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1417293814.3562 - val_loss: 1728695480.9863\n",
      "Epoch 460/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1430948181.9178 - val_loss: 1731111921.9726\n",
      "Epoch 461/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1392115491.0685 - val_loss: 1728101558.3562\n",
      "Epoch 462/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1391704651.3973 - val_loss: 1730407646.6849\n",
      "Epoch 463/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1397668762.3014 - val_loss: 1730222709.4795\n",
      "Epoch 464/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1401502764.7123 - val_loss: 1734749359.3425\n",
      "Epoch 465/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1398941646.9041 - val_loss: 1726238569.2055\n",
      "Epoch 466/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1410756365.1507 - val_loss: 1728166305.3151\n",
      "Epoch 467/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1427462931.7260 - val_loss: 1756035601.5342\n",
      "Epoch 468/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1400150286.0274 - val_loss: 1728791618.6301\n",
      "Epoch 469/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1395660081.9726 - val_loss: 1735520252.4932\n",
      "Epoch 470/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1400576162.1918 - val_loss: 1727600811.8356\n",
      "Epoch 471/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1399874491.6164 - val_loss: 1736026626.6301\n",
      "Epoch 472/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1454466126.0274 - val_loss: 1736519872.0000\n",
      "Epoch 473/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1392848348.9315 - val_loss: 1732692294.1370\n",
      "Epoch 474/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1381843627.8356 - val_loss: 1723522649.4247\n",
      "Epoch 475/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1385861858.6301 - val_loss: 1731404579.0685\n",
      "Epoch 476/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1381265641.2055 - val_loss: 1729285060.3836\n",
      "Epoch 477/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1381617113.8630 - val_loss: 1724095977.2055\n",
      "Epoch 478/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1388066830.9041 - val_loss: 1724274014.6849\n",
      "Epoch 479/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1382845653.0411 - val_loss: 1727315668.1644\n",
      "Epoch 480/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1389293721.4247 - val_loss: 1723509488.2192\n",
      "Epoch 481/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1401117185.7534 - val_loss: 1725396956.9315\n",
      "Epoch 482/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1428193093.2603 - val_loss: 1732154622.2466\n",
      "Epoch 483/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1408435105.3151 - val_loss: 1724964383.5616\n",
      "Epoch 484/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1395709942.3562 - val_loss: 1730885289.6438\n",
      "Epoch 485/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1390796807.0137 - val_loss: 1720153948.9315\n",
      "Epoch 486/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1409117298.8493 - val_loss: 1718190791.8904\n",
      "Epoch 487/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1380061849.4247 - val_loss: 1761850410.0822\n",
      "Epoch 488/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1395751596.7123 - val_loss: 1728973624.9863\n",
      "Epoch 489/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1391917717.9178 - val_loss: 1724968717.1507\n",
      "Epoch 490/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1392781070.9041 - val_loss: 1733490355.7260\n",
      "Epoch 491/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1385911597.5890 - val_loss: 1725303527.4521\n",
      "Epoch 492/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1392769169.9726 - val_loss: 1725763468.2740\n",
      "Epoch 493/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1377049373.3699 - val_loss: 1723500558.0274\n",
      "Epoch 494/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1382978900.1644 - val_loss: 1723730667.8356\n",
      "Epoch 495/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1385777461.4795 - val_loss: 1733716602.7397\n",
      "Epoch 496/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1405209079.2329 - val_loss: 1720193178.3014\n",
      "Epoch 497/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1403507588.3836 - val_loss: 1730628413.3699\n",
      "Epoch 498/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1406643270.1370 - val_loss: 1721206989.1507\n",
      "Epoch 499/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1374410212.8219 - val_loss: 1737337456.2192\n",
      "Epoch 500/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1376810631.8904 - val_loss: 1724520275.2877\n",
      "Epoch 501/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1369944539.1781 - val_loss: 1735727210.0822\n",
      "Epoch 502/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1377267901.3699 - val_loss: 1719789276.9315\n",
      "Epoch 503/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1386710316.7123 - val_loss: 1726984072.7671\n",
      "Epoch 504/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1387412320.0000 - val_loss: 1729913510.5753\n",
      "Epoch 505/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1371868024.1096 - val_loss: 1728634899.2877\n",
      "Epoch 506/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1381022967.6712 - val_loss: 1723091512.1096\n",
      "Epoch 507/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1376453723.1781 - val_loss: 1724668731.6164\n",
      "Epoch 508/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1366040634.7397 - val_loss: 1722736252.4932\n",
      "Epoch 509/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1373248031.5616 - val_loss: 1725321238.7945\n",
      "Epoch 510/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1379018378.9589 - val_loss: 1723256393.6438\n",
      "Epoch 511/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1390071623.8904 - val_loss: 1721965375.1233\n",
      "Epoch 512/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1386122574.9041 - val_loss: 1732211608.5479\n",
      "Epoch 513/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1376153839.3425 - val_loss: 1726294193.9726\n",
      "Epoch 514/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1370150821.6986 - val_loss: 1717046279.0137\n",
      "Epoch 515/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1371904487.4521 - val_loss: 1725452764.0548\n",
      "Epoch 516/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1379351249.5342 - val_loss: 1720011651.5068\n",
      "Epoch 517/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1391809897.2055 - val_loss: 1728137672.7671\n",
      "Epoch 518/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1385085502.2466 - val_loss: 1719613170.8493\n",
      "Epoch 519/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1377505845.4795 - val_loss: 1727581832.7671\n",
      "Epoch 520/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1376831524.8219 - val_loss: 1716496971.3973\n",
      "Epoch 521/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1366845300.6027 - val_loss: 1727040135.0137\n",
      "Epoch 522/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1366488387.5068 - val_loss: 1714232702.2466\n",
      "Epoch 523/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1373456844.2740 - val_loss: 1724955032.5479\n",
      "Epoch 524/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1381764542.2466 - val_loss: 1728041896.3288\n",
      "Epoch 525/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1374507963.6164 - val_loss: 1717856575.1233\n",
      "Epoch 526/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1371370048.0000 - val_loss: 1714192461.1507\n",
      "Epoch 527/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1367573982.6849 - val_loss: 1718291804.9315\n",
      "Epoch 528/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1388756742.1370 - val_loss: 1714767500.2740\n",
      "Epoch 529/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1400819598.9041 - val_loss: 1721063532.7123\n",
      "Epoch 530/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1369294908.0548 - val_loss: 1715234838.7945\n",
      "Epoch 531/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1365974087.8904 - val_loss: 1727266774.7945\n",
      "Epoch 532/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1371416879.3425 - val_loss: 1719779440.2192\n",
      "Epoch 533/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1368155889.9726 - val_loss: 1713327876.3836\n",
      "Epoch 534/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1361473770.9589 - val_loss: 1714277491.7260\n",
      "Epoch 535/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1371953826.1918 - val_loss: 1712069058.6301\n",
      "Epoch 536/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1381700971.8356 - val_loss: 1728807197.8082\n",
      "Epoch 537/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1397756529.0959 - val_loss: 1713116026.7397\n",
      "Epoch 538/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1361795958.3562 - val_loss: 1735187742.6849\n",
      "Epoch 539/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1359761660.4932 - val_loss: 1715538700.2740\n",
      "Epoch 540/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1374063402.9589 - val_loss: 1735740851.7260\n",
      "Epoch 541/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1390316961.3151 - val_loss: 1718856633.8630\n",
      "Epoch 542/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1381521324.7123 - val_loss: 1715598651.6164\n",
      "Epoch 543/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1357143036.9315 - val_loss: 1717331392.8767\n",
      "Epoch 544/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1369375933.3699 - val_loss: 1709254406.1370\n",
      "Epoch 545/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1345762778.3014 - val_loss: 1730756423.0137\n",
      "Epoch 546/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1370407249.5342 - val_loss: 1716077760.8767\n",
      "Epoch 547/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1359246087.8904 - val_loss: 1720879630.0274\n",
      "Epoch 548/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1375265909.9178 - val_loss: 1737241599.1233\n",
      "Epoch 549/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1386830110.2466 - val_loss: 1714495786.0822\n",
      "Epoch 550/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1368504236.7123 - val_loss: 1712946852.8219\n",
      "Epoch 551/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1352147833.8630 - val_loss: 1713558240.4384\n",
      "Epoch 552/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1368845066.5205 - val_loss: 1732524076.7123\n",
      "Epoch 553/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1366100425.6438 - val_loss: 1706940738.6301\n",
      "Epoch 554/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1368612398.4658 - val_loss: 1707138633.6438\n",
      "Epoch 555/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1356778988.7123 - val_loss: 1707229717.0411\n",
      "Epoch 556/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1384934542.9041 - val_loss: 1711965541.6986\n",
      "Epoch 557/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1360313007.3425 - val_loss: 1713910398.2466\n",
      "Epoch 558/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1352962398.6849 - val_loss: 1713054092.2740\n",
      "Epoch 559/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1347698795.8356 - val_loss: 1712452042.5205\n",
      "Epoch 560/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1345908846.0274 - val_loss: 1707234023.4521\n",
      "Epoch 561/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1348889333.4795 - val_loss: 1717183645.8082\n",
      "Epoch 562/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1355486480.6575 - val_loss: 1710230368.4384\n",
      "Epoch 563/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1351585475.9452 - val_loss: 1707747545.4247\n",
      "Epoch 564/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1349330680.1096 - val_loss: 1707746595.0685\n",
      "Epoch 565/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1352751108.3836 - val_loss: 1717141125.2603\n",
      "Epoch 566/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1347865091.5068 - val_loss: 1725686433.3151\n",
      "Epoch 567/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1349731061.4795 - val_loss: 1711712129.7534\n",
      "Epoch 568/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1356097717.4795 - val_loss: 1709050220.7123\n",
      "Epoch 569/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1346298295.2329 - val_loss: 1717323393.7534\n",
      "Epoch 570/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1345672097.3151 - val_loss: 1711689328.2192\n",
      "Epoch 571/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1345598918.1370 - val_loss: 1711656279.6712\n",
      "Epoch 572/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1341265251.9452 - val_loss: 1716607025.0959\n",
      "Epoch 573/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1355232792.5479 - val_loss: 1706269394.4110\n",
      "Epoch 574/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1378843604.1644 - val_loss: 1720898847.5616\n",
      "Epoch 575/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1341190832.2192 - val_loss: 1705157077.9178\n",
      "Epoch 576/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1347472335.7808 - val_loss: 1710022101.9178\n",
      "Epoch 577/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1342863614.2466 - val_loss: 1707852558.0274\n",
      "Epoch 578/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1348532971.8356 - val_loss: 1715787684.8219\n",
      "Epoch 579/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1343078337.7534 - val_loss: 1703237842.4110\n",
      "Epoch 580/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1343527384.5479 - val_loss: 1739737350.1370\n",
      "Epoch 581/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1366473240.5479 - val_loss: 1699227628.7123\n",
      "Epoch 582/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1345358620.0548 - val_loss: 1706747079.0137\n",
      "Epoch 583/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1337240561.9726 - val_loss: 1717471598.4658\n",
      "Epoch 584/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1347222663.8904 - val_loss: 1712071734.3562\n",
      "Epoch 585/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1341653248.8767 - val_loss: 1703024739.9452\n",
      "Epoch 586/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1400320586.5205 - val_loss: 1753392257.7534\n",
      "Epoch 587/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1386515669.0411 - val_loss: 1701381026.1918\n",
      "Epoch 588/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1338858455.6712 - val_loss: 1719651177.2055\n",
      "Epoch 589/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1371471646.6849 - val_loss: 1709176660.1644\n",
      "Epoch 590/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1339931864.5479 - val_loss: 1703519605.4795\n",
      "Epoch 591/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1357084527.7808 - val_loss: 1714431540.6027\n",
      "Epoch 592/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1341880463.7808 - val_loss: 1701845244.4932\n",
      "Epoch 593/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1335730267.1781 - val_loss: 1709147320.1096\n",
      "Epoch 594/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1345786746.7397 - val_loss: 1698779715.5068\n",
      "Epoch 595/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1352305594.7397 - val_loss: 1702457862.1370\n",
      "Epoch 596/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1326138236.0548 - val_loss: 1741201381.6986\n",
      "Epoch 597/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1343487356.4932 - val_loss: 1698615832.5479\n",
      "Epoch 598/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1347429936.2192 - val_loss: 1706051464.7671\n",
      "Epoch 599/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1359507788.2740 - val_loss: 1711354764.2740\n",
      "Epoch 600/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1352333249.7534 - val_loss: 1697349279.5616\n",
      "Epoch 601/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1330081384.3288 - val_loss: 1715042533.6986\n",
      "Epoch 602/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1344404856.1096 - val_loss: 1695377485.1507\n",
      "Epoch 603/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1325975020.7123 - val_loss: 1703883281.5342\n",
      "Epoch 604/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1336685418.0822 - val_loss: 1705116691.2877\n",
      "Epoch 605/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1327861434.3014 - val_loss: 1700575018.9589\n",
      "Epoch 606/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1356760540.9315 - val_loss: 1717257422.9041\n",
      "Epoch 607/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1333103351.2329 - val_loss: 1703801903.3425\n",
      "Epoch 608/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1323239479.2329 - val_loss: 1698700815.7808\n",
      "Epoch 609/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1335965944.1096 - val_loss: 1726143631.7808\n",
      "Epoch 610/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1328447785.2055 - val_loss: 1698039580.0548\n",
      "Epoch 611/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1380606937.4247 - val_loss: 1724377196.7123\n",
      "Epoch 612/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1333437198.9041 - val_loss: 1694791704.5479\n",
      "Epoch 613/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1321442311.8904 - val_loss: 1695738788.8219\n",
      "Epoch 614/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1317170922.9589 - val_loss: 1695618435.0685\n",
      "Epoch 615/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1327660642.1918 - val_loss: 1719718926.0274\n",
      "Epoch 616/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1334267156.6027 - val_loss: 1692366197.4795\n",
      "Epoch 617/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1331667238.5753 - val_loss: 1693419203.5068\n",
      "Epoch 618/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1335422099.7260 - val_loss: 1724095237.2603\n",
      "Epoch 619/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1343659735.6712 - val_loss: 1694710250.9589\n",
      "Epoch 620/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1342511155.7260 - val_loss: 1704739036.0548\n",
      "Epoch 621/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1338386048.0000 - val_loss: 1704551587.9452\n",
      "Epoch 622/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1328388110.0274 - val_loss: 1688959042.6301\n",
      "Epoch 623/1100\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 395820736.000 - 0s 25us/sample - loss: 1327668438.7945 - val_loss: 1705650787.0685\n",
      "Epoch 624/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1317637807.3425 - val_loss: 1690708984.9863\n",
      "Epoch 625/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1319988535.2329 - val_loss: 1701005933.5890\n",
      "Epoch 626/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1324355359.1233 - val_loss: 1690386307.5068\n",
      "Epoch 627/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1318311591.0137 - val_loss: 1692497777.0959\n",
      "Epoch 628/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1327751307.8356 - val_loss: 1696229183.1233\n",
      "Epoch 629/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1315300805.2603 - val_loss: 1686386593.3151\n",
      "Epoch 630/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1325898543.3425 - val_loss: 1695034203.1781\n",
      "Epoch 631/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1334124668.4932 - val_loss: 1712550058.0822\n",
      "Epoch 632/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1353068019.7260 - val_loss: 1686802989.5890\n",
      "Epoch 633/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1313704338.4110 - val_loss: 1690075279.7808\n",
      "Epoch 634/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1311302304.4384 - val_loss: 1687469296.2192\n",
      "Epoch 635/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1327237922.6301 - val_loss: 1687705698.1918\n",
      "Epoch 636/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1321430467.9452 - val_loss: 1680587637.9178\n",
      "Epoch 637/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1325986601.6438 - val_loss: 1699843584.0000\n",
      "Epoch 638/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1313769406.2466 - val_loss: 1690055571.2877\n",
      "Epoch 639/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1306740911.3425 - val_loss: 1690590154.5205\n",
      "Epoch 640/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1308251996.0548 - val_loss: 1702482547.7260\n",
      "Epoch 641/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1307159180.2740 - val_loss: 1685714560.0000\n",
      "Epoch 642/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1321487451.1781 - val_loss: 1681778783.5616\n",
      "Epoch 643/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1319040129.7534 - val_loss: 1682896533.0411\n",
      "Epoch 644/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1313742511.7808 - val_loss: 1691064733.8082\n",
      "Epoch 645/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1327572363.3973 - val_loss: 1681343246.0274\n",
      "Epoch 646/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1300932693.9178 - val_loss: 1685654300.0548\n",
      "Epoch 647/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1307680433.9726 - val_loss: 1680565893.2603\n",
      "Epoch 648/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1298460508.9315 - val_loss: 1695202118.1370\n",
      "Epoch 649/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1314377871.3425 - val_loss: 1685280746.9589\n",
      "Epoch 650/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1314970318.9041 - val_loss: 1680514011.1781\n",
      "Epoch 651/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1295326172.4932 - val_loss: 1700060794.7397\n",
      "Epoch 652/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1302907457.3151 - val_loss: 1678110595.5068\n",
      "Epoch 653/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1321121812.1644 - val_loss: 1681102276.3836\n",
      "Epoch 654/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1323880010.0822 - val_loss: 1689157086.6849\n",
      "Epoch 655/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1295908663.2329 - val_loss: 1678140349.3699\n",
      "Epoch 656/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1313826921.2055 - val_loss: 1692772366.0274\n",
      "Epoch 657/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1311021838.9041 - val_loss: 1677773476.8219\n",
      "Epoch 658/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1299934954.5205 - val_loss: 1678591279.3425\n",
      "Epoch 659/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1297163361.3151 - val_loss: 1675959089.0959\n",
      "Epoch 660/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1296296729.4247 - val_loss: 1686210368.8767\n",
      "Epoch 661/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1298446494.6849 - val_loss: 1682763511.2329\n",
      "Epoch 662/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1289467285.9178 - val_loss: 1676436683.3973\n",
      "Epoch 663/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1288021091.9452 - val_loss: 1676551971.0685\n",
      "Epoch 664/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1289585862.1370 - val_loss: 1688306405.6986\n",
      "Epoch 665/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1291721780.6027 - val_loss: 1672560741.6986\n",
      "Epoch 666/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1300184214.3562 - val_loss: 1676156500.1644\n",
      "Epoch 667/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1288375814.5753 - val_loss: 1671546676.6027\n",
      "Epoch 668/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1307803648.0000 - val_loss: 1718597172.6027\n",
      "Epoch 669/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1282508595.2877 - val_loss: 1671684433.5342\n",
      "Epoch 670/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1313609987.9452 - val_loss: 1726219355.1781\n",
      "Epoch 671/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1320045883.6164 - val_loss: 1677269660.0548\n",
      "Epoch 672/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1311615794.4110 - val_loss: 1690562384.6575\n",
      "Epoch 673/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1291305275.6164 - val_loss: 1662296166.5753\n",
      "Epoch 674/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1318536213.9178 - val_loss: 1665344408.5479\n",
      "Epoch 675/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1309786396.9315 - val_loss: 1669450584.5479\n",
      "Epoch 676/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1280755235.0685 - val_loss: 1679522673.9726\n",
      "Epoch 677/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1279121384.3288 - val_loss: 1662410443.3973\n",
      "Epoch 678/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1292377715.2877 - val_loss: 1664737146.7397\n",
      "Epoch 679/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1300095997.3699 - val_loss: 1660089710.4658\n",
      "Epoch 680/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1287566926.0274 - val_loss: 1670211797.9178\n",
      "Epoch 681/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1277139989.0411 - val_loss: 1667357543.4521\n",
      "Epoch 682/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1274419021.1507 - val_loss: 1666974248.3288\n",
      "Epoch 683/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1278583942.1370 - val_loss: 1657606180.8219\n",
      "Epoch 684/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1271557990.1370 - val_loss: 1680146498.6301\n",
      "Epoch 685/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1275337021.3699 - val_loss: 1657864946.8493\n",
      "Epoch 686/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1314295435.8356 - val_loss: 1675391140.8219\n",
      "Epoch 687/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1267221912.1096 - val_loss: 1654110535.8904\n",
      "Epoch 688/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1282043482.3014 - val_loss: 1656416834.6301\n",
      "Epoch 689/1100\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 1274236247.6712 - val_loss: 1677412629.0411\n",
      "Epoch 690/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1282110211.5068 - val_loss: 1652834556.4932\n",
      "Epoch 691/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1276894079.1233 - val_loss: 1684785671.0137\n",
      "Epoch 692/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1272593428.1644 - val_loss: 1652481356.2740\n",
      "Epoch 693/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1277038426.7397 - val_loss: 1666559281.9726\n",
      "Epoch 694/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1269040538.3014 - val_loss: 1650198324.6027\n",
      "Epoch 695/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1273705986.6301 - val_loss: 1648851433.2055\n",
      "Epoch 696/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1276336970.9589 - val_loss: 1662846949.6986\n",
      "Epoch 697/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1264702912.0000 - val_loss: 1651016725.0411\n",
      "Epoch 698/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1261721992.7671 - val_loss: 1665872936.3288\n",
      "Epoch 699/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1262119266.1918 - val_loss: 1646072570.7397\n",
      "Epoch 700/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1284940988.0548 - val_loss: 1695422768.2192\n",
      "Epoch 701/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1332901129.2055 - val_loss: 1640684312.5479\n",
      "Epoch 702/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1295105122.1918 - val_loss: 1640018414.4658\n",
      "Epoch 703/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1316042202.3014 - val_loss: 1682352941.5890\n",
      "Epoch 704/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1270221838.0274 - val_loss: 1640513991.8904\n",
      "Epoch 705/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1284400987.1781 - val_loss: 1641613553.9726\n",
      "Epoch 706/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1264198609.0959 - val_loss: 1676418040.9863\n",
      "Epoch 707/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1267947996.0548 - val_loss: 1659785522.8493\n",
      "Epoch 708/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1256077059.0685 - val_loss: 1634867109.6986\n",
      "Epoch 709/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1261820309.4795 - val_loss: 1635311803.6164\n",
      "Epoch 710/1100\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 1250663606.3562 - val_loss: 1631622235.1781\n",
      "Epoch 711/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1263087726.0274 - val_loss: 1637172511.5616\n",
      "Epoch 712/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1245733867.8356 - val_loss: 1631838595.5068\n",
      "Epoch 713/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1258116620.2740 - val_loss: 1627490454.7945\n",
      "Epoch 714/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1247961539.0685 - val_loss: 1677864705.7534\n",
      "Epoch 715/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1271183314.4110 - val_loss: 1629473378.1918\n",
      "Epoch 716/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1246139173.2603 - val_loss: 1651370296.9863\n",
      "Epoch 717/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1258444269.1507 - val_loss: 1646732133.6986\n",
      "Epoch 718/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1247780686.9041 - val_loss: 1625524004.8219\n",
      "Epoch 719/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1236563479.6712 - val_loss: 1641357774.9041\n",
      "Epoch 720/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1242737575.4521 - val_loss: 1625176651.3973\n",
      "Epoch 721/1100\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1003022720.00 - 0s 23us/sample - loss: 1239046741.0411 - val_loss: 1664149702.1370\n",
      "Epoch 722/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1256883098.3014 - val_loss: 1623308359.8904\n",
      "Epoch 723/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1255228881.5342 - val_loss: 1625966777.8630\n",
      "Epoch 724/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1238264587.3973 - val_loss: 1634437619.7260\n",
      "Epoch 725/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1239196990.6849 - val_loss: 1617651361.3151\n",
      "Epoch 726/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1233149228.2740 - val_loss: 1616908050.4110\n",
      "Epoch 727/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1225892771.0685 - val_loss: 1634366227.2877\n",
      "Epoch 728/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1238008504.1096 - val_loss: 1625613273.4247\n",
      "Epoch 729/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1237511747.5068 - val_loss: 1610512752.2192\n",
      "Epoch 730/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1251479036.9315 - val_loss: 1621728827.6164\n",
      "Epoch 731/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1252348642.6301 - val_loss: 1632949381.2603\n",
      "Epoch 732/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1237138720.4384 - val_loss: 1620991821.1507\n",
      "Epoch 733/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1233598375.4521 - val_loss: 1607179733.9178\n",
      "Epoch 734/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1228291504.6575 - val_loss: 1612597402.3014\n",
      "Epoch 735/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1220497841.0959 - val_loss: 1610689870.0274\n",
      "Epoch 736/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1213476302.9041 - val_loss: 1606638087.8904\n",
      "Epoch 737/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1221560968.7671 - val_loss: 1599950954.0822\n",
      "Epoch 738/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1202033092.3836 - val_loss: 1635497099.3973\n",
      "Epoch 739/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1215536082.8493 - val_loss: 1603588562.8493\n",
      "Epoch 740/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1214038018.6301 - val_loss: 1609972856.9863\n",
      "Epoch 741/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1211916012.7123 - val_loss: 1602228869.2603\n",
      "Epoch 742/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1264450379.3973 - val_loss: 1626973657.4247\n",
      "Epoch 743/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1220396716.7123 - val_loss: 1596811281.5342\n",
      "Epoch 744/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1219189618.8493 - val_loss: 1601060537.8630\n",
      "Epoch 745/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1215798527.1233 - val_loss: 1591221949.3699\n",
      "Epoch 746/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1208957152.4384 - val_loss: 1595336167.4521\n",
      "Epoch 747/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1201876106.5205 - val_loss: 1594639282.8493\n",
      "Epoch 748/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1206177423.3425 - val_loss: 1595748767.5616\n",
      "Epoch 749/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1209644521.6438 - val_loss: 1592545362.4110\n",
      "Epoch 750/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1197936546.1918 - val_loss: 1600415689.6438\n",
      "Epoch 751/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1207927125.9178 - val_loss: 1586197614.4658\n",
      "Epoch 752/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1196097876.1644 - val_loss: 1606705325.5890\n",
      "Epoch 753/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1189926590.6849 - val_loss: 1591952262.1370\n",
      "Epoch 754/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1204659214.9041 - val_loss: 1642035129.8630\n",
      "Epoch 755/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1231417463.6712 - val_loss: 1583070232.5479\n",
      "Epoch 756/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1188307813.6986 - val_loss: 1640017001.2055\n",
      "Epoch 757/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1197773746.8493 - val_loss: 1586581540.8219\n",
      "Epoch 758/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1206701213.8082 - val_loss: 1610247827.2877\n",
      "Epoch 759/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1224078924.2740 - val_loss: 1583472580.3836\n",
      "Epoch 760/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1182736700.4932 - val_loss: 1614748664.9863\n",
      "Epoch 761/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1193095550.6849 - val_loss: 1580546491.6164\n",
      "Epoch 762/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1190211711.5616 - val_loss: 1607453627.6164\n",
      "Epoch 763/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1200983241.2055 - val_loss: 1578945008.2192\n",
      "Epoch 764/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1177982910.2466 - val_loss: 1603208886.3562\n",
      "Epoch 765/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1182342723.9452 - val_loss: 1580561906.8493\n",
      "Epoch 766/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1186673456.2192 - val_loss: 1579854340.3836\n",
      "Epoch 767/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1184570947.5068 - val_loss: 1579127249.5342\n",
      "Epoch 768/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1179930409.2055 - val_loss: 1573187061.4795\n",
      "Epoch 769/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1188520724.6027 - val_loss: 1598353450.0822\n",
      "Epoch 770/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1197189256.7671 - val_loss: 1568189660.9315\n",
      "Epoch 771/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1170503338.9589 - val_loss: 1591722778.3014\n",
      "Epoch 772/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1168547893.4795 - val_loss: 1569619987.2877\n",
      "Epoch 773/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1176822941.8082 - val_loss: 1586370811.6164\n",
      "Epoch 774/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1180415943.4521 - val_loss: 1564232952.1096\n",
      "Epoch 775/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1169338223.3425 - val_loss: 1586818552.9863\n",
      "Epoch 776/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1175318928.6575 - val_loss: 1559407807.1233\n",
      "Epoch 777/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1176391733.4795 - val_loss: 1563166120.3288\n",
      "Epoch 778/1100\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 1171480492.2740 - val_loss: 1561916138.9589\n",
      "Epoch 779/1100\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 1209342044.0548 - val_loss: 1605908057.4247\n",
      "Epoch 780/1100\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 1189529364.1644 - val_loss: 1561119538.8493\n",
      "Epoch 781/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1168829450.9589 - val_loss: 1566837896.7671\n",
      "Epoch 782/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1151063239.0137 - val_loss: 1563411507.7260\n",
      "Epoch 783/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1156373408.8767 - val_loss: 1555315316.6027\n",
      "Epoch 784/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1152438855.8904 - val_loss: 1555860743.0137\n",
      "Epoch 785/1100\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 1146837138.8493 - val_loss: 1564719230.2466\n",
      "Epoch 786/1100\n",
      "1168/1168 [==============================] - 0s 28us/sample - loss: 1154819146.5205 - val_loss: 1551218490.7397\n",
      "Epoch 787/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1145150286.9041 - val_loss: 1558837907.2877\n",
      "Epoch 788/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1141411049.2055 - val_loss: 1554049693.8082\n",
      "Epoch 789/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1137596005.6986 - val_loss: 1598423699.2877\n",
      "Epoch 790/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1158906769.0959 - val_loss: 1567852209.9726\n",
      "Epoch 791/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1142931121.9726 - val_loss: 1563538270.6849\n",
      "Epoch 792/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1147004086.7945 - val_loss: 1546045152.4384\n",
      "Epoch 793/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1155182070.3562 - val_loss: 1619040995.9452\n",
      "Epoch 794/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1156795387.1781 - val_loss: 1566622137.8630\n",
      "Epoch 795/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1132915239.4521 - val_loss: 1551999894.7945\n",
      "Epoch 796/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1133261179.6164 - val_loss: 1559399210.9589\n",
      "Epoch 797/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1124055629.1507 - val_loss: 1554787471.7808\n",
      "Epoch 798/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1133380626.4110 - val_loss: 1580474528.4384\n",
      "Epoch 799/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1130621196.7123 - val_loss: 1552298812.4932\n",
      "Epoch 800/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1127743896.5479 - val_loss: 1558572487.8904\n",
      "Epoch 801/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1122107134.2466 - val_loss: 1569169600.8767\n",
      "Epoch 802/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1119897943.6712 - val_loss: 1552700305.5342\n",
      "Epoch 803/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1129169328.2192 - val_loss: 1541139047.4521\n",
      "Epoch 804/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1126710611.7260 - val_loss: 1556451163.1781\n",
      "Epoch 805/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1110086420.1644 - val_loss: 1565224993.3151\n",
      "Epoch 806/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1108105039.7808 - val_loss: 1550095429.2603\n",
      "Epoch 807/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1106736846.9041 - val_loss: 1539519690.5205\n",
      "Epoch 808/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1105393192.7671 - val_loss: 1554733784.9863\n",
      "Epoch 809/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1101328487.0137 - val_loss: 1550766727.0137\n",
      "Epoch 810/1100\n",
      "1168/1168 [==============================] - 0s 18us/sample - loss: 1101900565.0411 - val_loss: 1544253802.9589\n",
      "Epoch 811/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1101477543.4521 - val_loss: 1544492935.0137\n",
      "Epoch 812/1100\n",
      "1168/1168 [==============================] - 0s 18us/sample - loss: 1111615676.4932 - val_loss: 1567673976.9863\n",
      "Epoch 813/1100\n",
      "1168/1168 [==============================] - 0s 18us/sample - loss: 1104333105.9726 - val_loss: 1540314396.0548\n",
      "Epoch 814/1100\n",
      "1168/1168 [==============================] - 0s 18us/sample - loss: 1109540099.5068 - val_loss: 1555241391.3425\n",
      "Epoch 815/1100\n",
      "1168/1168 [==============================] - 0s 18us/sample - loss: 1108066178.1918 - val_loss: 1568540861.3699\n",
      "Epoch 816/1100\n",
      "1168/1168 [==============================] - 0s 18us/sample - loss: 1119581145.4247 - val_loss: 1543081343.1233\n",
      "Epoch 817/1100\n",
      "1168/1168 [==============================] - 0s 18us/sample - loss: 1104025678.0274 - val_loss: 1556322652.9315\n",
      "Epoch 818/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1095567473.9726 - val_loss: 1531496507.6164\n",
      "Epoch 819/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1097874287.7808 - val_loss: 1552104517.2603\n",
      "Epoch 820/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1090295219.2877 - val_loss: 1537730156.7123\n",
      "Epoch 821/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1084988963.5068 - val_loss: 1537825204.6027\n",
      "Epoch 822/1100\n",
      "1168/1168 [==============================] - 0s 18us/sample - loss: 1079469315.9452 - val_loss: 1540517253.2603\n",
      "Epoch 823/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1085913948.9315 - val_loss: 1548784440.1096\n",
      "Epoch 824/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1097244945.0959 - val_loss: 1528151384.5479\n",
      "Epoch 825/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1102934275.9452 - val_loss: 1581703755.3973\n",
      "Epoch 826/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1129466223.7808 - val_loss: 1518244721.9726\n",
      "Epoch 827/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1094800268.2740 - val_loss: 1524982691.0685\n",
      "Epoch 828/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1148910743.6712 - val_loss: 1645300462.4658\n",
      "Epoch 829/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1117546091.3973 - val_loss: 1526573608.3288\n",
      "Epoch 830/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1092629312.4384 - val_loss: 1575228012.7123\n",
      "Epoch 831/1100\n",
      "1168/1168 [==============================] - 0s 26us/sample - loss: 1105490594.6301 - val_loss: 1520550526.2466\n",
      "Epoch 832/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1100646601.2055 - val_loss: 1513125255.0137\n",
      "Epoch 833/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1088486046.2466 - val_loss: 1534339915.3973\n",
      "Epoch 834/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1059590566.5753 - val_loss: 1518255934.2466\n",
      "Epoch 835/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1063028924.0548 - val_loss: 1573498481.9726\n",
      "Epoch 836/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1087176426.5205 - val_loss: 1514390468.3836\n",
      "Epoch 837/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1083825785.8630 - val_loss: 1564041032.7671\n",
      "Epoch 838/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1057584437.4795 - val_loss: 1514892579.0685\n",
      "Epoch 839/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1064214070.7945 - val_loss: 1525440278.7945\n",
      "Epoch 840/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1088439463.4521 - val_loss: 1531666422.3562\n",
      "Epoch 841/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1061422761.2055 - val_loss: 1505507619.0685\n",
      "Epoch 842/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1066173560.5479 - val_loss: 1515679152.2192\n",
      "Epoch 843/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1053423197.3699 - val_loss: 1504732556.2740\n",
      "Epoch 844/1100\n",
      "1168/1168 [==============================] - 0s 27us/sample - loss: 1051342015.1233 - val_loss: 1511316906.9589\n",
      "Epoch 845/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1051828206.9041 - val_loss: 1504347556.8219\n",
      "Epoch 846/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 1053574292.6027 - val_loss: 1547911061.0411\n",
      "Epoch 847/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1059813733.6986 - val_loss: 1497147676.0548\n",
      "Epoch 848/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1050523486.6849 - val_loss: 1517184533.0411\n",
      "Epoch 849/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1048184141.1507 - val_loss: 1513821859.9452\n",
      "Epoch 850/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1054912755.2877 - val_loss: 1512795417.4247\n",
      "Epoch 851/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1046700067.5068 - val_loss: 1506269435.6164\n",
      "Epoch 852/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1028854191.3425 - val_loss: 1520390799.7808\n",
      "Epoch 853/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1063230214.1370 - val_loss: 1494625332.6027\n",
      "Epoch 854/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1051619559.4521 - val_loss: 1507591550.2466\n",
      "Epoch 855/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1031810194.8493 - val_loss: 1511493212.9315\n",
      "Epoch 856/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1032054799.3425 - val_loss: 1496269508.3836\n",
      "Epoch 857/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1030671591.4521 - val_loss: 1518836374.7945\n",
      "Epoch 858/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1028161602.1918 - val_loss: 1513313367.6712\n",
      "Epoch 859/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1033147704.9863 - val_loss: 1508432373.4795\n",
      "Epoch 860/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1029606994.4110 - val_loss: 1514200774.1370\n",
      "Epoch 861/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1019492778.0822 - val_loss: 1487822669.1507\n",
      "Epoch 862/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 1028248380.4932 - val_loss: 1502545779.7260\n",
      "Epoch 863/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1023640418.6301 - val_loss: 1525982954.9589\n",
      "Epoch 864/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1020892940.7123 - val_loss: 1493824853.9178\n",
      "Epoch 865/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1026751717.2603 - val_loss: 1505307944.3288\n",
      "Epoch 866/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1029179010.6301 - val_loss: 1531104589.1507\n",
      "Epoch 867/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1046434577.9726 - val_loss: 1480527265.3151\n",
      "Epoch 868/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1010470581.4795 - val_loss: 1520286905.8630\n",
      "Epoch 869/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1018937419.3973 - val_loss: 1499242415.3425\n",
      "Epoch 870/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1015427301.2603 - val_loss: 1491912320.0000\n",
      "Epoch 871/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1007347454.6849 - val_loss: 1533411205.2603\n",
      "Epoch 872/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1017860943.7808 - val_loss: 1485559853.5890\n",
      "Epoch 873/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1014405234.8493 - val_loss: 1497806926.9041\n",
      "Epoch 874/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1004764352.4384 - val_loss: 1483801802.5205\n",
      "Epoch 875/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1004960059.1781 - val_loss: 1504007148.7123\n",
      "Epoch 876/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1008796284.0548 - val_loss: 1475299266.6301\n",
      "Epoch 877/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 1006417466.7397 - val_loss: 1500805053.3699\n",
      "Epoch 878/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1000813981.3699 - val_loss: 1479366391.2329\n",
      "Epoch 879/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 1026553800.7671 - val_loss: 1479864639.1233\n",
      "Epoch 880/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1021392524.7123 - val_loss: 1468454191.3425\n",
      "Epoch 881/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1014147075.9452 - val_loss: 1544519066.3014\n",
      "Epoch 882/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 985767138.1918 - val_loss: 1465164586.9589\n",
      "Epoch 883/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1021085289.2055 - val_loss: 1493882464.4384\n",
      "Epoch 884/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1020860557.1507 - val_loss: 1497798814.6849\n",
      "Epoch 885/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1029502129.5342 - val_loss: 1547124867.5068\n",
      "Epoch 886/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 992153483.3973 - val_loss: 1469486307.9452\n",
      "Epoch 887/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1004262891.8356 - val_loss: 1528616284.9315\n",
      "Epoch 888/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 987912287.5616 - val_loss: 1465544854.7945\n",
      "Epoch 889/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 1001366681.8630 - val_loss: 1467627891.7260\n",
      "Epoch 890/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 982234592.0000 - val_loss: 1484464266.5205\n",
      "Epoch 891/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 982764302.9041 - val_loss: 1498778890.5205\n",
      "Epoch 892/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 994814720.8767 - val_loss: 1471600832.8767\n",
      "Epoch 893/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 979922358.7945 - val_loss: 1491188984.9863\n",
      "Epoch 894/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 979796448.8767 - val_loss: 1490641581.5890\n",
      "Epoch 895/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 974589536.8767 - val_loss: 1462282711.6712\n",
      "Epoch 896/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 989621478.1370 - val_loss: 1493122007.6712\n",
      "Epoch 897/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 21us/sample - loss: 974928044.7123 - val_loss: 1461875281.5342\n",
      "Epoch 898/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 992649837.5890 - val_loss: 1515874750.2466\n",
      "Epoch 899/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 976247242.9589 - val_loss: 1467441802.5205\n",
      "Epoch 900/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 988560680.7671 - val_loss: 1495768086.7945\n",
      "Epoch 901/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 984185951.5616 - val_loss: 1462394933.4795\n",
      "Epoch 902/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 966672577.7534 - val_loss: 1462191160.1096\n",
      "Epoch 903/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 980332532.1644 - val_loss: 1505965795.9452\n",
      "Epoch 904/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 987233299.7260 - val_loss: 1456471204.8219\n",
      "Epoch 905/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 969548015.3425 - val_loss: 1457252923.6164\n",
      "Epoch 906/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 977099854.0274 - val_loss: 1452204929.7534\n",
      "Epoch 907/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 961943463.4521 - val_loss: 1468926234.3014\n",
      "Epoch 908/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 962639382.3562 - val_loss: 1448623765.9178\n",
      "Epoch 909/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 954306101.9178 - val_loss: 1487604602.7397\n",
      "Epoch 910/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 953907461.6986 - val_loss: 1456161686.7945\n",
      "Epoch 911/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 961480337.5342 - val_loss: 1459956551.8904\n",
      "Epoch 912/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 979874655.1233 - val_loss: 1447799902.6849\n",
      "Epoch 913/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 1007629500.9315 - val_loss: 1469086983.8904\n",
      "Epoch 914/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 980678536.7671 - val_loss: 1509973665.3151\n",
      "Epoch 915/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 1007881141.4795 - val_loss: 1451456043.8356\n",
      "Epoch 916/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 968391860.6027 - val_loss: 1465122865.0959\n",
      "Epoch 917/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 961494015.1233 - val_loss: 1449822320.2192\n",
      "Epoch 918/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 938618181.2603 - val_loss: 1501685746.8493\n",
      "Epoch 919/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 961490854.1370 - val_loss: 1487232994.1918\n",
      "Epoch 920/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 946918719.5616 - val_loss: 1455347352.5479\n",
      "Epoch 921/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 957780824.1096 - val_loss: 1442129731.0685\n",
      "Epoch 922/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 946466041.8630 - val_loss: 1486895188.1644\n",
      "Epoch 923/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 957259181.5890 - val_loss: 1467105179.6164\n",
      "Epoch 924/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 947649505.3151 - val_loss: 1452640329.6438\n",
      "Epoch 925/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 941831797.9178 - val_loss: 1445953311.5616\n",
      "Epoch 926/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 957856551.4521 - val_loss: 1521817279.1233\n",
      "Epoch 927/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 947700864.4384 - val_loss: 1435764738.6301\n",
      "Epoch 928/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 949662542.4658 - val_loss: 1496074822.1370\n",
      "Epoch 929/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 960503993.8630 - val_loss: 1488672135.0137\n",
      "Epoch 930/1100\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 740333888.000 - 0s 21us/sample - loss: 948493806.4658 - val_loss: 1443354571.3973\n",
      "Epoch 931/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 943762474.0822 - val_loss: 1499303741.3699\n",
      "Epoch 932/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 929570463.5616 - val_loss: 1429416761.8630\n",
      "Epoch 933/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 929084628.6027 - val_loss: 1452465656.9863\n",
      "Epoch 934/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 928737626.3014 - val_loss: 1440709921.3151\n",
      "Epoch 935/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 961030901.4795 - val_loss: 1541354232.1096\n",
      "Epoch 936/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 926535671.2329 - val_loss: 1429297094.1370\n",
      "Epoch 937/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 947151919.3425 - val_loss: 1430233492.1644\n",
      "Epoch 938/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 938235015.4521 - val_loss: 1429477559.2329\n",
      "Epoch 939/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 932348994.6301 - val_loss: 1524306968.5479\n",
      "Epoch 940/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 938954272.0000 - val_loss: 1433600995.0685\n",
      "Epoch 941/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 928437217.7534 - val_loss: 1426598836.6027\n",
      "Epoch 942/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 920907021.1507 - val_loss: 1446589436.4932\n",
      "Epoch 943/1100\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 656544064.000 - 0s 20us/sample - loss: 930113020.4932 - val_loss: 1429264434.8493\n",
      "Epoch 944/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 935562385.9726 - val_loss: 1471747871.5616\n",
      "Epoch 945/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 925795046.1370 - val_loss: 1463001073.9726\n",
      "Epoch 946/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 917706639.7808 - val_loss: 1429854483.2877\n",
      "Epoch 947/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 924017064.3288 - val_loss: 1449905153.7534\n",
      "Epoch 948/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 909952289.7534 - val_loss: 1414464640.0000\n",
      "Epoch 949/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 967913135.7808 - val_loss: 1436072572.4932\n",
      "Epoch 950/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 978414609.9726 - val_loss: 1584913757.8082\n",
      "Epoch 951/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 926044117.9178 - val_loss: 1441080896.8767\n",
      "Epoch 952/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 991843257.8630 - val_loss: 1471910329.8630\n",
      "Epoch 953/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 967337401.8630 - val_loss: 1493772584.3288\n",
      "Epoch 954/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 924304778.0822 - val_loss: 1427316534.3562\n",
      "Epoch 955/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 909055489.7534 - val_loss: 1433990175.5616\n",
      "Epoch 956/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 912099848.3288 - val_loss: 1458656240.2192\n",
      "Epoch 957/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 910654086.1370 - val_loss: 1426324209.9726\n",
      "Epoch 958/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 918683853.5890 - val_loss: 1413991464.3288\n",
      "Epoch 959/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 898890411.3973 - val_loss: 1493029474.1918\n",
      "Epoch 960/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 912444901.2603 - val_loss: 1450651804.0548\n",
      "Epoch 961/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 21us/sample - loss: 903438664.3288 - val_loss: 1420877231.7808\n",
      "Epoch 962/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 934147046.1370 - val_loss: 1426153328.2192\n",
      "Epoch 963/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 908487832.5479 - val_loss: 1523261114.7397\n",
      "Epoch 964/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 933231027.2877 - val_loss: 1438352129.7534\n",
      "Epoch 965/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 899785390.4658 - val_loss: 1423851788.2740\n",
      "Epoch 966/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 894227844.8219 - val_loss: 1421946973.8082\n",
      "Epoch 967/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 895444955.1781 - val_loss: 1433308468.6027\n",
      "Epoch 968/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 893238752.4384 - val_loss: 1435320972.2740\n",
      "Epoch 969/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 896684122.7397 - val_loss: 1493401235.2877\n",
      "Epoch 970/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 909990232.1096 - val_loss: 1426852669.3699\n",
      "Epoch 971/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 895055615.5616 - val_loss: 1416607031.2329\n",
      "Epoch 972/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 904421728.4384 - val_loss: 1490281589.4795\n",
      "Epoch 973/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 911711914.0822 - val_loss: 1481997525.9178\n",
      "Epoch 974/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 890034984.7671 - val_loss: 1422952231.4521\n",
      "Epoch 975/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 889073884.0548 - val_loss: 1424470841.8630\n",
      "Epoch 976/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 894682840.1096 - val_loss: 1419034674.8493\n",
      "Epoch 977/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 927222721.7534 - val_loss: 1468679642.3014\n",
      "Epoch 978/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 894453969.5342 - val_loss: 1434056239.7808\n",
      "Epoch 979/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 905666771.2877 - val_loss: 1432292702.6849\n",
      "Epoch 980/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 884097187.0685 - val_loss: 1428532028.4932\n",
      "Epoch 981/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 897323827.7260 - val_loss: 1525417615.7808\n",
      "Epoch 982/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 895255979.3973 - val_loss: 1404093119.1233\n",
      "Epoch 983/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 876639133.8082 - val_loss: 1523487956.1644\n",
      "Epoch 984/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 925831718.1370 - val_loss: 1424951193.4247\n",
      "Epoch 985/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 911916621.5890 - val_loss: 1402914695.4521\n",
      "Epoch 986/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 899089978.3014 - val_loss: 1406515302.5753\n",
      "Epoch 987/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 894820013.5890 - val_loss: 1412054982.1370\n",
      "Epoch 988/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 932394605.5890 - val_loss: 1565161977.8630\n",
      "Epoch 989/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 938889922.6301 - val_loss: 1421911681.7534\n",
      "Epoch 990/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 897249841.5342 - val_loss: 1430110797.1507\n",
      "Epoch 991/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 868460090.7397 - val_loss: 1416177315.0685\n",
      "Epoch 992/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 871799005.3699 - val_loss: 1406643580.9315\n",
      "Epoch 993/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 907277791.1233 - val_loss: 1440003220.1644\n",
      "Epoch 994/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 873795149.5890 - val_loss: 1456601379.0685\n",
      "Epoch 995/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 870321438.6849 - val_loss: 1412752780.2740\n",
      "Epoch 996/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 877062574.9041 - val_loss: 1441974840.9863\n",
      "Epoch 997/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 856589315.0685 - val_loss: 1410355410.4110\n",
      "Epoch 998/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 868337006.9041 - val_loss: 1415666887.8904\n",
      "Epoch 999/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 865373552.6575 - val_loss: 1469929700.8219\n",
      "Epoch 1000/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 863726053.2603 - val_loss: 1417319310.9041\n",
      "Epoch 1001/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 859755555.9452 - val_loss: 1410868579.0685\n",
      "Epoch 1002/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 881511403.8356 - val_loss: 1479954168.9863\n",
      "Epoch 1003/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 873462759.8904 - val_loss: 1438961702.5753\n",
      "Epoch 1004/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 856616856.9863 - val_loss: 1411647334.5753\n",
      "Epoch 1005/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 861067303.8904 - val_loss: 1426153352.7671\n",
      "Epoch 1006/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 855436986.3014 - val_loss: 1424860823.6712\n",
      "Epoch 1007/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 856964386.6301 - val_loss: 1449412586.9589\n",
      "Epoch 1008/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 850972565.9178 - val_loss: 1412280947.7260\n",
      "Epoch 1009/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 855727220.6027 - val_loss: 1439644331.8356\n",
      "Epoch 1010/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 849360160.4384 - val_loss: 1404902661.6986\n",
      "Epoch 1011/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 867803010.1918 - val_loss: 1475268106.5205\n",
      "Epoch 1012/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 850901275.6164 - val_loss: 1404411093.9178\n",
      "Epoch 1013/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 866505185.7534 - val_loss: 1413972139.8356\n",
      "Epoch 1014/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 868329335.2329 - val_loss: 1537128366.4658\n",
      "Epoch 1015/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 873564107.3973 - val_loss: 1411192807.4521\n",
      "Epoch 1016/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 869856407.2329 - val_loss: 1402846256.2192\n",
      "Epoch 1017/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 843067648.8767 - val_loss: 1443765866.9589\n",
      "Epoch 1018/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 849389091.5068 - val_loss: 1422768136.7671\n",
      "Epoch 1019/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 848329515.8356 - val_loss: 1417590591.1233\n",
      "Epoch 1020/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 850820093.8082 - val_loss: 1391130948.3836\n",
      "Epoch 1021/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 841204455.8904 - val_loss: 1462082653.8082\n",
      "Epoch 1022/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 844456846.0274 - val_loss: 1409764248.5479\n",
      "Epoch 1023/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 858113462.3562 - val_loss: 1398471617.7534\n",
      "Epoch 1024/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 848822351.3425 - val_loss: 1418267456.8767\n",
      "Epoch 1025/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 841803122.4110 - val_loss: 1487981077.0411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1026/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 883412894.2466 - val_loss: 1431354751.5616\n",
      "Epoch 1027/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 869858236.7123 - val_loss: 1396071374.9041\n",
      "Epoch 1028/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 844340013.5890 - val_loss: 1413941728.4384\n",
      "Epoch 1029/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 841856682.0822 - val_loss: 1403968794.3014\n",
      "Epoch 1030/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 845155786.9589 - val_loss: 1428486943.5616\n",
      "Epoch 1031/1100\n",
      "1168/1168 [==============================] - 0s 25us/sample - loss: 836242599.4521 - val_loss: 1396624331.3973\n",
      "Epoch 1032/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 826935364.3836 - val_loss: 1428722862.0274\n",
      "Epoch 1033/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 845286473.8630 - val_loss: 1440293688.9863\n",
      "Epoch 1034/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 824643998.2466 - val_loss: 1414367153.0959\n",
      "Epoch 1035/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 827857752.9863 - val_loss: 1402478588.4932\n",
      "Epoch 1036/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 826437462.3562 - val_loss: 1441115342.9041\n",
      "Epoch 1037/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 827152265.2055 - val_loss: 1399206755.0685\n",
      "Epoch 1038/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 833839914.0822 - val_loss: 1409644292.8219\n",
      "Epoch 1039/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 823711986.8493 - val_loss: 1410696612.8219\n",
      "Epoch 1040/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 824062382.0274 - val_loss: 1402539345.9726\n",
      "Epoch 1041/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 819809818.7397 - val_loss: 1414481912.9863\n",
      "Epoch 1042/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 818893969.0959 - val_loss: 1436807294.2466\n",
      "Epoch 1043/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 822679690.5205 - val_loss: 1459929610.5205\n",
      "Epoch 1044/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 846925455.7808 - val_loss: 1389595526.1370\n",
      "Epoch 1045/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 822469430.7945 - val_loss: 1451358800.6575\n",
      "Epoch 1046/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 818996914.8493 - val_loss: 1396543739.6164\n",
      "Epoch 1047/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 829764041.6438 - val_loss: 1422854019.5068\n",
      "Epoch 1048/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 829487151.7808 - val_loss: 1431713514.0822\n",
      "Epoch 1049/1100\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 561515904.000 - 0s 21us/sample - loss: 818337681.5342 - val_loss: 1382108707.9452\n",
      "Epoch 1050/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 812945616.6575 - val_loss: 1409533155.9452\n",
      "Epoch 1051/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 837300637.8082 - val_loss: 1471477177.8630\n",
      "Epoch 1052/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 830067550.2466 - val_loss: 1392213779.2877\n",
      "Epoch 1053/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 853060553.6438 - val_loss: 1377774523.6164\n",
      "Epoch 1054/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 846915231.1233 - val_loss: 1388888172.7123\n",
      "Epoch 1055/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 825991809.7534 - val_loss: 1389435137.7534\n",
      "Epoch 1056/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 825463568.6575 - val_loss: 1497654934.7945\n",
      "Epoch 1057/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 820136033.5342 - val_loss: 1401231185.5342\n",
      "Epoch 1058/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 813897287.0137 - val_loss: 1389107780.3836\n",
      "Epoch 1059/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 804620595.7260 - val_loss: 1399632344.5479\n",
      "Epoch 1060/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 800435551.5616 - val_loss: 1425664933.6986\n",
      "Epoch 1061/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 804232168.3288 - val_loss: 1400343376.6575\n",
      "Epoch 1062/1100\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 595213504.000 - 0s 23us/sample - loss: 807490744.5479 - val_loss: 1396101522.4110\n",
      "Epoch 1063/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 811446272.8767 - val_loss: 1404408727.6712\n",
      "Epoch 1064/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 804030552.1096 - val_loss: 1375343596.7123\n",
      "Epoch 1065/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 805247000.1096 - val_loss: 1375900041.6438\n",
      "Epoch 1066/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 792323303.8904 - val_loss: 1422736348.9315\n",
      "Epoch 1067/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 821864737.7534 - val_loss: 1456385592.1096\n",
      "Epoch 1068/1100\n",
      "1168/1168 [==============================] - 0s 19us/sample - loss: 826927616.0000 - val_loss: 1387109859.0685\n",
      "Epoch 1069/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 793144186.7397 - val_loss: 1425275763.2877\n",
      "Epoch 1070/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 791425023.1233 - val_loss: 1387235247.3425\n",
      "Epoch 1071/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 792222159.3425 - val_loss: 1385445365.4795\n",
      "Epoch 1072/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 789104448.4384 - val_loss: 1383944800.0000\n",
      "Epoch 1073/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 819915396.3836 - val_loss: 1455991111.8904\n",
      "Epoch 1074/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 814194948.8219 - val_loss: 1437723376.2192\n",
      "Epoch 1075/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 819246711.6712 - val_loss: 1397043627.8356\n",
      "Epoch 1076/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 803113884.0548 - val_loss: 1481455776.8767\n",
      "Epoch 1077/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 798219731.2877 - val_loss: 1405437783.6712\n",
      "Epoch 1078/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 782703895.2329 - val_loss: 1408614492.0548\n",
      "Epoch 1079/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 784291849.2055 - val_loss: 1396168432.2192\n",
      "Epoch 1080/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 783064087.2329 - val_loss: 1417140266.0822\n",
      "Epoch 1081/1100\n",
      "1168/1168 [==============================] - 0s 24us/sample - loss: 778539050.5205 - val_loss: 1402513156.3836\n",
      "Epoch 1082/1100\n",
      "1168/1168 [==============================] - 0s 23us/sample - loss: 778825781.4795 - val_loss: 1390381154.1918\n",
      "Epoch 1083/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 774859198.6849 - val_loss: 1408638058.9589\n",
      "Epoch 1084/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 773223759.7808 - val_loss: 1377355302.5753\n",
      "Epoch 1085/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 794709940.6027 - val_loss: 1371433899.8356\n",
      "Epoch 1086/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 785294698.5205 - val_loss: 1399969865.6438\n",
      "Epoch 1087/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 773055350.7945 - val_loss: 1378335824.6575\n",
      "Epoch 1088/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 791708952.9863 - val_loss: 1410817213.3699\n",
      "Epoch 1089/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 771076843.3973 - val_loss: 1380403452.4932\n",
      "Epoch 1090/1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 21us/sample - loss: 806462061.5890 - val_loss: 1447830556.9315\n",
      "Epoch 1091/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 779356339.2877 - val_loss: 1391831040.8767\n",
      "Epoch 1092/1100\n",
      "1168/1168 [==============================] - 0s 21us/sample - loss: 773868866.1918 - val_loss: 1375935318.7945\n",
      "Epoch 1093/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 778011430.5753 - val_loss: 1365813111.2329\n",
      "Epoch 1094/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 772944978.8493 - val_loss: 1389763289.4247\n",
      "Epoch 1095/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 762343827.2877 - val_loss: 1414471825.9726\n",
      "Epoch 1096/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 768646026.9589 - val_loss: 1373065998.0274\n",
      "Epoch 1097/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 780202566.5753 - val_loss: 1375093035.8356\n",
      "Epoch 1098/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 774272870.1370 - val_loss: 1385672682.9589\n",
      "Epoch 1099/1100\n",
      "1168/1168 [==============================] - 0s 20us/sample - loss: 764681061.2603 - val_loss: 1387955309.5890\n",
      "Epoch 1100/1100\n",
      "1168/1168 [==============================] - 0s 22us/sample - loss: 754966127.7808 - val_loss: 1401565825.7534\n"
     ]
    }
   ],
   "source": [
    "model = KerasRegressor(build_fn=regress,batch_size=80)\n",
    "history = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=1100,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27388.057243948024"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_train, model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37437.49208553949"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "#test_mse = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sqrt(train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sqrt(test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss / Mean Squared Error')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[126834.75 154830.02 214981.77 ... 184299.14 142361.1  229289.86]\n"
     ]
    }
   ],
   "source": [
    "sale_price_keras = model.predict(test_new)\n",
    "print(sale_price_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1461</td>\n",
       "      <td>126834.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1462</td>\n",
       "      <td>154830.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1463</td>\n",
       "      <td>214981.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1464</td>\n",
       "      <td>190520.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1465</td>\n",
       "      <td>169189.343750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id      SalePrice\n",
       "0  1461  126834.750000\n",
       "1  1462  154830.015625\n",
       "2  1463  214981.765625\n",
       "3  1464  190520.109375\n",
       "4  1465  169189.343750"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub['SalePrice'] = sale_price_keras\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
